[
  {
    "objectID": "template_qmds/26-pvalues-discussion-notes.html",
    "href": "template_qmds/26-pvalues-discussion-notes.html",
    "title": "P-value Discussion (Notes)",
    "section": "",
    "text": "In this activity, you will read an article about p-values and then discuss a set of prompts as a group. Each group will then share out some highlights of your discussion to the class.\n\n\nYour instructor will assign you to one of four groups, each with a different reading to complete ahead of class time. You are of course welcome to read more than only your assigned reading if you choose to! The group reading assignments are as follows:\n\nGroup 1: The ASA Statement on p-values\nGroup 2: Scientific method: Statistical error\nGroup 3: Moving to a World Beyond “p &lt; 0.05”\nGroup 4: Statistical tests, p-values, confidence intervals, and power: a guide to misinterpretations"
  },
  {
    "objectID": "template_qmds/26-pvalues-discussion-notes.html#readings-and-videos",
    "href": "template_qmds/26-pvalues-discussion-notes.html#readings-and-videos",
    "title": "P-value Discussion (Notes)",
    "section": "",
    "text": "Your instructor will assign you to one of four groups, each with a different reading to complete ahead of class time. You are of course welcome to read more than only your assigned reading if you choose to! The group reading assignments are as follows:\n\nGroup 1: The ASA Statement on p-values\nGroup 2: Scientific method: Statistical error\nGroup 3: Moving to a World Beyond “p &lt; 0.05”\nGroup 4: Statistical tests, p-values, confidence intervals, and power: a guide to misinterpretations"
  },
  {
    "objectID": "template_qmds/26-pvalues-discussion-notes.html#step-1-review",
    "href": "template_qmds/26-pvalues-discussion-notes.html#step-1-review",
    "title": "P-value Discussion (Notes)",
    "section": "Step 1: Review",
    "text": "Step 1: Review\nTake a few minutes to review the assigned reading. Make note of the things you found most important, the things you found most confusing, or the things you still have questions about."
  },
  {
    "objectID": "template_qmds/26-pvalues-discussion-notes.html#step-2-discuss",
    "href": "template_qmds/26-pvalues-discussion-notes.html#step-2-discuss",
    "title": "P-value Discussion (Notes)",
    "section": "Step 2: Discuss",
    "text": "Step 2: Discuss\nOnce everyone in your group is ready, find an empty space at the whiteboard.\nYou’ll discuss the following questions with your group and write down your key takeaways from your conversation on the board. After a few minutes, rotate to the next station: respond/react to the previous responses (e.g., add a + if you agree) and add your own. Repeat until you’ve visited all stations.\nQuestions:\n\nHow does this reading relate to your previous knowledge about p-values?\nWas anything that you learned from the reading particularly surprising?\nWhat are some of the ways in which p-values are often misinterpreted and/or misused?\nWhat suggestions does the article offer in terms of how p-values should be used?\nDoes the article provide any suggestions in terms of other tools that we should use instead of or in addition to p-values?\nWhat are your main takeaways from the reading?"
  },
  {
    "objectID": "template_qmds/26-pvalues-discussion-notes.html#step-3-share-out",
    "href": "template_qmds/26-pvalues-discussion-notes.html#step-3-share-out",
    "title": "P-value Discussion (Notes)",
    "section": "Step 3: Share out",
    "text": "Step 3: Share out\nDesignate someone from your group who will share the highlights of your discussion with the full class!"
  },
  {
    "objectID": "template_qmds/26-pvalues-discussion-notes.html#done",
    "href": "template_qmds/26-pvalues-discussion-notes.html#done",
    "title": "P-value Discussion (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/24-hypothesis-testing-considerations-notes.html",
    "href": "template_qmds/24-hypothesis-testing-considerations-notes.html",
    "title": "Hypothesis testing: additional considerations (Notes)",
    "section": "",
    "text": "No template file today - all exercises can be viewed from the webpage\n\n\n\nBy the end of this lesson, you should be able to:\n\nDifferentiate between more and less accurate interpretations of p-values\nExplain how different factors affect statistical power\nExplain the difference between practical and statistical significance\nExplain how multiple testing impacts the conduct and interpretation of statistical research\n\n\n\n\nNo new readings or videos for today."
  },
  {
    "objectID": "template_qmds/24-hypothesis-testing-considerations-notes.html#learning-goals",
    "href": "template_qmds/24-hypothesis-testing-considerations-notes.html#learning-goals",
    "title": "Hypothesis testing: additional considerations (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDifferentiate between more and less accurate interpretations of p-values\nExplain how different factors affect statistical power\nExplain the difference between practical and statistical significance\nExplain how multiple testing impacts the conduct and interpretation of statistical research"
  },
  {
    "objectID": "template_qmds/24-hypothesis-testing-considerations-notes.html#readings-and-videos",
    "href": "template_qmds/24-hypothesis-testing-considerations-notes.html#readings-and-videos",
    "title": "Hypothesis testing: additional considerations (Notes)",
    "section": "",
    "text": "No new readings or videos for today."
  },
  {
    "objectID": "template_qmds/24-hypothesis-testing-considerations-notes.html#exercise-1-conceptual-understanding",
    "href": "template_qmds/24-hypothesis-testing-considerations-notes.html#exercise-1-conceptual-understanding",
    "title": "Hypothesis testing: additional considerations (Notes)",
    "section": "Exercise 1: Conceptual understanding",
    "text": "Exercise 1: Conceptual understanding\n\nSuppose that you and a friend are in two different sections (each with the same number of students) of the same class. On your respective midterm exams, you each obtained 85%, and the class average in both of your classes was 80%. Could one of you or your friend still be considered further above the class average than the other? Briefly explain.\nNow suppose that your section’s test scores were more tightly packed around 80%: maybe the standard deviation of your section’s scores was 2.5, whereas the standard deviation of your friend’s section’s scores was 5. Which of you or your friend was further above the class average? Explain/justify your answer.\nBroadly speaking, does a p-value measure the chance of a hypothesis being true, or, the chance of the data having occurred?\nWhy can’t a p-value measure the other quantity that you didn’t choose in Part c?\nExplain in words why, in calculating a p-value, we need to assume that the null hypothesis is true.\nSuppose that a hypothesis test yields a p-value of 1e-6 (\\(1\\times 10^{-6}\\)). What can you tell about the magnitude of the effect or the uncertainty of the effect from this p-value? (i.e., What can you tell about the coefficient estimate or the standard error?)"
  },
  {
    "objectID": "template_qmds/24-hypothesis-testing-considerations-notes.html#exercise-2-statistical-vs.-practical-significance",
    "href": "template_qmds/24-hypothesis-testing-considerations-notes.html#exercise-2-statistical-vs.-practical-significance",
    "title": "Hypothesis testing: additional considerations (Notes)",
    "section": "Exercise 2: Statistical vs. practical significance",
    "text": "Exercise 2: Statistical vs. practical significance\nMusic researchers compiled information on 16,216 Spotify songs. They looked at the relationship between a song’s genre (latin vs. not latin) and song duration in seconds. Their modeling code and output is below:\nspotify_model &lt;- lm(duration ~ latin_genre, data = spotify)\ncoef(summary(spotify_model))\n##                   Estimate Std. Error   t value   Pr(&gt;|t|)\n## (Intercept)     212.673908  0.4165491 510.56143 0.00000000\n## latin_genreTRUE   1.555355  0.7435700   2.09174 0.03647731\n\nInterpret the latin_genreTRUE coefficient.\nIn the context of song listening, is this a large or small effect size?\nReport and interpret the p-value for the latin_genreTRUE coefficient.\nUse the p-value to make a yes/no decision about the evidence for a relationship between genre and song duration.\nThis exercise highlights the difference between statistical significance and practical significance—explain how. That is, when might we observe statistically significant results that aren’t practically significant?"
  },
  {
    "objectID": "template_qmds/24-hypothesis-testing-considerations-notes.html#exercise-3-power",
    "href": "template_qmds/24-hypothesis-testing-considerations-notes.html#exercise-3-power",
    "title": "Hypothesis testing: additional considerations (Notes)",
    "section": "Exercise 3: Power",
    "text": "Exercise 3: Power\nStatistical power is the probability of rejecting the null hypothesis when the alternative hypothesis is true. We are frequently testing hypotheses to investigate differences or relationships, so in this context, statistical power is the probability of detecting a relationship when there truly is a relationship.\nNavigate to this page to look at an interactive visualization of the factors that influence statistical power.\nUnder “Settings”, next to the “Solve for?” text, click “Power”. You will vary the 3 different parameters (significance level, sample size, and effect size) one at a time to understand how these factors affect power.\nSome context behind this interactive visualization:\n\nVisualization is based on a one sample Z-test:\nThis is a test for whether the true population mean equals a particular value. (e.g., true mean = 30)\nThe effect size slider is measured with a metric called Cohen’s d:\n\nCohen’s d = magnitude of effect/standard deviation of response variable\nHere: how far is the true mean from the null value in units of SD?\ne.g., If the null value is 30, true mean is 40, and the true population SD of the quantity is 5, the Cohen’s d effect size is (40-30)/5 = 2.\n\n\n\nWhat is your intuition about how changing the significance level will change power? Check your intuition with the visualization and explain why this happens.\nRepeat Part a for the sample size.\nRepeat Part a for the effect size."
  },
  {
    "objectID": "template_qmds/24-hypothesis-testing-considerations-notes.html#exercise-4-ethical-considerations",
    "href": "template_qmds/24-hypothesis-testing-considerations-notes.html#exercise-4-ethical-considerations",
    "title": "Hypothesis testing: additional considerations (Notes)",
    "section": "Exercise 4: Ethical considerations",
    "text": "Exercise 4: Ethical considerations\n\nVisit this page and look at both the comic at the top and the various ways in which researchers have described p-values that do not fall below the \\(\\alpha = 0.05\\) significance level threshold. What ethical consideration is arising here? (Just for fun: a related xkcd comic)\nTake a look at the xkcd comic here. What ethical consideration is arising here?"
  },
  {
    "objectID": "template_qmds/24-hypothesis-testing-considerations-notes.html#done",
    "href": "template_qmds/24-hypothesis-testing-considerations-notes.html#done",
    "title": "Hypothesis testing: additional considerations (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/22-hypothesis-testing-discovery-notes.html",
    "href": "template_qmds/22-hypothesis-testing-discovery-notes.html",
    "title": "Hypothesis testing: discovery (Notes)",
    "section": "",
    "text": "You can download a template file to work with here.\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder.\n\n\n\nBy the end of this lesson, you should be able to:\n\nUnderstand how standard errors and confidence intervals enable us to make statistical inferences\nArticulate how we can formalize a research question as a testable, statistical hypothesis\n\n\n\n\nThis is a discovery activity, so no assigned readings/videos today."
  },
  {
    "objectID": "template_qmds/22-hypothesis-testing-discovery-notes.html#learning-goals",
    "href": "template_qmds/22-hypothesis-testing-discovery-notes.html#learning-goals",
    "title": "Hypothesis testing: discovery (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nUnderstand how standard errors and confidence intervals enable us to make statistical inferences\nArticulate how we can formalize a research question as a testable, statistical hypothesis"
  },
  {
    "objectID": "template_qmds/22-hypothesis-testing-discovery-notes.html#readings-and-videos",
    "href": "template_qmds/22-hypothesis-testing-discovery-notes.html#readings-and-videos",
    "title": "Hypothesis testing: discovery (Notes)",
    "section": "",
    "text": "This is a discovery activity, so no assigned readings/videos today."
  },
  {
    "objectID": "template_qmds/22-hypothesis-testing-discovery-notes.html#exercise-1",
    "href": "template_qmds/22-hypothesis-testing-discovery-notes.html#exercise-1",
    "title": "Hypothesis testing: discovery (Notes)",
    "section": "Exercise 1",
    "text": "Exercise 1\nResearch question: Is there evidence that the mercury concentration in fish (Concen) differs according to the River they were sampled from?\n\npart a: fit the model\nFit a simple linear regression model that would address our research question\n\nmod_fish &lt;- ___\nsummary(mod_fish)\n## Error in parse(text = input): &lt;text&gt;:1:14: unexpected input\n## 1: mod_fish &lt;- __\n##                  ^\n\nInterpret the intercept from this model.\n\nResponse\n\n\n\npart b: construct a CI\nUsing the 68-95-99.7 rule, construct an approximate 95% confidence interval for the intercept term, and provide an appropriate interpretation.\n\nResponse\n\nCompare your CI to an exact 95% confidence interval for the model coefficients:\n\nconfint(mod_fish, level=0.95)\n## Error: object 'mod_fish' not found\n\n\n\npart c: what can we conclude from multiple samples?\nSuppose we take 200 different samples of fish from the Lumber River. Based on these results, in how many of those samples would you expect to observe mean mercury concentration greater than 1.25ppm?\n\nResponse\n\n\n\npart d: intuition for constructing & interpreting test statistics\nSuppose previous environmental studies have found little evidence of mercury pollution in other rivers in the area, so perhaps our “default” assumption is that fish from the Lumber river should have an expected mercury concentration of 0ppm. How many standard errors is our sample estimate (1.078ppm) away from this expectation? What are three possible conclusions?\n\nResponse\n\n\n\npart e: do individual observations contradict our conclusions?\nNow suppose we sample a single fish from the Lumber River and find it has a mercury concentration of 2.5ppm. Are you surprised by this result? Why or why not? (Hint: create a code chunk that calculates the mean, standard deviation, and maximum of the Concen variable in each river in our original sample)\n\nResponse"
  },
  {
    "objectID": "template_qmds/22-hypothesis-testing-discovery-notes.html#exercise-2",
    "href": "template_qmds/22-hypothesis-testing-discovery-notes.html#exercise-2",
    "title": "Hypothesis testing: discovery (Notes)",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet’s look at the model summary output again:\n\nsummary(mod_fish)\n## Error: object 'mod_fish' not found\n\n\npart a: interpret model coefficient\nNow, let’s interpret the RiverWacamaw coefficient. Based only on the coefficient (don’t think about the standard error yet), what can we say about the difference in mercury concentration among fish in the two rivers?\n\nResponse\n\n\n\npart b: construct a CI\nUsing the 68-95-99.7 rule, construct an approximate 95% confidence interval for the RiverWacamaw coefficient, and provide an appropriate interpretation.\n\nResponse\n\n\n\npart c: interpreting the CI\nDo you believe it plausible that the mean mercury concentration of the fish population in the Wacamaw River is approximately the same as that of the fish population in the Lumber River? How would you confirm this? What assumptions are you making?\n\nResponse\n\n\n\npart d: effect of sample size on our conclusions\nSuppose we sample 10 times as many fish from the Wacamaw River, and get a similar coefficient estimate (0.2). Thinking back to the Central Limit Theorem, what should happen to the standard error of the RiverWacamaw coefficient? How small of a standard error would we need to more conclusively say that there is an actual difference in mean mercury concentrations of the Lumber River and Wacamaw River fish populations?\n\nResponse\n\n\n\npart e: reconciling parameter estimates and uncertainty\nSuppose the true population coefficient for the RiverWacamawparameter is 0.02 (i.e. the average mercury concentration is 0.02ppm higher for the Wacamaw River fish population compared to that of the Lumber River). Is this meaningful?\n\nResponse\n\n\n\npart f (CHALLENGE)\nUsing the model summary output, report the mean mercury concentration for our sample of fish from the Wacamaw River:\n\nsummary(mod_fish)\n## Error: object 'mod_fish' not found\n\n\nResponse:\n\nWhich of the following values do you think is the standard error of the sample mean for the Wacamaw River?\n\n0.11712\n0.08866\n0.11712 + 0.08866 = 0.20578\n0.11712 - 0.08866 = 0.02846\nsomething else\n\nTo answer this question, look at the code chunk below, which fits the same model, but uses the Wacamaw River as our reference category instead of the Lumber River:\n\nmod_fish2 &lt;- lm(Concen ~ River, data=fish %&gt;% mutate(River=ifelse(River == \"Wacamaw\", paste0(\"_\", River), River)))\nsummary(mod_fish2)\n## \n## Call:\n## lm(formula = Concen ~ River, data = fish %&gt;% mutate(River = ifelse(River == \n##     \"Wacamaw\", paste0(\"_\", River), River)))\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1664 -0.5681 -0.1764  0.4219  2.4219 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  1.27643    0.07652  16.681   &lt;2e-16 ***\n## RiverLumber -0.19835    0.11712  -1.694   0.0922 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.7575 on 169 degrees of freedom\n## Multiple R-squared:  0.01669,    Adjusted R-squared:  0.01087 \n## F-statistic: 2.868 on 1 and 169 DF,  p-value: 0.09218\n\nCompare this to the output for mod_fish. What do you notice about the standard errors of the intercepts (i.e., the standard errors of the means for each river) compared to the standard errors of the RiverWacamaw and RiverLumber coefficients (i.e., the standard errors of the differences between the means)?\n\nResponse:"
  },
  {
    "objectID": "template_qmds/22-hypothesis-testing-discovery-notes.html#reflection",
    "href": "template_qmds/22-hypothesis-testing-discovery-notes.html#reflection",
    "title": "Hypothesis testing: discovery (Notes)",
    "section": "Reflection",
    "text": "Reflection\nBased on this activity and the inference tools you’ve learned about so far (sampling distributions, standard errors, confidence intervals), can you think of and describe a way that you can quantify evidence “for” or “against” a coefficient being equal to some particular value? (for example, we have evidence that the average mercury concentration in Lumber River fish is ~1.08ppm, and the standard error of this estimate suggests that observing a fish with 0ppm is very unlikely. How can we quantify that evidence?)\n\nResponse:"
  },
  {
    "objectID": "template_qmds/22-hypothesis-testing-discovery-notes.html#done",
    "href": "template_qmds/22-hypothesis-testing-discovery-notes.html#done",
    "title": "Hypothesis testing: discovery (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html",
    "href": "template_qmds/20-bootstrapping-notes.html",
    "title": "Bootstrapping (Notes)",
    "section": "",
    "text": "You will not be able to render this document until you’ve filled in the code!\n\n\nLet \\(\\beta\\) be some population parameter and \\(\\hat{\\beta}\\) be a sample estimate of \\(\\beta\\). In order to study the potential error in \\(\\hat{\\beta}\\), you will…\n\nexplore two approaches to approximating the sampling distribution of \\(\\hat{\\beta}\\):\n\nCentral Limit Theorem (CLT)\nbootstrapping\n\nidentify the difference between sampling and resampling\nintuit how bootstrapping results can be used to make inferences about \\(\\beta\\)\n\n\n\n\nPlease watch the following video after class:\n\nbootstrapping"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#learning-goals",
    "href": "template_qmds/20-bootstrapping-notes.html#learning-goals",
    "title": "Bootstrapping (Notes)",
    "section": "",
    "text": "Let \\(\\beta\\) be some population parameter and \\(\\hat{\\beta}\\) be a sample estimate of \\(\\beta\\). In order to study the potential error in \\(\\hat{\\beta}\\), you will…\n\nexplore two approaches to approximating the sampling distribution of \\(\\hat{\\beta}\\):\n\nCentral Limit Theorem (CLT)\nbootstrapping\n\nidentify the difference between sampling and resampling\nintuit how bootstrapping results can be used to make inferences about \\(\\beta\\)"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#readings-and-videos",
    "href": "template_qmds/20-bootstrapping-notes.html#readings-and-videos",
    "title": "Bootstrapping (Notes)",
    "section": "",
    "text": "Please watch the following video after class:\n\nbootstrapping"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#exercise-1-sample-vs-population",
    "href": "template_qmds/20-bootstrapping-notes.html#exercise-1-sample-vs-population",
    "title": "Bootstrapping (Notes)",
    "section": "Exercise 1: sample vs population",
    "text": "Exercise 1: sample vs population\n\nIn the summary table, is the Length coefficient 0.058 the population slope \\(\\beta_1\\) or a sample estimate \\(\\hat{\\beta}_1\\)?\nIf it’s a sample estimate, how accurate do you think it is?"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#exercise-2-the-rub",
    "href": "template_qmds/20-bootstrapping-notes.html#exercise-2-the-rub",
    "title": "Bootstrapping (Notes)",
    "section": "Exercise 2: The rub",
    "text": "Exercise 2: The rub\nSince we don’t know \\(\\beta_1\\), we can’t know the exact error in \\(\\hat{\\beta}_1\\)! This is where sampling distributions come in. They describe how estimates \\(\\hat{\\beta}_1\\) might vary from sample to sample, thus how far these estimates might fall from \\(\\beta_1\\):\n\nIn past activities, we used simulations to approximate the sampling distribution. For example, we took and evaluated 500 different samples of 10 counties from the population of 3142 counties. Why can’t we do that here in our fish example?"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#exercise-3-clt",
    "href": "template_qmds/20-bootstrapping-notes.html#exercise-3-clt",
    "title": "Bootstrapping (Notes)",
    "section": "Exercise 3: CLT",
    "text": "Exercise 3: CLT\nIn practice, we can’t observe the sampling distribution and its corresponding standard error. But we can approximate them. When our sample size n is “large enough”, we might approximate the sampling distribution using the CLT:\n\\[\\hat{\\beta}_1 \\sim N(\\beta_1, \\text{standard error}^2)\\]\nThe standard error in the CLT is approximated from our sample via some formula \\(c / \\sqrt{n}\\) where “c” is complicated. Obtain and interpret this standard error from the model summary table:\n\ncoef(summary(fish_model))\n##                Estimate  Std. Error   t value     Pr(&gt;|t|)\n## (Intercept) -1.13164542 0.213614796 -5.297598 3.617750e-07\n## Length       0.05812749 0.005227593 11.119359 6.641225e-22"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#reflect",
    "href": "template_qmds/20-bootstrapping-notes.html#reflect",
    "title": "Bootstrapping (Notes)",
    "section": "REFLECT",
    "text": "REFLECT\nGreat! We can approximate the sampling distribution and standard error using the CLT. BUT:\n\nthe quality of this approximation hinges upon the validity of the Central Limit theorem which hinges upon the validity of the theoretical model assumptions\nthe CLT uses complicated formulas for the standard error estimates, thus can feel a little mysterious\n\nLet’s explore how we can use bootstrapping to complement (not entirely replace) the CLT. The saying “to pull oneself up by the bootstraps” is often attributed to Rudolf Erich Raspe’s 1781 The Surprising Adventures of Baron Munchausen in which the character pulls himself out of a swamp by his hair (not bootstraps). In short, it means to get something from nothing, through your own effort:\n\nIn this spirit, statistical bootstrapping doesn’t make any probability model assumptions. It uses only the information from our one sample to approximate standard errors."
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#exercise-4-challenge",
    "href": "template_qmds/20-bootstrapping-notes.html#exercise-4-challenge",
    "title": "Bootstrapping (Notes)",
    "section": "Exercise 4: Challenge",
    "text": "Exercise 4: Challenge\nRecall that we have a sample size of 171 fish:\n\nnrow(fish)\n## [1] 171\n\nWe’ll obtain a bootstrapping distribution of \\(\\hat{\\beta}_1\\) by taking many (500) different samples of 171 fish and exploring the degree to which \\(\\hat{\\beta}_1\\) varies from sample to sample. Let’s try doing this as we did in past activities:\n\n# Build 500 models using samples of size 171\nfish_models_bad_simulation &lt;- mosaic::do(500)*(\n  fish %&gt;% \n    sample_n(size = 171, replace = FALSE) %&gt;% \n    with(lm(Concen ~ Length))\n)\n\nhead(fish_models_bad_simulation)\n##   Intercept     Length     sigma r.squared        F numdf dendf .row .index\n## 1 -1.131645 0.05812749 0.5805245 0.4224989 123.6401     1   169    1      1\n## 2 -1.131645 0.05812749 0.5805245 0.4224989 123.6401     1   169    1      2\n## 3 -1.131645 0.05812749 0.5805245 0.4224989 123.6401     1   169    1      3\n## 4 -1.131645 0.05812749 0.5805245 0.4224989 123.6401     1   169    1      4\n## 5 -1.131645 0.05812749 0.5805245 0.4224989 123.6401     1   169    1      5\n## 6 -1.131645 0.05812749 0.5805245 0.4224989 123.6401     1   169    1      6\n\nWhat’s funny about the results? Why do you think this happened? How might you adjust the code to “fix” things?"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#exercise-5-resampling",
    "href": "template_qmds/20-bootstrapping-notes.html#exercise-5-resampling",
    "title": "Bootstrapping (Notes)",
    "section": "Exercise 5: Resampling",
    "text": "Exercise 5: Resampling\nIn practice, we take one sample of size n from the population. To obtain a bootstrapping distribution of some sample estimate \\(\\hat{\\beta}\\), we…\n\ntake many resamples of size n with replacement from the sample\ncalculate \\(\\hat{\\beta}\\) using each resample\n\n\nLet’s wrap our minds around the idea of resampling using a small example of 5 fish:\n\n# Define data\nsmall_sample &lt;- data.frame(\n  id = 1:5,\n  Length = c(44, 43, 54, 52, 40))\n\nsmall_sample\n##   id Length\n## 1  1     44\n## 2  2     43\n## 3  3     54\n## 4  4     52\n## 5  5     40\n\nThis sample has a mean Length of 46.6 cm:\n\nsmall_sample %&gt;% \n  summarize(mean(Length))\n##   mean(Length)\n## 1         46.6\n\n\nThe chunk below samples 5 fish without replacement from our small_sample of 5 fish, and calculates their mean length. Run it several times. How do the sample and resulting mean change?\n\n\nsample_1 &lt;- sample_n(small_sample, size = 5, replace = FALSE)\nsample_1\n##   id Length\n## 1  5     40\n## 2  2     43\n## 3  4     52\n## 4  3     54\n## 5  1     44\n\nsample_1 %&gt;% \n  summarize(mean(Length))\n##   mean(Length)\n## 1         46.6\n\n\nSampling our sample without replacement merely returns our original sample. Instead, resample 5 fish from our small_sample with replacement. Run it several times. What do you notice about the samples? About their mean lengths?\n\n\nsample_2 &lt;- sample_n(small_sample, size = 5, replace = TRUE)\nsample_2\n##   id Length\n## 1  5     40\n## 2  5     40\n## 3  2     43\n## 4  1     44\n## 5  1     44\n\nsample_2 %&gt;% \n  summarize(mean(Length))\n##   mean(Length)\n## 1         42.2\n\n\nResampling our sample provides insight into the variability, hence potential error, in our sample estimates. (This works better when we have a sample bigger than 5!) As you observed in part b, each resample might include some fish from the original sample several times and others not at all. Why is this ok?"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#exercise-6-bootstrapping",
    "href": "template_qmds/20-bootstrapping-notes.html#exercise-6-bootstrapping",
    "title": "Bootstrapping (Notes)",
    "section": "Exercise 6: Bootstrapping",
    "text": "Exercise 6: Bootstrapping\nWe’re ready to bootstrap! Fix one line of the code below to obtain 500 bootstrap estimates of the model of Concen by Length:\n\n# Set the seed so we get the same results\nset.seed(155)\n\n# Build 500 bootstrap models using REsamples of size 171\nfish_models_bootstrap &lt;- mosaic::do(500)*(\n  fish %&gt;% \n    sample_n(size = 171, replace = FALSE) %&gt;% \n    with(lm(Concen ~ Length))\n)\n\nhead(fish_models_bootstrap)\n##   Intercept     Length     sigma r.squared        F numdf dendf .row .index\n## 1 -1.131645 0.05812749 0.5805245 0.4224989 123.6401     1   169    1      1\n## 2 -1.131645 0.05812749 0.5805245 0.4224989 123.6401     1   169    1      2\n## 3 -1.131645 0.05812749 0.5805245 0.4224989 123.6401     1   169    1      3\n## 4 -1.131645 0.05812749 0.5805245 0.4224989 123.6401     1   169    1      4\n## 5 -1.131645 0.05812749 0.5805245 0.4224989 123.6401     1   169    1      5\n## 6 -1.131645 0.05812749 0.5805245 0.4224989 123.6401     1   169    1      6"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#exercise-7-bootstrap-results",
    "href": "template_qmds/20-bootstrapping-notes.html#exercise-7-bootstrap-results",
    "title": "Bootstrapping (Notes)",
    "section": "Exercise 7: Bootstrap results",
    "text": "Exercise 7: Bootstrap results\nRecall that we started with 1 sample, thus 1 estimate of the model:\n\ncoef(summary(fish_model))\n##                Estimate  Std. Error   t value     Pr(&gt;|t|)\n## (Intercept) -1.13164542 0.213614796 -5.297598 3.617750e-07\n## Length       0.05812749 0.005227593 11.119359 6.641225e-22\n\n\nWe now have 500 (resample) bootstrap estimates of the model. These vary around the red line in the plot below. What does the red line represent: the actual population model or fish_model (the estimated model calculated from our original fish sample)?\n\n\nfish %&gt;% \n  ggplot(aes(y = Concen, x = Length)) +\n  geom_abline(data = fish_models_bootstrap, aes(intercept = Intercept, slope = Length), color = \"gray\") + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\n\nNow focus on just the 500 (resample) bootstrap estimates of the slope Length coefficient. Before plotting the distribution of these resampled slopes, what do you anticipate? What shape do you expect the distribution will have? Around what value do you expect it to be centered?\nCheck your intuition with the plot below. Was your intuition right?\n\n\nfish_models_bootstrap %&gt;% \n  ggplot(aes(x = Length)) + \n  geom_density()"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#exercise-8-bootstrap-standard-errors",
    "href": "template_qmds/20-bootstrapping-notes.html#exercise-8-bootstrap-standard-errors",
    "title": "Bootstrapping (Notes)",
    "section": "Exercise 8: Bootstrap standard errors",
    "text": "Exercise 8: Bootstrap standard errors\nSince they’re calculated from resamples of our sample, not different samples from the population, the 500 bootstrap estimates of the slope are centered around our original sample estimate. Importantly:\nThe degree to which the bootstrap estimates vary from the original sample estimate provides insight in the degree to which our original sample estimate might vary from the actual population slope (i.e. its standard error)!\n\nUse the bootstrap estimates of the Length slope coefficient to approximate the standard error of 0.05813, our original sample estimate. HINT: standard deviation\n\n\n# fish_models_bootstrap %&gt;% \n#   ___(___(Length))\n\n\nHow does this bootstrapped approximation of standard error compare to that calculated via (a complicated mystery) formula and reported in the model summary table?\n\n\ncoef(summary(fish_model))\n##                Estimate  Std. Error   t value     Pr(&gt;|t|)\n## (Intercept) -1.13164542 0.213614796 -5.297598 3.617750e-07\n## Length       0.05812749 0.005227593 11.119359 6.641225e-22"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#pause-powerful-stuff",
    "href": "template_qmds/20-bootstrapping-notes.html#pause-powerful-stuff",
    "title": "Bootstrapping (Notes)",
    "section": "Pause: Powerful stuff!",
    "text": "Pause: Powerful stuff!\nJust pause here to appreciate how awesome it is that you approximated the potential error in our sample estimates using simulation and your sample data alone – no “theorems” or complicated formulas. You might say we pulled ourselves up by the bootstraps."
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#exercise-9-looking-ahead-at-intervals",
    "href": "template_qmds/20-bootstrapping-notes.html#exercise-9-looking-ahead-at-intervals",
    "title": "Bootstrapping (Notes)",
    "section": "Exercise 9: Looking ahead at intervals",
    "text": "Exercise 9: Looking ahead at intervals\nIn the past few activities, we’ve been exploring sampling variability and error. These tools are critical in using our sample to make inferences about the broader population. We’ll explore inference more formally in the weeks ahead. Here, use your intuition to apply our bootstrapping results:\n\nfish %&gt;% \n  ggplot(aes(y = Concen, x = Length)) +\n  geom_abline(data = fish_models_bootstrap, aes(intercept = Intercept, slope = Length), color = \"gray\")\n\n\n\n\n\n\n\n\n\nfish_models_bootstrap %&gt;% \n  ggplot(aes(x = Length)) + \n  geom_density()\n\n\n\n\n\n\n\n\n\nOur original sample estimate of the Length coefficient, 0.0581, was simply our best guess of the actual coefficient among all fish. But we know it’s wrong. Based on the plots above, provide a bigger range or interval of plausible values for the actual coefficient among all fish.\nWe can do better than visual approximations. Use the fish_models_bootstrap results to provide a more specific interval of plausible values for the actual Length coefficient. THINK: Do you think we should use the full range of observed bootstrap estimates? Just a fraction?\n\n\n# fish_models_bootstrap %&gt;% \n#   summarize(___(Length, ___), ___(Length, ___))"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#exercise-10-looking-ahead-at-hypothesis-testing",
    "href": "template_qmds/20-bootstrapping-notes.html#exercise-10-looking-ahead-at-hypothesis-testing",
    "title": "Bootstrapping (Notes)",
    "section": "Exercise 10: Looking ahead at hypothesis testing",
    "text": "Exercise 10: Looking ahead at hypothesis testing\nSome researchers claim that mercury content is associated with the length of a fish. Let’s use our bootstrapping results to test this hypothesis.\n\nBased on only the plot below of our bootstrap models, do you think our sample data supports this hypothesis?\n\n\nfish %&gt;% \n  ggplot(aes(y = Concen, x = Length)) +\n  geom_abline(data = fish_models_bootstrap, aes(intercept = Intercept, slope = Length), color = \"gray\")\n\n\n\n\n\n\n\n\n\nWhat about numerical evidence? Based on the interval you calculated in part b of the previous exercise, do you think our sample data supports this hypothesis?"
  },
  {
    "objectID": "template_qmds/20-bootstrapping-notes.html#done",
    "href": "template_qmds/20-bootstrapping-notes.html#done",
    "title": "Bootstrapping (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "",
    "text": "Let \\(\\beta\\) be some population parameter and \\(\\hat{\\beta}\\) be a sample estimate of \\(\\beta\\). Our goals for the day are to:\n\nuse simulation to solidify our understanding of sampling distributions and standard errors\nexplore and compare two approaches to approximating the sampling distribution of \\(\\hat{\\beta}\\):\n\nCentral Limit Theorem (CLT)\nbootstrapping\n\nexplore the impact of sample size on sampling distributions and standard errors\n\n\n\n\nPlease watch/do the following videos and readings before class:\n\nReading: Section 6.7 in the STAT 155 Notes\nVideo 1: sampling distributions\nVideo 2: Central Limit Theorem\nVideo 3: bootstrapping"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#learning-goals",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#learning-goals",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "",
    "text": "Let \\(\\beta\\) be some population parameter and \\(\\hat{\\beta}\\) be a sample estimate of \\(\\beta\\). Our goals for the day are to:\n\nuse simulation to solidify our understanding of sampling distributions and standard errors\nexplore and compare two approaches to approximating the sampling distribution of \\(\\hat{\\beta}\\):\n\nCentral Limit Theorem (CLT)\nbootstrapping\n\nexplore the impact of sample size on sampling distributions and standard errors"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#readings-and-videos",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#readings-and-videos",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "",
    "text": "Please watch/do the following videos and readings before class:\n\nReading: Section 6.7 in the STAT 155 Notes\nVideo 1: sampling distributions\nVideo 2: Central Limit Theorem\nVideo 3: bootstrapping"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#reflect",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#reflect",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "REFLECT",
    "text": "REFLECT\nGreat! We have two options. Here are some things to think about / reflect on:\n\nWe can approximate the sampling distribution and standard error using the CLT. BUT:\n\nthe quality of this approximation hinges upon the validity of the Central Limit theorem which hinges upon the validity of the theoretical model assumptions, as well as a large sample size\nthe CLT uses theoretical formulas for the standard error estimates, thus can feel a little mysterious without a solid foundation in probability theory\n\nWe can approxiate the sampling distribution and standard error using bootstrapping. BUT:\n\nit feels magical. The statistical theory behind bootstrapping is quite complicated, and there are certain obscure cases (none that we will encounter in Stat 155) where the assumptions underlying bootstrapping fail to hold\n\n\nNeither approach is perfect, but they complement one another. Bootrapping in particular, while it cannot and should not replace the CLT, gives us some nice intuition behind the idea of resampling, which is fundamental for hypothesis testing (which we’ll get to shortly!).\n\nReflect: Before testing them out, what questions do you have about either approach? What do you think would help you build more intuition for the CLT and/or bootstrapping? Does one approach resonate with you more than the other?"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-1-500-samples-of-size-10",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-1-500-samples-of-size-10",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Exercise 1: 500 samples of size 10",
    "text": "Exercise 1: 500 samples of size 10\nRecall that we can sample 10 observations from our dataset using sample_n():\n\n# Run this chunk a few times to explore the different samples you get\nfish %&gt;% \n  sample_n(size = 10, replace = TRUE)\n## # A tibble: 10 × 5\n##    River   Station Length Weight Concen\n##    &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n##  1 Wacamaw      10   25.5    203   0.76\n##  2 Lumber        6   30      380   1.4 \n##  3 Wacamaw       8   31      415   0.53\n##  4 Lumber        3   38.5    773   1.3 \n##  5 Wacamaw       8   33      518   0.55\n##  6 Lumber        2   33.5    448   0.99\n##  7 Lumber        3   40.5    863   1.1 \n##  8 Wacamaw      10   47.3   1632   1.5 \n##  9 Wacamaw       7   34.5    496   0.58\n## 10 Wacamaw      10   28.3    274   0.5\n\nWe can also take a sample and then use the data to estimate the model:\n\n# Run this chunk a few times to explore the different sample models you get\nfish %&gt;% \n  sample_n(size = 10, replace = TRUE) %&gt;% \n  with(lm(Concen ~ Length))\n## \n## Call:\n## lm(formula = Concen ~ Length)\n## \n## Coefficients:\n## (Intercept)       Length  \n##     0.02074      0.03012\n\nWe can also take multiple unique samples and build a sample model from each.\nThe code below obtains 500 separate samples of 10 fish, and stores the model estimates from each:\n\n# Set the seed so that we all get the same results\nset.seed(155)\n\n# Store the sample models\nsample_models_10 &lt;- mosaic::do(500)*(\n  fish %&gt;% \n    sample_n(size = 10, replace = TRUE) %&gt;% \n    with(lm(Concen ~ Length))\n)\n\n# Check it out\nhead(sample_models_10)\n##    Intercept     Length     sigma r.squared          F numdf dendf .row .index\n## 1 -2.7811151 0.09635286 0.5737186 0.7273700  21.343798     1     8    1      1\n## 2 -3.0684939 0.10953496 0.3797853 0.9347767 114.655478     1     8    1      2\n## 3 -1.2880671 0.05933243 0.7457468 0.3776855   4.855237     1     8    1      3\n## 4 -0.9248832 0.05411299 0.6251621 0.4796674   7.374782     1     8    1      4\n## 5 -1.0882523 0.05349699 0.3441272 0.4985134   7.952571     1     8    1      5\n## 6 -0.4558309 0.04074788 0.7785389 0.1719987   1.661820     1     8    1      6\ndim(sample_models_10)\n## [1] 500   9\n\n\nWhat’s the point of the do() function?!? If you’ve taken any COMP classes, what process do you think do() is a shortcut for?\nWhat is stored in the Intercept, Length, and r.squared columns of the results?\nWe’ll obtain a bootstrapping distribution of \\(\\hat{\\beta}_1\\) by taking many (500, in this case) different samples of every fish in our dataset (171 of them) and exploring the degree to which \\(\\hat{\\beta}_1\\) varies from sample to sample.\n\nEdit the code below to obtain a bootstrapping distribution.\n\n# Set the seed so that we all get the same results\nset.seed(155)\n\n# Store the sample models\nsample_models_boot &lt;- mosaic::do(___)*(\n  fish %&gt;% \n    sample_n(size = ___, replace = TRUE) %&gt;% \n    with(lm(Concen ~ Length))\n)\n## Error in parse(text = input): &lt;text&gt;:5:35: unexpected input\n## 4: # Store the sample models\n## 5: sample_models_boot &lt;- mosaic::do(__\n##                                      ^"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-2-why-resampling-replace-true",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-2-why-resampling-replace-true",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Exercise 2: Why “resampling” (replace = TRUE)?",
    "text": "Exercise 2: Why “resampling” (replace = TRUE)?\nLet’s wrap our minds around the idea of resampling, before coming back to our boostrapping distribution, using a small example of 5 fish:\n\n# Define data\nsmall_sample &lt;- data.frame(\n  id = 1:5,\n  Length = c(44, 43, 54, 52, 40))\n\nsmall_sample\n##   id Length\n## 1  1     44\n## 2  2     43\n## 3  3     54\n## 4  4     52\n## 5  5     40\n\nThis sample has a mean Length of 46.6 cm:\n\nsmall_sample %&gt;% \n  summarize(mean(Length))\n##   mean(Length)\n## 1         46.6\n\n\nThe chunk below samples 5 fish without replacement from our small_sample of 5 fish, and calculates their mean length. Run it several times. How do the sample and resulting mean change?\n\n\nsample_1 &lt;- sample_n(small_sample, size = 5, replace = FALSE)\nsample_1\n##   id Length\n## 1  4     52\n## 2  2     43\n## 3  3     54\n## 4  5     40\n## 5  1     44\n\nsample_1 %&gt;% \n  summarize(mean(Length))\n##   mean(Length)\n## 1         46.6\n\n\nSampling our sample without replacement merely returns our original sample. Instead, resample 5 fish from our small_sample with replacement. Run it several times. What do you notice about the samples? About their mean lengths?\n\n\nsample_2 &lt;- sample_n(small_sample, size = 5, replace = TRUE)\nsample_2\n##   id Length\n## 1  1     44\n## 2  5     40\n## 3  1     44\n## 4  4     52\n## 5  5     40\n\nsample_2 %&gt;% \n  summarize(mean(Length))\n##   mean(Length)\n## 1           44\n\nResampling our sample provides insight into the variability, hence potential error, in our sample estimates. (This works better when we have a sample bigger than 5!) As you observed in part b, each resample might include some fish from the original sample several times and others not at all.\nBonus Fact: Sampling with replacement also ensures that our resampled observations are independent, which we need in order for bootstrapping to “work”!"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-3-sampling-distribution",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-3-sampling-distribution",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Exercise 3: Sampling distribution",
    "text": "Exercise 3: Sampling distribution\nCheck out the resulting 500 bootstrapped sample models:\n\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen)) + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_abline(data = sample_models_boot, \n              aes(intercept = Intercept, slope = Length), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n## Error: object 'sample_models_boot' not found\n\nLet’s focus on the slopes of these 500 sample models.\nA plot of the 500 slopes approximates the sampling distribution of the sample slopes.\n\nsample_models_boot %&gt;% \n  ggplot(aes(x = Length)) + \n  geom_density() + \n  geom_vline(xintercept = 0.05813, color = \"red\") \n## Error: object 'sample_models_boot' not found\n\nDescribe the sampling distribution:\n\nWhat’s its general shape?\nWhere is it roughly centered?\nRoughly what’s its spread / i.e. what’s the range of estimates you observed?"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-4-standard-error",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-4-standard-error",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Exercise 4: Standard error",
    "text": "Exercise 4: Standard error\nFor a more rigorous assessment of the spread among the sample slopes, let’s calculate their standard deviation:\n\nsample_models_boot %&gt;% \n  summarize(sd(Length))\n## Error: object 'sample_models_boot' not found\n\nRecall: The standard deviation of sample estimates is called a “standard error”.\nIt measures the typical distance of a sample estimate from the actual population value.\nCompare the bootstrapped standard error to the standard error reported from our regression model (see the Std. Error column):\n\ncoef(summary(fish_model))\n##                Estimate  Std. Error   t value     Pr(&gt;|t|)\n## (Intercept) -1.13164542 0.213614796 -5.297598 3.617750e-07\n## Length       0.05812749 0.005227593 11.119359 6.641225e-22\n\nAre they roughly equivalent?\n\nYour response here"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-5-central-limit-theorem-clt",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-5-central-limit-theorem-clt",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Exercise 5: Central Limit Theorem (CLT)",
    "text": "Exercise 5: Central Limit Theorem (CLT)\nRecall that the CLT assumes that, so long as our sample size is “big enough”, the sampling distribution of the sample slope will be Normal.\nSpecifically, all possible sample slopes will vary Normally around the population slope.\n\nDo your simulation results support this assumption? Why or why not?\nWant more intuition into the CLT? Watch this video explanation using bunnies and dragons: https://www.youtube.com/watch?v=jvoxEYmQHNM"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-6-using-the-clt",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-6-using-the-clt",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Exercise 6: Using the CLT",
    "text": "Exercise 6: Using the CLT\nLet \\(\\hat{\\beta}_1\\) be an estimate of the (super)population slope parameter \\(\\beta_1\\) calculated from a sample of 10 fish (sample_models_10).\nEstimate the standard error of the slope from these resampled estimates\n\n# Hint: Adapt the code from Exercise 5...\n\nYou should get a SE of roughly 0.026.\nThus, by the CLT, the sampling distribution of \\(\\hat{\\beta}_1\\) is:\n\\[\\hat{\\beta}_1 \\sim N(\\beta_1, 0.26^2)\\]\nUse this result with the 68-95-99.7 property of the Normal model to understand the potential error in a slope estimate.\n\nThere are many possible samples of 10 fish. What percent of these will produce an estimate \\(\\hat{\\beta}_1\\) that’s within 0.052, i.e. 2 standard errors, of the actual population slope \\(\\beta_1\\)?\nMore than 2 standard errors from \\(\\beta_1\\)?\nMore than 0.079, i.e. 3 standard errors, above \\(\\beta_1\\)?"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-7-clt-and-the-68-95-99.7-rule",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-7-clt-and-the-68-95-99.7-rule",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Exercise 7: CLT and the 68-95-99.7 Rule",
    "text": "Exercise 7: CLT and the 68-95-99.7 Rule\nFill in the blanks below to complete some general properties assumed by the CLT:\n\n___% of samples will produce \\(\\hat{\\beta}_1\\) estimates within 1 st. err. of \\(\\beta_1\\)\n___% of samples will produce \\(\\hat{\\beta}_1\\) estimates within 2 st. err. of \\(\\beta_1\\)\n___% of samples will produce \\(\\hat{\\beta}_1\\) estimates within 3 st. err. of \\(\\beta_1\\)"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-8-increasing-sample-size",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-8-increasing-sample-size",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Exercise 8: Increasing sample size",
    "text": "Exercise 8: Increasing sample size\nNow that we have a sense of the potential variability and error in sample estimates, let’s consider the impact of sample size.\nSuppose we were to increase our sample size from n = 10 to n = 50 or n = 100 fish. What impact do you anticipate this having on our sample estimates of the population parameters:\n\nDo you expect there to be more or less variability among the sample model lines?\nAround what value would you expect the sampling distribution of sample slopes to be centered?\nWhat general shape would you expect that sampling distribution to have?\nIn comparison to estimates based on the samples of size 10, do you think the estimates based on samples of size 50 will be closer to or farther from the true slope (on average)?"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-9-500-samples-of-size-n",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-9-500-samples-of-size-n",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Exercise 9: 500 samples of size n",
    "text": "Exercise 9: 500 samples of size n\nLet’s increase the sample size in our simulation.\nFill in the blanks to take 500 samples of size 50, and build a sample model from each.\n\nset.seed(155)\nsample_models_50 &lt;- mosaic::do(___)*(\n  fish %&gt;% \n    ___(size = ___, replace = FALSE) %&gt;% \n    ___(___(Concen ~ Length))\n)\n\n# Check it out\nhead(sample_models_50)\n## Error in parse(text = input): &lt;text&gt;:2:33: unexpected input\n## 1: set.seed(155)\n## 2: sample_models_50 &lt;- mosaic::do(__\n##                                    ^\n\nSimilarly, take 500 samples of size 100, and build a sample model from each.\n\nset.seed(155)\nsample_models_100 &lt;- mosaic::do(___)*(\n  fish %&gt;% \n    ___(size = ___, replace = FALSE) %&gt;% \n    ___(___(Concen ~ Length))\n)\n\n# Check it out\nhead(sample_models_100)\n## Error in parse(text = input): &lt;text&gt;:2:34: unexpected input\n## 1: set.seed(155)\n## 2: sample_models_100 &lt;- mosaic::do(__\n##                                     ^"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-10-impact-of-sample-size-part-i",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-10-impact-of-sample-size-part-i",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Exercise 10: Impact of sample size (part I)",
    "text": "Exercise 10: Impact of sample size (part I)\nCompare and contrast the 500 sets of sample models when using samples of size 10, 50, and 100.\n\n# 500 sample models using samples of size 10\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_abline(data = sample_models_10, \n              aes(intercept = Intercept, slope = Length), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\n\n# 500 sample models using samples of size 50\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_abline(data = sample_models_50, \n              aes(intercept = Intercept, slope = Length), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n## Error: object 'sample_models_50' not found\n\n\n# 500 sample models using samples of size 100\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_abline(data = sample_models_100, \n              aes(intercept = Intercept, slope = Length), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n## Error: object 'sample_models_100' not found\n\n\nWhat happens to our sample models as sample size increases? Was this what you expected?"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-11-impact-of-sample-size-part-ii",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-11-impact-of-sample-size-part-ii",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Exercise 11: Impact of sample size (part II)",
    "text": "Exercise 11: Impact of sample size (part II)\nLet’s focus on just the sampling distributions of our 500 slope estimates \\(\\hat{\\beta}_1\\).\nFor easy comparison, plot the estimates based on samples of size 10, 50, and 100 on the same frame:\n\n# Don't think too hard about this code!\n# Combine the estimates & sample size into a new data set\n# Then plot it\n\ndata.frame(estimates = c(sample_models_10$Length, sample_models_50$Length, sample_models_100$Length),\n           sample_size = rep(c(\"10\",\"50\",\"100\"), each = 500)) %&gt;% \n  mutate(sample_size = fct_relevel(sample_size, c(\"10\", \"50\", \"100\"))) %&gt;% \n  ggplot(aes(x = estimates, color = sample_size)) + \n  geom_density() + \n  geom_vline(xintercept = 0.05813, color = \"red\", linetype = \"dashed\") + \n  labs(title = \"Sampling distributions of the sample slope\")\n## Error: object 'sample_models_50' not found\n\n\nHow do the shapes, centers, and spreads of these sampling distributions compare? Was this what you expected?"
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-12-properties-of-sampling-distributions",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#exercise-12-properties-of-sampling-distributions",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Exercise 12: Properties of sampling distributions",
    "text": "Exercise 12: Properties of sampling distributions\nIn light of your observations, complete the following statements about the sampling distribution of the sample slope.\n\nFor all sample sizes, the shape of the sampling distribution is roughly ___ and the sampling distribution is roughly centered around ___, the sample estimate from our original data.\nAs sample size increases:\nThe average sample slope estimate INCREASES / DECREASES / IS FAIRLY STABLE.\nThe standard error of the sample slopes INCREASES / DECREASES / IS FAIRLY STABLE.\nThus, as sample size increases, our sample slopes become MORE RELIABLE / LESS RELIABLE."
  },
  {
    "objectID": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#done",
    "href": "template_qmds/19-20-sampling-dist-clt-bootstrap-notes.html#done",
    "title": "Sampling distributions, the CLT, and Bootstrapping (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/17-logistic-multivariate-evaluation-notes.html",
    "href": "template_qmds/17-logistic-multivariate-evaluation-notes.html",
    "title": "Multiple logistic regression (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nConstruct multiple logistic regression models in R\nInterpret coefficients in multiple logistic regression models\nUse multiple logistic regression models to make predictions\nEvaluate the quality of logistic regression models by using predicted probability boxplots and by computing and interpreting accuracy, sensitivity, specificity, false positive rate, and false negative rate\n\n\n\n\nPlease go through the following reading or videos before class.\n\nReading: Section 4.4 in the STAT 155 Notes\nVideos:\n\nPart 1: Concepts (script)\nPart 2: R Code (script)\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/17-logistic-multivariate-evaluation-notes.html#learning-goals",
    "href": "template_qmds/17-logistic-multivariate-evaluation-notes.html#learning-goals",
    "title": "Multiple logistic regression (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nConstruct multiple logistic regression models in R\nInterpret coefficients in multiple logistic regression models\nUse multiple logistic regression models to make predictions\nEvaluate the quality of logistic regression models by using predicted probability boxplots and by computing and interpreting accuracy, sensitivity, specificity, false positive rate, and false negative rate"
  },
  {
    "objectID": "template_qmds/17-logistic-multivariate-evaluation-notes.html#readings-and-videos",
    "href": "template_qmds/17-logistic-multivariate-evaluation-notes.html#readings-and-videos",
    "title": "Multiple logistic regression (Notes)",
    "section": "",
    "text": "Please go through the following reading or videos before class.\n\nReading: Section 4.4 in the STAT 155 Notes\nVideos:\n\nPart 1: Concepts (script)\nPart 2: R Code (script)\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/17-logistic-multivariate-evaluation-notes.html#exercise-1-graphical-and-numerical-summaries",
    "href": "template_qmds/17-logistic-multivariate-evaluation-notes.html#exercise-1-graphical-and-numerical-summaries",
    "title": "Multiple logistic regression (Notes)",
    "section": "Exercise 1: Graphical and numerical summaries",
    "text": "Exercise 1: Graphical and numerical summaries\nOur research question involves three categorical variables: received_callback (1 = yes, 0 = no), gender (f = female, m = male), and race (Black, White). Let’s start by creating a mosaic plot to visually compare inferred binary gender and callbacks:\n\n# create mosaic plot of callback vs gender\nggplot(resume) + \n    geom_mosaic(aes(x = product(gender), fill = received_callback)) +\n    scale_fill_manual(\"Received Callback? \\n(1 = yes, 0 = no)\", values = c(\"lightblue\", \"steelblue\")) + \n    labs(x = \"Inferred Binary Gender (f = female, m = male)\", y = \"Received Callback? (1 = yes, 0 = no)\")\n\n\n\n\n\n\n\n\nIn this activity, we’re also interested in looking at the relationship between inferred race and callbacks. One way we can add a third variable to a plot is to use the facet_grid function, particularly when that third variable is categorical. Let’s try that now:\n\n# create mosaic plot of callback vs gender and race\nggplot(resume) + \n    geom_mosaic(aes(x = product(gender), fill = received_callback)) +\n    facet_grid(. ~ race) +\n    scale_fill_manual(\"Received Callback? \\n(1 = yes, 0 = no)\", values = c(\"lightblue\", \"steelblue\")) + \n    labs(x = \"Inferred Binary Gender (f = female, m = male)\", y = \"Received Callback? (1 = yes, 0 = no)\")\n\n\n\n\n\n\n\n\nHere’s another way of looking at the relationship between these three variables, switching the placement of gender and race in the mosaic plot:\n\n# create mosaic plot of callback vs gender and race\nggplot(resume) + \n    geom_mosaic(aes(x = product(received_callback, race), fill = received_callback)) +\n    facet_grid(. ~ gender) +\n    scale_fill_manual(\"Received Callback? \\n(1 = yes, 0 = no)\", values = c(\"lightblue\", \"steelblue\")) + \n    labs(x = \"Inferred Race\", y = \"Received Callback? (1 = yes, 0 = no)\")\n\n\n\n\n\n\n\n\nWhen we are comparing three categorical variables, a useful numerical summary is to calculate relative frequencies/proportions of cases falling into each category of the outcome variable, conditional on which categories of the explanatory variables they fall into. Run this code chunk to calculate the conditional proportion of resumes that did nor did not receive a callback, given the inferred gender and race of the applicant:\n\n# corresponding numerical summaries\nresume %&gt;%\n    group_by(race, gender) %&gt;%\n    count(received_callback) %&gt;%\n    group_by(race, gender) %&gt;%\n    mutate(condprop = n/sum(n))\n## # A tibble: 8 × 5\n## # Groups:   race, gender [4]\n##   race  gender received_callback     n condprop\n##   &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n## 1 black f                      0  1761   0.934 \n## 2 black f                      1   125   0.0663\n## 3 black m                      0   517   0.942 \n## 4 black m                      1    32   0.0583\n## 5 white f                      0  1676   0.901 \n## 6 white f                      1   184   0.0989\n## 7 white m                      0   524   0.911 \n## 8 white m                      1    51   0.0887\n\nWrite a short description that summarizes the information you gain from these visualizations and numerical summaries. Write this summary using good sentences that tell a story and do not resemble a checklist. Don’t forget to consider the context of the data, and make sure that your summary addresses our research question: does an applicant’s inferred gender or race have an effect on the chance that they receive a callback?"
  },
  {
    "objectID": "template_qmds/17-logistic-multivariate-evaluation-notes.html#exercise-2-logistic-regression-modeling",
    "href": "template_qmds/17-logistic-multivariate-evaluation-notes.html#exercise-2-logistic-regression-modeling",
    "title": "Multiple logistic regression (Notes)",
    "section": "Exercise 2: Logistic regression modeling",
    "text": "Exercise 2: Logistic regression modeling\nNext, we’ll fit a logistic regression model to these data, modeling the log odds of receiving a callback as a function of the applicant’s inferred gender and race:\n\\[\\log(Odds[ReceivedCallback = 1 \\mid gender, race]) = \\beta_0 + \\beta_1 genderm + \\beta_2 racewhite\\]\nFill in the blanks in the code below to fit this logistic regression model.\n\n# fit logistic model and save it as object called \"mod1\"\nmod1 &lt;- glm(received_callback ~ gender + race, data = ___, family = ___)\n## Error in parse(text = input): &lt;text&gt;:2:56: unexpected input\n## 1: # fit logistic model and save it as object called \"mod1\"\n## 2: mod1 &lt;- glm(received_callback ~ gender + race, data = __\n##                                                           ^\n\nThen, run the code chunk below to get the coefficient estimates and exponentiated estimates, presented in a nicely formatted table:\n\n# print out tidy summary of mod, focusing on estimates & exponentiated estimates\ntidy(mod1) %&gt;%\n    select(term, estimate) %&gt;%\n    mutate(estimate_exp = exp(estimate))\n## Error: object 'mod1' not found\n\nWrite an interpretation of each of the exponentiated coefficients in your logistic regression model."
  },
  {
    "objectID": "template_qmds/17-logistic-multivariate-evaluation-notes.html#exercise-3-interaction-terms",
    "href": "template_qmds/17-logistic-multivariate-evaluation-notes.html#exercise-3-interaction-terms",
    "title": "Multiple logistic regression (Notes)",
    "section": "Exercise 3: Interaction terms",
    "text": "Exercise 3: Interaction terms\n\nDo you think it would make sense to add an interaction term (between gender and race) to our logistic regression model? Why/why not?\nLet’s try adding an interaction between gender and race. Update the code below to fit this new interaction model.\n\n\n# fit logistic model and save it as object called \"mod2\"\nmod2 &lt;- glm(received_callback ~ ___, data = resume, family = ___)\n## Error in parse(text = input): &lt;text&gt;:2:34: unexpected input\n## 1: # fit logistic model and save it as object called \"mod2\"\n## 2: mod2 &lt;- glm(received_callback ~ __\n##                                     ^\n\nThen, run the code chunk below to get the coefficient estimates and exponentiated estimates for this interaction model, presented in a nicely formatted table:\n\n# print out tidy summary of mod, focusing on estimates & exponentiated estimates\ntidy(mod2) %&gt;%\n    select(term, estimate) %&gt;%\n    mutate(estimate_exp = exp(estimate))\n## Error: object 'mod2' not found\n\n\n(CHALLENGE) Write out the logistic regression model formula separately for males and for females. Based on this how would we interpret the exponentiated coefficients in this model?"
  },
  {
    "objectID": "template_qmds/17-logistic-multivariate-evaluation-notes.html#exercise-4-prediction",
    "href": "template_qmds/17-logistic-multivariate-evaluation-notes.html#exercise-4-prediction",
    "title": "Multiple logistic regression (Notes)",
    "section": "Exercise 4: Prediction",
    "text": "Exercise 4: Prediction\nWe can use our models to predict whether or not a resume will receive a call back based on the inferred gender and race of the applicant. Run the code below to use the predict() function to predict the probability of getting a call back for four job applicants: a person inferred to be a black female, a person inferred to be black male, a person inferred to be a white female, and a person inferred to be a white male.\n\n# set up data frame with people we want to predict for\npredict_data &lt;- data.frame(\n    gender = c(\"f\", \"m\", \"f\", \"m\"),\n    race = c(\"black\", \"black\", \"white\", \"white\")\n)\nprint(predict_data)\n##   gender  race\n## 1      f black\n## 2      m black\n## 3      f white\n## 4      m white\n\n# prediction based on model without interaction\nmod1 %&gt;%\n    predict(newdata = predict_data, type = \"response\")\n## Error: object 'mod1' not found\n\n# prediction based on model with interaction\nmod2 %&gt;%\n    predict(newdata = predict_data, type = \"response\")\n## Error: object 'mod2' not found\n\nReport and compare the predictions we get from predict(). Do they make sense to you based on your understanding of the data? Combine insights from visualizations and modeling to write a few sentences summarizing findings for our research question: does an applicant’s inferred gender and race have an effect on the chance that they receive a callback after submitting their resume for an open job posting?"
  },
  {
    "objectID": "template_qmds/17-logistic-multivariate-evaluation-notes.html#exercise-5-evaluating-logistic-models-with-plots",
    "href": "template_qmds/17-logistic-multivariate-evaluation-notes.html#exercise-5-evaluating-logistic-models-with-plots",
    "title": "Multiple logistic regression (Notes)",
    "section": "Exercise 5: Evaluating logistic models with plots",
    "text": "Exercise 5: Evaluating logistic models with plots\nWe’ll fit one more model that adds on to the interaction model to also include years of college, years of work experience, and resume quality. The augment() code takes our fitted models and stores the predicted probabilities in a variable called .fitted. Then we use boxplots to show the predicted probabilities of receiving a callback in those who actually did and did not receive a callback.\n\nmod3 &lt;- glm(received_callback ~ gender*race + years_college + years_experience + resume_quality, data = resume, family = \"binomial\")\n\nmod1_output &lt;- augment(mod1, type.predict = \"response\") # Store predicted probabilities in a variable called .fitted\n## Error: object 'mod1' not found\nmod2_output &lt;- augment(mod2, type.predict = \"response\")\n## Error: object 'mod2' not found\nmod3_output &lt;- augment(mod3, type.predict = \"response\")\n\nggplot(mod1_output, aes(x = factor(received_callback), y = .fitted)) +\n    geom_boxplot()\n## Error: object 'mod1_output' not found\nggplot(mod2_output, aes(x = factor(received_callback), y = .fitted)) +\n    geom_boxplot()\n## Error: object 'mod2_output' not found\nggplot(mod3_output, aes(x = factor(received_callback), y = .fitted)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nSummarize what you learn about the ability of the 3 models to differentiate those who actually did and did not receive a callback. What model seems best, and why?\nIf you had to draw a horizontal line across each of the boxplots that vertically separates the left and right boxplots well, where would you place them?"
  },
  {
    "objectID": "template_qmds/17-logistic-multivariate-evaluation-notes.html#exercise-6-evaluating-logistic-models-with-evaluation-metrics",
    "href": "template_qmds/17-logistic-multivariate-evaluation-notes.html#exercise-6-evaluating-logistic-models-with-evaluation-metrics",
    "title": "Multiple logistic regression (Notes)",
    "section": "Exercise 6: Evaluating logistic models with evaluation metrics",
    "text": "Exercise 6: Evaluating logistic models with evaluation metrics\nSometimes we may need to go beyond the predicted probabilities from our model and try to classify individuals into one of the two binary outcomes (received or did not receive a callback). How high of a predicted probability would we need from our model in order to be convinced that the person actually got a callback? This is the idea behind the horizontal lines that we drew in the previous exercise.\nLet’s explore using a probability threshold of 0.08 (8%) to make a binary prediction for each case:\n\nIf a model’s predicted probability of getting a callback is greater than or equal to 8.5%, we’ll predict they got a callback.\nIf the predicted probability is below 8%, we’ll predict they didn’t get a callback.\n\nWe can visualize this threshold on our predicted probability boxplots:\n\nggplot(mod1_output, aes(x = factor(received_callback), y = .fitted)) +\n    geom_boxplot() +\n    geom_hline(yintercept = 0.08, color = \"red\")\n## Error: object 'mod1_output' not found\nggplot(mod2_output, aes(x = factor(received_callback), y = .fitted)) +\n    geom_boxplot() +\n    geom_hline(yintercept = 0.08, color = \"red\")\n## Error: object 'mod2_output' not found\nggplot(mod3_output, aes(x = factor(received_callback), y = .fitted)) +\n    geom_boxplot() +\n    geom_hline(yintercept = 0.08, color = \"red\")\n\n\n\n\n\n\n\n\nNext, we can use our threshold to classify each person in our dataset based on their predicted probability of getting a callback: we’ll predict that everyone with a predicted probability higher than our threshold got a callback, and otherwise they did not. Then, we’ll compare our model’s prediction to the true outcome (whether or not they actually did get a callback).\n\n# get binary predictions for mod1 and compare to truth\nthreshold &lt;- 0.08\nmod1_output %&gt;%\n    mutate(predictCallback = .fitted &gt;= threshold) %&gt;% ## predict callback if probability greater than or equal to threshold\n    count(received_callback, predictCallback) ## compare actual and predicted callbacks\n## Error: object 'mod1_output' not found\n\nmod2_output %&gt;%\n    mutate(predictCallback = .fitted &gt;= threshold) %&gt;%\n    count(received_callback, predictCallback)\n## Error: object 'mod2_output' not found\n\nmod3_output %&gt;%\n    mutate(predictCallback = .fitted &gt;= threshold) %&gt;%\n    count(received_callback, predictCallback)\n## # A tibble: 4 × 3\n##   received_callback predictCallback     n\n##               &lt;dbl&gt; &lt;lgl&gt;           &lt;int&gt;\n## 1                 0 FALSE            2465\n## 2                 0 TRUE             2013\n## 3                 1 FALSE             159\n## 4                 1 TRUE              233\n\nWe can use the count() output to fill create contingency tables of the results. (These tables are also called confusion matrices.)\n\nFill in the confusion matrix for Model 3.\n\n\nModels 1 and 2: (Both models result in the same confusion matrix.)\n\n\n\n\n\nPredict callback\nPredict no callback\nTotal\n\n\n\n\nActually got callback\n235\n157\n392\n\n\nActually did not\n2200\n2278\n4478\n\n\nTotal\n2435\n2435\n4870\n\n\n\n\nModel 3:\n\n\n\n\n\nPredict callback\nPredict no callback\nTotal\n\n\n\n\nActually got callback\n____\n____\n____\n\n\nActually did not\n____\n____\n____\n\n\nTotal\n____\n____\n____\n\n\n\n\nNow compute the following evaluation metrics for the models:\n\nModels 1 and 2:\n\nAccuracy: P(Predict Y Correctly)\nSensitivity: P(Predict Y = 1 | Actual Y = 1)\nSpecificity: P(Predict Y = 0 | Actual Y = 0)\nFalse negative rate: P(Predict Y = 0 | Actual Y = 1)\nFalse positive rate: P(Predict Y = 1 | Actual Y = 0)\n\nModel 3:\n\nAccuracy: P(Predict Y Correctly)\nSensitivity: P(Predict Y = 1 | Actual Y = 1)\nSpecificity: P(Predict Y = 0 | Actual Y = 0)\nFalse negative rate: P(Predict Y = 0 | Actual Y = 1)\nFalse positive rate: P(Predict Y = 1 | Actual Y = 0)\n\n\nImagine that we are a career center on a college campus and we want to use this model to help students that are looking for jobs. Consider the consequences of incorrectly predicting whether or not an individual will get a callback. What are the consequences of a false negative? What about a false positive? Which one is worse?"
  },
  {
    "objectID": "template_qmds/17-logistic-multivariate-evaluation-notes.html#reflection",
    "href": "template_qmds/17-logistic-multivariate-evaluation-notes.html#reflection",
    "title": "Multiple logistic regression (Notes)",
    "section": "Reflection",
    "text": "Reflection\nWhat are some similarities and differences between how we interpret and evaluate linear and logistic regression models?\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/17-logistic-multivariate-evaluation-notes.html#done",
    "href": "template_qmds/17-logistic-multivariate-evaluation-notes.html#done",
    "title": "Multiple logistic regression (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/15-prob-odds-notes.html",
    "href": "template_qmds/15-prob-odds-notes.html",
    "title": "Probability & Odds (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDistinguish between probabilities and odds, and convert one to the other\nMake appropriate visualizations for displaying relationships between multiple categorical variables (mosaic plots, stacked bar plots, etc.)\n\n\n\n\nGo through the following reading or videos before class:\n\nReading: Sections 2.5, and the Section 6.2 introduction in the STAT 155 Notes\nVideos:\n\nProb vs. Odds vs. Log Odds (script)\nCalculating Probability and Odds from 2x2 Tables (script)\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/15-prob-odds-notes.html#learning-goals",
    "href": "template_qmds/15-prob-odds-notes.html#learning-goals",
    "title": "Probability & Odds (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDistinguish between probabilities and odds, and convert one to the other\nMake appropriate visualizations for displaying relationships between multiple categorical variables (mosaic plots, stacked bar plots, etc.)"
  },
  {
    "objectID": "template_qmds/15-prob-odds-notes.html#readings-and-videos",
    "href": "template_qmds/15-prob-odds-notes.html#readings-and-videos",
    "title": "Probability & Odds (Notes)",
    "section": "",
    "text": "Go through the following reading or videos before class:\n\nReading: Sections 2.5, and the Section 6.2 introduction in the STAT 155 Notes\nVideos:\n\nProb vs. Odds vs. Log Odds (script)\nCalculating Probability and Odds from 2x2 Tables (script)\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/15-prob-odds-notes.html#exercise-1-exploring-first-steps-enrollment-and-gestational-age",
    "href": "template_qmds/15-prob-odds-notes.html#exercise-1-exploring-first-steps-enrollment-and-gestational-age",
    "title": "Probability & Odds (Notes)",
    "section": "Exercise 1: Exploring First Steps enrollment and Gestational Age",
    "text": "Exercise 1: Exploring First Steps enrollment and Gestational Age\nA baby born prior to 37 weeks is considered premature. In figuring out whether we have evidence that the First Steps program is associated with better birth outcomes than those not in the First Steps program, we can look at whether the individuals in the program are more likely to have preterm babies.\nBelow, we make a 2x2 table in R:\n\n# 2x2 Table: preterm vs. First Steps\nfirststeps %&gt;% \n    count(preterm, firstep)\n## # A tibble: 4 × 3\n##   preterm firstep     n\n##   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n## 1 No            0  1879\n## 2 No            1   343\n## 3 Yes           0   218\n## 4 Yes           1    60\n\nYou may be wondering why this is called a 2x2 table, when it looks as though the table has four rows and three columns. The data can be re-arranged (and usually is, in a formal report) as follows…\n\ntable(firststeps$preterm, firststeps$firstep)\n##      \n##          0    1\n##   No  1879  343\n##   Yes  218   60\n\n… but it’s much cleaner to code up the original way!\n\nHow many birth parents were enrolled in the First Steps program? Which rows did you use to calculate this number?\nWhat percentage of people in the study were enrolled in the First Steps program? Recall: there were 2500 participants! You can confirm this by adding up the entire third column of the table\nHow many birth parents who were enrolled in First Steps had a premature baby?\nWhat percentage of birth parents in First Steps had a premature baby? Think carefully about the numerator and denominator you use to calculate this!\nWhat percentage of birth parents who had a premature baby were enrolled in First Steps? Think carefully about the numerator and denominator you use to calculate this!\n\nCongratulations! If you’ve made it to this point, you already intuitively know what marginal and conditional probabilities are. Formally,\n\na marginal probability, denoted \\(P(A)\\) for an event \\(A\\), is the probability that \\(A\\) occurs overall. You calculated the marginal probability that people were enrolled in First Steps in part (b)! In this case, the denominator used to calculate the probability was the total number of people in the study.\na conditional probability, denoted \\(P(A | B)\\) for events \\(A\\) and \\(B\\), is the probability that \\(A\\) occurs given that event \\(B\\) occurs. You calculated the conditional probability that a premature baby was born given that a parent was in First Steps in part (d)! In this case, the denominator used to calculate the probability was the total number of birth parents in the First Steps program. You also calculated a conditional probability in part (e).\n\nUsing formal probability notation, write the probabilities you calculated in parts (b), (d), and (e) as\n\n\n\\(P(\\text{First Steps})\\) = ___\n\n\n\n\n\\(P(\\text{Preterm} | \\text{First Steps})\\) = ___\n\n\n\n\n\\(P(___ | ___)\\) = __\n\n\nNote that the conditional probabilities calculated in parts (d) and (e) are not the same! This is because which event you condition on alters the denominator, and the event you’re interested in alters the numerator.\n\nTo determine if gestational age differed by enrollment in First Steps, we’ll want to calculate the conditional probability that a baby is born prematurely given First Steps enrollment (done!), and given that a parent is not enrolled in First Steps. Use the 2x2 table to calculated this conditional probability.\n\n\n\\(P(\\text{Preterm} |\\text{Not in First Steps})\\) = ___\n\n\nA ratio of conditional probabilities, where the conditioning event is the same for both, tells us how many times more likely an event is to occur for one group compared to another. Calculate how many times more likely a birth parent enrolled in First Steps is to have a premature baby compared to birth parents not enrolled in First Steps.\n\n\\[\n\\frac{(\\text{Preterm} | \\text{First Steps})}{P(\\text{Preterm} | \\text{Not in First Steps})} =\n\\]\n\nWrite a two-sentence summary, appropriate for a general audience, summarizing your results in terms of a ratio of probabilities. Does gestational age appear to differ greatly by First Steps enrollment? What does this imply about the effectiveness of the First Steps program, if anything?\nTo go along with your summary, let’s make a visualization! There are three basic options for visualization two categorical variables. All are perfectly valid, but some may be more useful to read than others, and display different information.\n\nYou’ll see one other fancier option (called a mosaic plot) in the next activity.\n\n# Side-by-side bar chart\nfirststeps %&gt;%\n  ggplot(aes(firstep, fill = preterm)) +\n  geom_bar(position = \"dodge\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n# Stacked bar chart\nfirststeps %&gt;%\n  ggplot(aes(firstep, fill = preterm)) +\n  geom_bar() +\n  theme_classic()\n\n\n\n\n\n\n\n\n# Stacked relative frequency bar chart\nfirststeps %&gt;%\n  ggplot(aes(firstep, fill = preterm)) +\n  geom_bar(position = \"fill\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nBonus Question: Which of the above three plots allows you to directly see the conditional probabilities we calculated previously?\n\nPlace your answer here"
  },
  {
    "objectID": "template_qmds/15-prob-odds-notes.html#exercise-2-exploring-first-steps-enrollment-and-low-birthweights",
    "href": "template_qmds/15-prob-odds-notes.html#exercise-2-exploring-first-steps-enrollment-and-low-birthweights",
    "title": "Probability & Odds (Notes)",
    "section": "Exercise 2: Exploring First Steps enrollment and Low birthweights",
    "text": "Exercise 2: Exploring First Steps enrollment and Low birthweights\nAnother birth outcome we can consider when comparing those enrolled in the First Steps program to those not enrolled is birth weight. A baby is considered to have low birth weight when birth weight is less than 2500 grams.\n\nFill in the code below to make a table comparing low_bwt to firsteps.\n\n\n# 2x2 Table: low_bwt vs. First Steps\n\n\nUsing the table from part (a), calculate the following conditional probabilities:\n\n\n\\(P(\\text{Low birth weight} | \\text{First Steps})\\) = ___\n\n\n\\(P(\\text{Normal birth weight} | \\text{First Steps})\\) = ___\n\n\n\\(P(\\text{Low birth weight} | \\text{Not in First Steps})\\) = ___\n\n\n\\(P(\\text{Normal birth weight} | \\text{Not in First Steps})\\) = ___\n\nAn additional numerical summary that is often useful when working with indicator variables is odds. Odds are defined as\n\\[\nOdds = \\frac{p}{1 - p}\n\\]\nwhere \\(p\\) is the probability that an event occurs. Therefore, if we know \\(p\\), we can calculate the odds that an event happens! Similarly, if we know the odds, we can calculate \\(p\\) using\n\\[\np = Odds / (1 + Odds)\n\\]\nWe can also calculate odds from our 2x2 (or 3x2, 4x2, …) tables. In colloquial terms, probabilities are “yes”’s over “total”’s, and odds are “yes”’s over “no’s”. In pseudo-math:\n\\[\np = \\frac{Yes}{Total}, \\quad Odds = \\frac{Yes}{No}\n\\] We’ll see why odds are especially useful when we have binary outcome variables in a regression model in the next activity. For now, note that they’re also commonly used in lots of contexts: sports, gambling, case-control studies, etc.\n\nUsing your answer to part (b), calculate the following odds\n\n\n\\(Odds(\\text{Low birth weight} | \\text{First Steps})\\) = ___\n\n\n\\(Odds(\\text{Normal birth weight} | \\text{First Steps})\\) = ___\n\n\n\\(Odds(\\text{Low birth weight} | \\text{Not in First Steps})\\) = ___\n\n\n\\(Odds(\\text{Normal birth weight} | \\text{Not in First Steps})\\) = ___\n\n\nA ratio of odds (called an odds ratio, unsurprisingly) tells us how many times higher or greater the odds are that an event occurs, comparing one group to another. This might sound irritatingly circular. The key here is that while odds ratios do allow us to compare binary/indicator outcomes from one group to one another, they do not tell us how much more likely an event is to occur comparing those same groups. This is distinct from ratios of probabilities!\n\nCalculate the ratio of the odds of having a low-birth-weight baby, comparing those in the First Steps program to those not in the First Steps program (i.e., how many times higher/lower is the odds of having a low-birth-weight baby among those in First Steps as compared to those not in First Steps?)\n\nWrite a two-sentence summary, appropriate for a general audience, summarizing your results in terms of an odds ratio. Does birth weight appear to differ greatly by First Steps enrollment? What does this imply about the effectiveness of the First Steps program, if anything?\nTo go along with your summary, add code below to make one of the three visualization options we tried out in Exercise 1.\n\n\n# Insert code here..."
  },
  {
    "objectID": "template_qmds/15-prob-odds-notes.html#reflection",
    "href": "template_qmds/15-prob-odds-notes.html#reflection",
    "title": "Probability & Odds (Notes)",
    "section": "Reflection",
    "text": "Reflection\nPrompt\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/15-prob-odds-notes.html#render-your-work",
    "href": "template_qmds/15-prob-odds-notes.html#render-your-work",
    "title": "Probability & Odds (Notes)",
    "section": "Render your work",
    "text": "Render your work\n\nClick the “Render” button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\nScroll through and inspect the document to check that your work translated to the HTML format correctly.\nClose the browser tab.\nGo to the “Background Jobs” pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your “Activities” subfolder within your “STAT155” folder and locate the HTML file. You can open it again in your browser to double check."
  },
  {
    "objectID": "template_qmds/15-prob-odds-notes.html#exercise-3-conditional-vs.-marginal-probabilities",
    "href": "template_qmds/15-prob-odds-notes.html#exercise-3-conditional-vs.-marginal-probabilities",
    "title": "Probability & Odds (Notes)",
    "section": "Exercise 3: Conditional vs. Marginal probabilities",
    "text": "Exercise 3: Conditional vs. Marginal probabilities\nSuppose we select a person at random from the entire global population. For each of the following probabilities, which do you think is bigger? Explain your reasoning.\n\nP(lung cancer) or P(lung cancer | smoker)\n\n\nYour response here\n\n\nP(likes McDonald’s) or P(likes McDonald’s | vegetarian)\n\n\nYour response here\n\n\nP(smart | Mac grad) or P(Mac grad | smart)\n\n\nYour response here"
  },
  {
    "objectID": "template_qmds/15-prob-odds-notes.html#exercise-4-probability-practice",
    "href": "template_qmds/15-prob-odds-notes.html#exercise-4-probability-practice",
    "title": "Probability & Odds (Notes)",
    "section": "Exercise 4: Probability practice",
    "text": "Exercise 4: Probability practice\nLet’s explore whether birthweight of a baby varies by whether or not it was the first child that a mother had, and whether this relationship differs by First Steps enrollment. We make a table below:\n\nfirststeps %&gt;%\n  count(firstchild, low_bwt, firstep)\n## # A tibble: 8 × 4\n##   firstchild low_bwt firstep     n\n##   &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n## 1 No         low           0    43\n## 2 No         low           1    13\n## 3 No         not low       0  1080\n## 4 No         not low       1   198\n## 5 Yes        low           0    59\n## 6 Yes        low           1    12\n## 7 Yes        not low       0   915\n## 8 Yes        not low       1   180\n\n\nWhat is the probability that a mother enrolled in First steps who is having their first child, has a baby who is born at a low birthweight? Calculate your answer, and write it using formal probability notation.\n\n\nP(___ | ___) = ?\n\n\nWhat is the probability that a mother not enrolled in First steps who is having their first child, has a baby who is born at a low birthweight? Calculate your answer, and write it using formal probability notation.\n\n\nP(___ | ___) = ?\n\n\nWhat is the probability that a mother’s first child has a low birthweight? Calculate your answer, and write it using formal probability notation.\n\n\nP(___ | ___) = ?\n\n\nHow many times more likely is a child to be born at a low birthweight, comparing children who are the first born to those not first born?\n\n\nP(___ | ) / P( | ___) = ?"
  },
  {
    "objectID": "template_qmds/15-prob-odds-notes.html#done",
    "href": "template_qmds/15-prob-odds-notes.html#done",
    "title": "Probability & Odds (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/13-mlr-model-building-1-notes.html",
    "href": "template_qmds/13-mlr-model-building-1-notes.html",
    "title": "Multiple linear regression: model building (part 1) (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDistinguish between descriptive, predictive, and causal research questions\nIterate on your group’s research question to make it more precise and answerable\nChoose appropriate model(s) for addressing your group’s research question\n\n\n\n\nPlease watch the following video before class.\n\nVideo: Causal Diagrams and Confounding Variables\n\nFile organization: If you would like to take notes in this document, download the template and save it in the “Activities” subfolder of your “STAT155” folder. You are more than welcome to take notes in a separate google document, shared with your project group, if you’d find that more useful!"
  },
  {
    "objectID": "template_qmds/13-mlr-model-building-1-notes.html#learning-goals",
    "href": "template_qmds/13-mlr-model-building-1-notes.html#learning-goals",
    "title": "Multiple linear regression: model building (part 1) (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDistinguish between descriptive, predictive, and causal research questions\nIterate on your group’s research question to make it more precise and answerable\nChoose appropriate model(s) for addressing your group’s research question"
  },
  {
    "objectID": "template_qmds/13-mlr-model-building-1-notes.html#readings-and-videos",
    "href": "template_qmds/13-mlr-model-building-1-notes.html#readings-and-videos",
    "title": "Multiple linear regression: model building (part 1) (Notes)",
    "section": "",
    "text": "Please watch the following video before class.\n\nVideo: Causal Diagrams and Confounding Variables\n\nFile organization: If you would like to take notes in this document, download the template and save it in the “Activities” subfolder of your “STAT155” folder. You are more than welcome to take notes in a separate google document, shared with your project group, if you’d find that more useful!"
  },
  {
    "objectID": "template_qmds/13-mlr-model-building-1-notes.html#step-1-review",
    "href": "template_qmds/13-mlr-model-building-1-notes.html#step-1-review",
    "title": "Multiple linear regression: model building (part 1) (Notes)",
    "section": "Step 1: Review",
    "text": "Step 1: Review\nTake a look at the first project checkpoint (your statistical analysis plan) that your group submitted. As part of this checkpoint, you should have come up with a research question. Record the research question you came up with: we’ll iterate on this question throughout the activity!\n\nRecord your research question here\n\nAnswer the following questions as a group (some of this may already be in your statistical analysis plan!):\n\nWho would be interested in the answer to this question?\nWhat variables do you need in a dataset to address this question?\nWhat data summaries (not models) would help you answer this question, and why?\nWhat plots (not models) would help you address your research question, and why?"
  },
  {
    "objectID": "template_qmds/13-mlr-model-building-1-notes.html#step-2-descriptive-research-questions",
    "href": "template_qmds/13-mlr-model-building-1-notes.html#step-2-descriptive-research-questions",
    "title": "Multiple linear regression: model building (part 1) (Notes)",
    "section": "Step 2: Descriptive Research Questions",
    "text": "Step 2: Descriptive Research Questions\nDescriptive research questions are questions that seek to better understand the relationships between variables, without interest in causality. In practice, nearly every research question asked is ultimately interested in causality, but practical constraints (such as unmeasured confounding) lead us to ask descriptive questions instead.\nIf we’re only interested in associations (not causality), we don’t need to adjust for potential confounding variables in our model.\n\nFor your group’s chosen research question, write a model statement that would address a descriptive version of your research question below:\n\nModel statement for a descriptive question here"
  },
  {
    "objectID": "template_qmds/13-mlr-model-building-1-notes.html#step-3-predictive-research-questions",
    "href": "template_qmds/13-mlr-model-building-1-notes.html#step-3-predictive-research-questions",
    "title": "Multiple linear regression: model building (part 1) (Notes)",
    "section": "Step 3: Predictive Research Questions",
    "text": "Step 3: Predictive Research Questions\nPredictive research questions seek to determine if (and how well) we can predict outcomes for new / future events, using the information we already have. We’ve seen a bit of prediction in this course when we talked about fitted values!\n\nWith your groups, discuss the following:\n\nIs your research question predictive, or inferential? Inferential questions seek to understand the relationships between variables.\nIf your question were predictive, who would be interested/invested in the results from your project? How could the results from your project be used in practice?\nAre there any variables that are not available to you in your data that you would include in your predictive model if you could? Why or why not?"
  },
  {
    "objectID": "template_qmds/13-mlr-model-building-1-notes.html#step-3-causal-research-questions",
    "href": "template_qmds/13-mlr-model-building-1-notes.html#step-3-causal-research-questions",
    "title": "Multiple linear regression: model building (part 1) (Notes)",
    "section": "Step 3: Causal Research Questions",
    "text": "Step 3: Causal Research Questions\nCausal research questions are ultimately what most inferential statistics is interested in, regardless of whether or not we end up being able to make causal conclusions. From the videos for today, you learned about different types of variables, and whether or not they should be included or excluded from a model, depending on your causal research question.\nWith your groups, make a causal diagram (DAG) on the whiteboard for your research question. Consider including all variables you wish you had access to, even if they aren’t available in your data (this will help you later when talking about limitations of your analysis in your final paper), but certainly include relevant variables that are available in your data.\nFor each variable in your DAG that is available in your dataset, determine whether it should be included or excluded from your model. Use this to update your descriptive model statement from Step 2.\n\nModel statement for a causal question here\n\nNow look back at your DAG, and note if any of the variables that are not available in your data are potential confounders. If so, record them here (this means you likely won’t be able to draw causal conclusions):\n\nList of “unmeasured” confounding variables here"
  },
  {
    "objectID": "template_qmds/13-mlr-model-building-1-notes.html#step-4-reflection",
    "href": "template_qmds/13-mlr-model-building-1-notes.html#step-4-reflection",
    "title": "Multiple linear regression: model building (part 1) (Notes)",
    "section": "Step 4: Reflection",
    "text": "Step 4: Reflection\nToday was all about iterating on a research question, and using those questions to guide the way we explore data and fit statistical models. How confident do you feel in distinguishing between descriptive, predictive, and causal research questions? How confident do you feel in knowing which components of a model matter more or less, in each specific case? What might help you feel more confident?\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/13-mlr-model-building-1-notes.html#done",
    "href": "template_qmds/13-mlr-model-building-1-notes.html#done",
    "title": "Multiple linear regression: model building (part 1) (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/11-mlr-interaction-explore-notes.html",
    "href": "template_qmds/11-mlr-interaction-explore-notes.html",
    "title": "Multiple linear regression: exploring interaction (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe when it would be useful to include an interaction term to a model\nWrite a model formula for an interaction model\nInterpret the coefficients in an interaction model in the data context\n\n\n\n\nToday is a day to discover ideas, so no readings or videos to go through before class.\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/11-mlr-interaction-explore-notes.html#learning-goals",
    "href": "template_qmds/11-mlr-interaction-explore-notes.html#learning-goals",
    "title": "Multiple linear regression: exploring interaction (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe when it would be useful to include an interaction term to a model\nWrite a model formula for an interaction model\nInterpret the coefficients in an interaction model in the data context"
  },
  {
    "objectID": "template_qmds/11-mlr-interaction-explore-notes.html#readings-and-videos",
    "href": "template_qmds/11-mlr-interaction-explore-notes.html#readings-and-videos",
    "title": "Multiple linear regression: exploring interaction (Notes)",
    "section": "",
    "text": "Today is a day to discover ideas, so no readings or videos to go through before class.\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/11-mlr-interaction-explore-notes.html#exercise-1-wages-across-all-industries",
    "href": "template_qmds/11-mlr-interaction-explore-notes.html#exercise-1-wages-across-all-industries",
    "title": "Multiple linear regression: exploring interaction (Notes)",
    "section": "Exercise 1: Wages across all industries",
    "text": "Exercise 1: Wages across all industries\nThe plot below illustrates the relationship between wage and education for all of the industries in our cps dataset.\n\n# Plot\nggplot(cps, aes(y = wage, x = education, color = industry)) + \n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\nWhat about this plot indicates that it would be a good idea to fit an interaction model?\nWhat industry will R use as the reference category?\n(Challenge!) Before fitting the model in R, write down what you think the model formula will look like.\nFit a model that includes an interaction term between education and industry.\n\n\n# Fit an interaction model called wage_model\n\n\n# Display summarized model output\n\n\nIn what industry do wages increase the most per additional year of education? What is this increase?\nSimilarly, in what industry do wages increase the least per additional year of education? What is this increase?"
  },
  {
    "objectID": "template_qmds/11-mlr-interaction-explore-notes.html#exercise-2-thinking-beyond",
    "href": "template_qmds/11-mlr-interaction-explore-notes.html#exercise-2-thinking-beyond",
    "title": "Multiple linear regression: exploring interaction (Notes)",
    "section": "Exercise 2: Thinking beyond",
    "text": "Exercise 2: Thinking beyond\nDo you think there are other variables (which may or may not be in our cps data) that have an interaction with industry in affecting wages? If you were to fit an interaction model, what results might you expect to find?"
  },
  {
    "objectID": "template_qmds/11-mlr-interaction-explore-notes.html#reflection",
    "href": "template_qmds/11-mlr-interaction-explore-notes.html#reflection",
    "title": "Multiple linear regression: exploring interaction (Notes)",
    "section": "Reflection",
    "text": "Reflection\nThrough the exercises above, we developed ideas about when to fit interaction models and how to interpret results. Describe what makes sense and what is still unclear about this topic.\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/11-mlr-interaction-explore-notes.html#done",
    "href": "template_qmds/11-mlr-interaction-explore-notes.html#done",
    "title": "Multiple linear regression: exploring interaction (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html",
    "href": "template_qmds/09-mlr-principles-notes.html",
    "title": "Multiple regression principles (Notes)",
    "section": "",
    "text": "Working with multiple predictors in our plots and models can get complicated!\nThere are no recipes for this process.\nBUT there are some guiding principles that assist in long-term retention, deeper understanding, and the ability to generalize our tools in new settings.\nBy the end of this lesson, you should be familiar with some general principles for…\n\nincorporating additional quantitative or categorical predictors in a visualization\nhow additional quantitative or categorical predictors impact the physical representation of a model\ninterpreting quantitative or categorical coefficients in a multiple regression model\n\n\n\n\nPlease watch the following video before class.\n\nInterpreting multivariate models (slides)"
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#learning-goals",
    "href": "template_qmds/09-mlr-principles-notes.html#learning-goals",
    "title": "Multiple regression principles (Notes)",
    "section": "",
    "text": "Working with multiple predictors in our plots and models can get complicated!\nThere are no recipes for this process.\nBUT there are some guiding principles that assist in long-term retention, deeper understanding, and the ability to generalize our tools in new settings.\nBy the end of this lesson, you should be familiar with some general principles for…\n\nincorporating additional quantitative or categorical predictors in a visualization\nhow additional quantitative or categorical predictors impact the physical representation of a model\ninterpreting quantitative or categorical coefficients in a multiple regression model"
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#readings-and-videos",
    "href": "template_qmds/09-mlr-principles-notes.html#readings-and-videos",
    "title": "Multiple regression principles (Notes)",
    "section": "",
    "text": "Please watch the following video before class.\n\nInterpreting multivariate models (slides)"
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-1-review-visualization",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-1-review-visualization",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 1: Review visualization",
    "text": "Exercise 1: Review visualization\nLet’s build a model of rides by windspeed (quantitative) and weekend status (categorical).\n\nWrite a model statement for this regression model.\nPlot & describe, in words, the relationship between these 3 variables.\n\n\n# Plot of rides vs windspeed & weekend\n# HINT: Start with a plot of rides vs windspeed, then add an aesthetic for weekend!"
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-2-review-model",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-2-review-model",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 2: Review model",
    "text": "Exercise 2: Review model\nLet’s build the model. Run the following code:\n\nbike_model_1 &lt;- lm(rides ~ windspeed + weekend, data = bikes)\ncoef(summary(bike_model_1))\n##               Estimate Std. Error   t value      Pr(&gt;|t|)\n## (Intercept) 4738.38053  147.53653 32.116659 1.208405e-141\n## windspeed    -63.97072   10.45274 -6.119997  1.528443e-09\n## weekendTRUE -925.15701  119.86330 -7.718434  3.891082e-14\n\nThe model formula with our coefficient estimates filled in is therefore:\nE[rides | windspeed, weekendTRUE] = 4738.38 - 63.97 * windspeed - 925.16 * weekendTRUE\nThis model formula is represented by 2 lines, one corresponding to weekends and the other to weekdays. Simplify the model formula above for weekdays and weekends:\nweekdays: rides = ___ - ___ windspeed\nweekends: rides = ___ - ___ windspeed"
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-3-review-coefficient-interpretation",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-3-review-coefficient-interpretation",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 3: Review coefficient interpretation",
    "text": "Exercise 3: Review coefficient interpretation\n\nThe intercept coefficient, 4738.38, represents the intercept of the sub-model for weekdays, the reference category. What’s its contextual interpretation?\nThe windspeed coefficient, -63.97, represents the shared slope of the weekend and weekday sub-models. What’s its contextual interpretation?\nThe weekendTRUE coefficient, -925.16, represents the change in intercept for the weekend vs weekday sub-model. What’s its contextual interpretation?"
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-4-2-categorical-predictors-visualization",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-4-2-categorical-predictors-visualization",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 4: 2 categorical predictors – visualization",
    "text": "Exercise 4: 2 categorical predictors – visualization\nThus far, we’ve explored a couple examples of multiple regression models that have 2 predictors, 1 quantitative and 1 categorical.\nSo what happens when both predictors are categorical?!\nTo this end, let’s model rides by weekend status and season.\nThe below code plots rides vs season.\nModify this code to also include information about weekend.\nHINT: Remember the visualization principle that additional categorical predictors require some sort of grouping mechanism / mechanism that distinguishes between the 2 groups.\n\n# rides vs season\nbikes %&gt;% \n  ggplot(aes(y = rides, x = season)) + \n  geom_boxplot()\n\n# rides vs season AND weekend\nbikes %&gt;%\n  ggplot(aes(y = rides, x = season, ___ = ___)) +\n  geom_boxplot()\n## Error in parse(text = input): &lt;text&gt;:8:38: unexpected input\n## 7: bikes %&gt;%\n## 8:   ggplot(aes(y = rides, x = season, __\n##                                         ^"
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-5-follow-up",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-5-follow-up",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 5: follow-up",
    "text": "Exercise 5: follow-up\n\nDescribe (in words) the relationship of ridership with season & weekend status.\nA model of rides by season alone would be represented by only 4 expected outcomes, 1 for each season. Considering this and the plot above, how do you anticipate a model of rides by season and weekend status will be represented?\n\n2 lines, 1 for each weekend status\n8 lines, 1 for each possible combination of season & weekend\n2 expected outcomes, 1 for each weekend status\n8 expected outcomes, 1 for each possible combination of season & weekend"
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-6-2-categorical-predictors-build-the-model",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-6-2-categorical-predictors-build-the-model",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 6: 2 categorical predictors – build the model",
    "text": "Exercise 6: 2 categorical predictors – build the model\nLet’s build the multiple regression model of rides vs season and weekend:\n\nbike_model_2 &lt;- lm(rides ~ weekend + season, bikes)\ncoef(summary(bike_model_2))\n##                Estimate Std. Error     t value      Pr(&gt;|t|)\n## (Intercept)   4260.4492   99.16363  42.9638294 1.384994e-201\n## weekendTRUE   -912.3324  103.23016  -8.8378473  7.298199e-18\n## seasonspring  -116.3824  132.76018  -0.8766364  3.809741e-01\n## seasonsummer   438.4424  132.06413   3.3199205  9.454177e-04\n## seasonwinter -1719.0572  133.30505 -12.8956646  2.081758e-34\n\nThus the model formula with coefficient estimates filled in is given by:\nE[rides | weekend, season] = 4260.45 - 912.33 weekendTRUE - 116.38 seasonspring + 438.44 seasonsummer - 1719.06 seasonwinter\n\nUse this model to predict the ridership on the following days:\n\n\n# a fall weekday\n4260.45 - 912.33*___ - 116.38*___  + 438.44*___ - 1719.06*___\n\n# a winter weekday    \n4260.45 - 912.33*___ - 116.38*___  + 438.44*___ - 1719.06*___\n\n# a fall weekend day        \n4260.45 - 912.33*___ - 116.38*___  + 438.44*___ - 1719.06*___\n\n# a winter weekend day\n4260.45 - 912.33*___ - 116.38*___  + 438.44*___ - 1719.06*___\n## Error in parse(text = input): &lt;text&gt;:2:19: unexpected input\n## 1: # a fall weekday\n## 2: 4260.45 - 912.33*__\n##                      ^\n\n\nWe only made 4 predictions here. How many possible predictions does this model produce? Is this consistent with your intuition in the previous exercise?"
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-7-2-categorical-predictors-interpret-the-model",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-7-2-categorical-predictors-interpret-the-model",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 7: 2 categorical predictors – interpret the model",
    "text": "Exercise 7: 2 categorical predictors – interpret the model\nUse your above predictions and visualization to fill in the below interpretations of the model coefficients.\nHint: What is the consequence of plugging in 0 or 1 for the different weekend and season categories?\n\nInterpreting 4260: On average, we expect there to be 4260 riders on (weekdays/weekends) during the (fall/spring/summer/winter).\nInterpreting -912: On average, in any season, we expect there to be 912 (more/fewer) riders on weekends than on ___.\n\nAn alternative interpretation: On average, we expect there to be 912 (more/fewer) riders on weekends than on ___, adjusting for season.\n\nInterpreting -1719: On average, on both weekdays and weekends, we expect there to be 1719 (more/fewer) riders in winter than in ___.\n\nAn alternative interpretation: On average, we expect there to be 1719 (more/fewer) riders in winter than in ___, controlling for weekday status."
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-8-2-quantitative-predictors-visualization",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-8-2-quantitative-predictors-visualization",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 8: 2 quantitative predictors – visualization",
    "text": "Exercise 8: 2 quantitative predictors – visualization\nNext, consider the relationship between rides and 2 quantitative predictors: windspeed and temp_feel. Check out the plot of this relationship below.\nThis reflect the visualization principle that quantitative variables require some sort of numerical scaling mechanism – rides and windspeed get numerical axes, and temp_feel gets a color scale.\n\nModify the code below to recreate this plot.\n\nbikes %&gt;%\n  ggplot(aes(y = rides, x = windspeed, ___ = ___)) +\n  geom_point()\n## Error in parse(text = input): &lt;text&gt;:2:41: unexpected input\n## 1: bikes %&gt;%\n## 2:   ggplot(aes(y = rides, x = windspeed, __\n##                                            ^"
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-9-follow-up",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-9-follow-up",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 9: follow-up",
    "text": "Exercise 9: follow-up\nDescribe (in words) the relationship of ridership with windspeed & temperature."
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-10-2-quantitative-predictors-modeling",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-10-2-quantitative-predictors-modeling",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 10: 2 quantitative predictors – modeling",
    "text": "Exercise 10: 2 quantitative predictors – modeling\nLet’s build the multiple regression model of rides vs windspeed and temp_feel:\n\nbike_model_3 &lt;- lm(rides ~ windspeed + temp_feel, data = bikes)\ncoef(summary(bike_model_3))\n##              Estimate Std. Error     t value     Pr(&gt;|t|)\n## (Intercept) -24.06464 299.303032 -0.08040225 9.359394e-01\n## windspeed   -36.54372   9.408116 -3.88427585 1.119805e-04\n## temp_feel    55.51648   3.330739 16.66791759 4.436963e-53\n\nThus the model formula with coefficient estimates filled in is given by,\nE[rides | windspeed, temp_feel] = -24.06 - 36.54 windspeed + 55.52 temp_feel\n\nInterpret the intercept coefficient, -24.06, in context.\nInterpret the windspeed coefficient, -36.54, in context.\nInterpret the temp_feel coefficient, 55.52, in context."
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-11-which-is-best",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-11-which-is-best",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 11: Which is “best”?",
    "text": "Exercise 11: Which is “best”?\nWe’ve now observed 3 different models of ridership, each having 2 predictors. The R-squared values of these models, along with those of the simple linear regression models with each predictor alone, are summarized below.\n\n\n\nmodel\npredictors\nR-squared\n\n\n\n\nbike_model_1\nwindspeed & weekend\n0.119\n\n\nbike_model_2\nweekend & season\n0.349\n\n\nbike_model_3\nwindspeed & temp_feel\n0.310\n\n\nbike_model_4\nwindspeed\n0.047\n\n\nbike_model_5\ntemp_feel\n0.296\n\n\nbike_model_6\nweekend\n0.074\n\n\nbike_model_7\nseason\n0.279\n\n\n\n\nWhich model does the best job of explaining the variability in ridership from day to day?\nIf you could only pick one predictor, which would it be?\nWhat happens to R-squared when we add a second predictor to our model, and why does this make sense? For example, how does the R-squared for model 1 (with both windspeed and weekend) compare to those of model 4 (only windspeed) and model 6 (only weekend)?\nAre 2 predictors always better than 1? Provide evidence and explain why this makes sense."
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-12-principles-of-interpretation",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-12-principles-of-interpretation",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 12: Principles of interpretation",
    "text": "Exercise 12: Principles of interpretation\nThese exercises have revealed some principles behind interpreting model coefficients, summarized below.\nReview and confirm that these make sense.\n\nPrinciples of interpretation\nConsider a multiple linear regression model:\n\\[E[Y | X_1, X_2, ..., X_p] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\]\nWe can interpret the coefficients as follows:\n\n\\(\\beta_0\\) (“beta 0”) is the y-intercept. It describes the average value of \\(Y\\) when \\(X_1, X_2,..., X_k\\) are all 0, ie. when all quantitative predictors are set to 0 and the categorical predictors are set to their reference levels.\n\\(\\beta_i\\) (“beta i”) is the coefficient of \\(X_i\\).\n\nIf \\(X_i\\) is quantitative, \\(\\beta_i\\) describes the average change in \\(Y\\) associated with a 1-unit increase in \\(X_i\\) while at a fixed set of the other \\(X\\).\nIf \\(X_i\\) represents a category of a categorical variable, \\(\\beta_i\\) describes the average difference in \\(Y\\) between this category and the reference category, while at a fixed set of the other \\(X\\)."
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-13-practice-1",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-13-practice-1",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 13: Practice 1",
    "text": "Exercise 13: Practice 1\nConsider the relationship of rides vs weekend and weather_cat.\n\nConstruct a visualization of this relationship.\n\nConstruct a model of this relationship.\n\nInterpret the first 3 model coefficients."
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-14-practice-2",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-14-practice-2",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 14: Practice 2",
    "text": "Exercise 14: Practice 2\nConsider the relationship of rides vs temp_feel and humidity.\n\nConstruct a visualization of this relationship.\n\nConstruct a model of this relationship.\n\nInterpret the first 3 model coefficients."
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-15-practice-3",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-15-practice-3",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 15: Practice 3",
    "text": "Exercise 15: Practice 3\nConsider the relationship of rides vs temp_feel and weather_cat.\n\nConstruct a visualization of this relationship.\n\nConstruct a model of this relationship.\n\nInterpret the first 3 model coefficients."
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#exercise-16-challenge",
    "href": "template_qmds/09-mlr-principles-notes.html#exercise-16-challenge",
    "title": "Multiple regression principles (Notes)",
    "section": "Exercise 16: CHALLENGE",
    "text": "Exercise 16: CHALLENGE\nWe’ve explored models with 2 predictors. What about 3 predictors?! Consider the relationship of rides vs temp_feel, humidity, AND weekend.\n\nConstruct a visualization of this relationship.\n\nConstruct a model of this relationship.\n\nInterpret each model coefficient."
  },
  {
    "objectID": "template_qmds/09-mlr-principles-notes.html#done",
    "href": "template_qmds/09-mlr-principles-notes.html#done",
    "title": "Multiple regression principles (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html",
    "href": "template_qmds/07-slr-cat-predictor-notes.html",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nWrite a model formula for a simple linear regression model with a categorical predictor using indicator variables\nInterpret the intercept and slope coefficients in a simple linear regression model with a categorical predictor\n\n\n\n\nComplete both the reading and the videos to go through before class.\n\nReading: Section 3.9 in the STAT 155 Notes only up through section 3.9.1 Indicator Variables\nVideos:\n\nSimple linear regression: categorical predictor (slides)\nR Code for Categorical Predictors\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#learning-goals",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#learning-goals",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nWrite a model formula for a simple linear regression model with a categorical predictor using indicator variables\nInterpret the intercept and slope coefficients in a simple linear regression model with a categorical predictor"
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#readings-and-videos",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#readings-and-videos",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "",
    "text": "Complete both the reading and the videos to go through before class.\n\nReading: Section 3.9 in the STAT 155 Notes only up through section 3.9.1 Indicator Variables\nVideos:\n\nSimple linear regression: categorical predictor (slides)\nR Code for Categorical Predictors\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#exercise-1-get-to-know-the-data",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#exercise-1-get-to-know-the-data",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\nWrite R code to answer the following:\n\nHow many cases and variables do we have? What does a case represent?\nWhat do the first few rows of the data look like?\nConstruct and interpret two different visualizations of the price variable.\nConstruct and interpret a visualization of the cut variable."
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#exercise-2-visualizations",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#exercise-2-visualizations",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Exercise 2: Visualizations",
    "text": "Exercise 2: Visualizations\nStart by visualizing this relationship of interest, that between price and cut.\n\nThe appropriate plot depends upon the type of variables we’re plotting. When exploring the relationship between a quantitative response (price) and a quantitative predictor (cut), a scatterplot was an effective choice. After running the code below, explain why a scatterplot is not effective for exploring the relationship between ridership and our categorical cut predictor.\n\n\n# Try a scatterplot\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_point()\n\n\n\n\n\n\n\n\n\nSeparately run each chunk below, with two plots. Comment (#) on what changes in the code / output.\n\n\n# Univariate boxplot\nggplot(diamonds, aes(y = price)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n# ???\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n# Univariate density plot\nggplot(diamonds, aes(x = price)) + \n    geom_density()\n\n\n\n\n\n\n\n\n\n# ???\nggplot(diamonds, aes(x = price, color = cut)) + \n    geom_density()\n\n\n\n\n\n\n\n\n\n# Univariate histogram\nggplot(diamonds, aes(x = price)) + \n    geom_histogram()\n\n\n\n\n\n\n\n\n\n# ???\nggplot(diamonds, aes(x = price)) + \n    geom_histogram() + \n    facet_wrap(~ cut)\n\n\n\n\n\n\n\n\n\nDo you notice anything interesting about the relationship between price and cut? What do you think might be happening here?"
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#exercise-3-numerical-summaries",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#exercise-3-numerical-summaries",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Exercise 3: Numerical summaries",
    "text": "Exercise 3: Numerical summaries\nLet’s follow up our plots with some numerical summaries.\n\nTo warm up, first calculate the mean price across all diamonds.\n\n\ndiamonds %&gt;% \n    ___(mean(___))\n## Error in parse(text = input): &lt;text&gt;:2:6: unexpected input\n## 1: diamonds %&gt;% \n## 2:     __\n##         ^\n\n\nTo summarize the trends we observed in the grouped plots above, we can calculate the mean price for each type of cut. This requires the inclusion of the group_by() function:\n\n\n# Calculate mean price by cut\ndiamonds %&gt;% \n    group_by(cut) %&gt;% \n    ___(mean(___))\n## Error in parse(text = input): &lt;text&gt;:4:6: unexpected input\n## 3:     group_by(cut) %&gt;% \n## 4:     __\n##         ^\n\n\nExamine the group mean measurements, and make sure that you can match these numbers up with what you see in the plots.\nBased on the results above, we can see that, on average, diamonds with a “Fair” cut tend to cost more than higher-quality cuts. Let’s construct a new variable named cutFair, using on the following criteria:\n\n\ncutFair = 1 if the diamond is of Fair cut\ncutFair = 0 otherwise (any other value of cut (Good, Very Good, Premium, Ideal))\n\nThe ifelse function allows to create a new variable from an existing one, based on whether or not the values in that variable meet a certain “condition” (remember, you can always look up function documentation in R by typing ?ifelse in the Console, and hitting enter!).\nFill in the following code to create cutFair. The condition was given to you already. Try to use this to complete the code.\n\n# In the first blank, put what value cutFair should have if the condition is \"met\", or TRUE\n# In the second blank, put what value cutFair should have if the condition is \"not met\", or FALSE\ndiamonds &lt;- diamonds %&gt;%\n  mutate(cutFair=ifelse(cut == \"Fair\", ___, ___))\n## Error in parse(text = input): &lt;text&gt;:4:41: unexpected input\n## 3: diamonds &lt;- diamonds %&gt;%\n## 4:   mutate(cutFair=ifelse(cut == \"Fair\", __\n##                                            ^\n\nVariables like cutFair that are coded as 0/1 to numerically indicate if a categorical variable is at a particular state are known as an indicator variable. You will sometimes see these referred to as a “binary variable” or “dichotomous variable”; you may also encounter the term “dummy variable” in older statistical literature.\n\nNow, let’s calculate the group means based on the new cutFair indicator variable:\n\n\ndiamonds %&gt;% \n    group_by(cutFair) %&gt;% \n    summarize(mean(price))\n## Error in `group_by()`:\n## ! Must group by variables found in `.data`.\n## ✖ Column `cutFair` is not found."
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories",
    "text": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories\nNext, let’s model the trend in the relationship between the cutFair and price variables using a simple linear regression model:\n\n# Construct the model\ndiamond_mod0 &lt;- lm(price ~ cutFair, data = diamonds)\n## Error in eval(predvars, data, env): object 'cutFair' not found\n\n# Summarize the model\ncoef(summary(diamond_mod0))\n## Error: object 'diamond_mod0' not found\n\nCompare these results to the output of exercise 3e. What do you notice? How do you interpret the intercept and cutFair coefficient terms from this model?\n\nyour answer here"
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Exercise 5: Modeling trend using a categorical predictor with >2 categories",
    "text": "Exercise 5: Modeling trend using a categorical predictor with &gt;2 categories\nUsing a single binary predictor like the cutFair indicator variable is useful when there are two clearly delineated categories. However, the cut variable actually contains 5 categories! Because we’ve collapsed all non-Fair classifications into a single category (i.e. cutFair = 0), the model above can’t tell us anything about the difference in expected price between, say, Premium and Ideal cuts. The good news is that it is very straightforward to model categorical predictors with &gt;2 categories. We can do this by using the cut variable as our predictor:\n\n# Construct the model\ndiamond_mod &lt;- lm(price ~ cut, data = diamonds)\n\n# Summarize the model\ncoef(summary(diamond_mod))\n##               Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)  4358.7578   98.78795 44.122361 0.000000e+00\n## cutGood      -429.8933  113.84940 -3.775982 1.595493e-04\n## cutVery Good -376.9979  105.16422 -3.584849 3.375707e-04\n## cutPremium    225.4999  104.39521  2.160060 3.077240e-02\n## cutIdeal     -901.2158  102.41155 -8.799943 1.408406e-18\n\n\nEven though we specified a single predictor variable in the model, we are seeing 4 coefficient estimates–why do you think this is the case?\n\n\nyour answer here\n\n\nNOTE: We see 4 indicator variables (for Good, Very Good, Premium, and Ideal), but we do not see cutFair in the model output. This is because Fair is the reference level of the cut variable (it’s first alphabetically). You’ll see below that it is, indeed, still in the model. You’ll also see why the term “reference level” makes sense!\n\n\nAfter examining the summary table output from the code chunk above, complete the model formula:\n\n\n\nE[price | cut] = ___ +/- ___ cutGood +/- ___ cutVery Good +/- ___ cutPremium +/- ___ cutIdeal"
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#exercise-6-making-sense-of-the-model",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#exercise-6-making-sense-of-the-model",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Exercise 6: Making sense of the model",
    "text": "Exercise 6: Making sense of the model\nRecall our model: E[price | cut] = 4358.7578 - 429.8933 cutGood - 376.9979 cutVery Good + 225.4999 cutPremium - 901.2158 cutIdeal\n\nUse the model formula to calculate the expected/typical price for diamonds of Good cut.\nSimilarly, calculate the expected/typical price for diamonds of Fair cut.\nRe-examine these 2 calculations. Where have you seen these numbers before?!"
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#exercise-7-interpreting-coefficients",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#exercise-7-interpreting-coefficients",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Exercise 7: Interpreting coefficients",
    "text": "Exercise 7: Interpreting coefficients\nRecall that our model formula is not a formula for a line. Thus we can’t interpret the coefficients as “slopes” as we have before. Taking this into account and reflecting upon your calculations above…\n\nInterpret the intercept coefficient (4358.7578) in terms of the data context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\nInterpret the cutGood and cutVery Good coefficients (-429.8933 and -376.9979) in terms of the data context. Hint: where did you use these value in the prediction calculations above?"
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#exercise-8-modeling-choices-challenge",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#exercise-8-modeling-choices-challenge",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Exercise 8: Modeling choices (CHALLENGE)",
    "text": "Exercise 8: Modeling choices (CHALLENGE)\nWhy do we fit this model in this way (using 4 indicator variables cutGood, cutVery Good, cutPremium, cutIdeal)? Instead, suppose that we created a single variable cutCat that gave each category a numerical value: 0 for Fair, 1 for Good, 2 for Very Good, 3 for Premium, and 4 for Ideal.\nHow would this change things? What are the pros and cons of each approach?"
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#reflection",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#reflection",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Reflection",
    "text": "Reflection\nThrough the exercises above, you learned how to build and interpret models that incorporate a categorical predictor variable. For the benefit of your future self, summarize how one can interpret the coefficients for a categorical predictor.\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#render-your-work",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#render-your-work",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Render your work",
    "text": "Render your work\n\nClick the “Render” button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\nScroll through and inspect the document to check that your work translated to the HTML format correctly.\nClose the browser tab.\nGo to the “Background Jobs” pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your “Activities” subfolder within your “STAT155” folder and locate the HTML file. You can open it again in your browser to double check."
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#exercise-9-diamond-color",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#exercise-9-diamond-color",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Exercise 9: Diamond color",
    "text": "Exercise 9: Diamond color\nConsider modeling price by color.\n\nBefore creating a visualization that shows the relationship between price and color, write down what you expect the plot to look like. Then construct and interpret an apporpriate plot.\nCompute the average price for each color.\nFit an appropriate linear model with lm() and display a short summary of the model.\nWrite out the model formula from the above summary.\nWhich color is the reference level? How can you tell from the model summary?\nInterpret the intercept and two other coefficients from the model in terms of the data context."
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#exercise-10-diamond-clarity",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#exercise-10-diamond-clarity",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Exercise 10: Diamond clarity",
    "text": "Exercise 10: Diamond clarity\nIf you want more practice, repeat the steps from Exercise 8 for the clarity variable."
  },
  {
    "objectID": "template_qmds/07-slr-cat-predictor-notes.html#done",
    "href": "template_qmds/07-slr-cat-predictor-notes.html#done",
    "title": "Simple linear regression: categorical predictor (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html",
    "href": "template_qmds/05-slr-model-eval-notes.html",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nUse residual plots to evaluate the correctness of a model\nExplain the rationale for the R-squared metric of model strength\nInterpret the R-squared metric\nThink about ethical implications of modeling by examining the impacts of biased data, power dynamics, the role of categorization, and the role of emotion and lived experience\n\n\n\n\nChoose either the reading or the videos to go through before class.\n\nReading: Sections 1.7, 3.7, and 3.8 in the STAT 155 Notes\n\nNote: You do not need to focus on the “Ladder of Power” in Section 3.8. Transformations in general will be the focus of the next activity we do.\n\nVideos:\n\nModel evaluation: is the model wrong? (slides)\nModel evaluation: is the model strong? (slides)\nModel evaluation: is the model fair? (slides)\nR Code for Evaluating and Using a Linear Model\n\n\n\n\n\nOne way to think about model evaluation is to consider whether or not underlying assumptions of our regression models are being met (or not). Asking ourselves if our models are “wrong”, “strong”, and “fair” approaches this from one perspective. To the first question (whether our model is wrong), recall the following four assumptions of linear regression:\n\nLinearity\nIndependence\nNormality\nEqual Variance\n\nNote that they spell “LINE” (how convenient!).\nBy assumptions, we mean that the above four “things” are needed mathematically in order for linear regression to “work”.\nWhereas we can check some of these assumptions using a residual plot, we need to examine the context of our data collection when checking the Independence assumption. What we mean by independence, is that the residuals in our model do not depend on one another. This may seem like an unsatisfying definition, so here are some examples:\n\nSuppose I want to understand the association between a person’s high school GPA and their college GPA. I collect data from every graduating senior, at three different high schools. If I have college GPA as my outcome, and high school GPA as my predictor, are my residuals independent? Probably not! It is reasonable to believe that students from the same high school may have similar GPAs, due to resources their high school may have had available, or specific teachers grading differently at one school or another. This is an example of clustering, where we have clusters of students within schools. The independence assumption of our linear regression model would be violated. One way to address this would be to include which high school they went to as an additional covariate in our regression model (we’ll get to this with multiple linear regression), and more advanced methods are covered in a course on Correlated Data.\nSuppose I want to understand the association between a mouse’s weight and their water consumption across time. I collect data for 365 days for ten different mice, recording their weight and water consumption each day of the year. If I have weight as my predictor and water consumption as my outcome, are my residuals independent? Nope! This is an example of correlated data that is longitudinal in nature: I have multiple observations per individual (mouse) across time. A mouse’s weight one day is certainly not independent of it’s weight the following day. The independence assumption of our linear regression model would again be violated. One way to address this would be to include “Mouse ID” as a predictor in our regression model (again, we’ll get to this with multiple linear regression).\n\nAll types of data that will violate the independence assumption of linear regression will have some sort of correlation structure (within individual, across time, across space, etc.). Think about clusters. If your observations fall neatly into specific clusters, your data may violate the independence assumption of linear regression.\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#learning-goals",
    "href": "template_qmds/05-slr-model-eval-notes.html#learning-goals",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nUse residual plots to evaluate the correctness of a model\nExplain the rationale for the R-squared metric of model strength\nInterpret the R-squared metric\nThink about ethical implications of modeling by examining the impacts of biased data, power dynamics, the role of categorization, and the role of emotion and lived experience"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#readings-and-videos",
    "href": "template_qmds/05-slr-model-eval-notes.html#readings-and-videos",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "",
    "text": "Choose either the reading or the videos to go through before class.\n\nReading: Sections 1.7, 3.7, and 3.8 in the STAT 155 Notes\n\nNote: You do not need to focus on the “Ladder of Power” in Section 3.8. Transformations in general will be the focus of the next activity we do.\n\nVideos:\n\nModel evaluation: is the model wrong? (slides)\nModel evaluation: is the model strong? (slides)\nModel evaluation: is the model fair? (slides)\nR Code for Evaluating and Using a Linear Model"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#model-assumptions",
    "href": "template_qmds/05-slr-model-eval-notes.html#model-assumptions",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "",
    "text": "One way to think about model evaluation is to consider whether or not underlying assumptions of our regression models are being met (or not). Asking ourselves if our models are “wrong”, “strong”, and “fair” approaches this from one perspective. To the first question (whether our model is wrong), recall the following four assumptions of linear regression:\n\nLinearity\nIndependence\nNormality\nEqual Variance\n\nNote that they spell “LINE” (how convenient!).\nBy assumptions, we mean that the above four “things” are needed mathematically in order for linear regression to “work”.\nWhereas we can check some of these assumptions using a residual plot, we need to examine the context of our data collection when checking the Independence assumption. What we mean by independence, is that the residuals in our model do not depend on one another. This may seem like an unsatisfying definition, so here are some examples:\n\nSuppose I want to understand the association between a person’s high school GPA and their college GPA. I collect data from every graduating senior, at three different high schools. If I have college GPA as my outcome, and high school GPA as my predictor, are my residuals independent? Probably not! It is reasonable to believe that students from the same high school may have similar GPAs, due to resources their high school may have had available, or specific teachers grading differently at one school or another. This is an example of clustering, where we have clusters of students within schools. The independence assumption of our linear regression model would be violated. One way to address this would be to include which high school they went to as an additional covariate in our regression model (we’ll get to this with multiple linear regression), and more advanced methods are covered in a course on Correlated Data.\nSuppose I want to understand the association between a mouse’s weight and their water consumption across time. I collect data for 365 days for ten different mice, recording their weight and water consumption each day of the year. If I have weight as my predictor and water consumption as my outcome, are my residuals independent? Nope! This is an example of correlated data that is longitudinal in nature: I have multiple observations per individual (mouse) across time. A mouse’s weight one day is certainly not independent of it’s weight the following day. The independence assumption of our linear regression model would again be violated. One way to address this would be to include “Mouse ID” as a predictor in our regression model (again, we’ll get to this with multiple linear regression).\n\nAll types of data that will violate the independence assumption of linear regression will have some sort of correlation structure (within individual, across time, across space, etc.). Think about clusters. If your observations fall neatly into specific clusters, your data may violate the independence assumption of linear regression.\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#exercise-1-is-the-model-correct",
    "href": "template_qmds/05-slr-model-eval-notes.html#exercise-1-is-the-model-correct",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Exercise 1: Is the model correct?",
    "text": "Exercise 1: Is the model correct?\nLet’s revisit the Capital Bikeshare data:\n\n# Load packages and import data\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nbikes &lt;- read_csv(\"https://mac-stat.github.io/data/bikeshare.csv\")\n\nWe previously explored a model of daily ridership among registered users as a function of temperature:\n\n# Fit a linear model\nbike_model &lt;- lm(riders_registered ~ temp_feel, data = bikes)\n\n# Check it out\nsummary(bike_model)\n## \n## Call:\n## lm(formula = riders_registered ~ temp_feel, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3607.1  -959.2  -153.8   998.2  3304.8 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -667.916    251.608  -2.655  0.00811 ** \n## temp_feel     57.892      3.306  17.514  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1310 on 729 degrees of freedom\n## Multiple R-squared:  0.2961, Adjusted R-squared:  0.2952 \n## F-statistic: 306.7 on 1 and 729 DF,  p-value: &lt; 2.2e-16\n\nPlot this relationship with both a curved and linear trend line. Based on this plot, do you think the model is correct? If not, which of the LINE assumptions does it violate?\n\n# Plot temp_feel vs riders_registered with a model trend\n___(___, aes(x = ___, y = ___)) + \n    geom___() + \n    geom___(se = FALSE, color = \"red\") +\n    geom___(method = \"lm\", se = FALSE)\n## Error in parse(text = input): &lt;text&gt;:2:2: unexpected input\n## 1: # Plot temp_feel vs riders_registered with a model trend\n## 2: __\n##     ^"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#exercise-2-residual-plots",
    "href": "template_qmds/05-slr-model-eval-notes.html#exercise-2-residual-plots",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Exercise 2: Residual plots",
    "text": "Exercise 2: Residual plots\nPlotting the residuals vs the predictions (also called “fitted values”) for each case can help us assess how wrong our model is. This will be a particularly important tool when evaluating models with multiple predictors. Construct the residual plot for bike_model. As with the scatterplot, this plot indicates that bike_model violates one of the LINE assumptions. Explain which assumption that is and how you can tell that from just the residual plot.\nNotes:\n\nInformation about the residuals (.resid) and predictions (.fitted) are stored within our model, thus we start our ggplot() with the model name as opposed to the raw dataset. We will rarely start ggplot() with a model instead of the data.\nWe can fix this model by adding a quadratic “transformation term”. We’ll discuss this idea in our next class.\n\n\n# Check out the residual plot for bike_model\nggplot(bike_model, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#exercise-3-whats-incorrect-about-this-model",
    "href": "template_qmds/05-slr-model-eval-notes.html#exercise-3-whats-incorrect-about-this-model",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Exercise 3: What’s incorrect about this model?",
    "text": "Exercise 3: What’s incorrect about this model?\nConsider another example. The mammals data includes data on the average brain weight (g) and body weight (kg) for a variety of mammals:\n\n# Import the data\nmammals &lt;- read_csv(\"https://mac-stat.github.io/data/mammals.csv\")\n\n# Check it out\nhead(mammals)\n## # A tibble: 6 × 4\n##    ...1 animal            body brain\n##   &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n## 1     1 Arctic fox        3.38  44.5\n## 2     2 Owl monkey        0.48  15.5\n## 3     3 Mountain beaver   1.35   8.1\n## 4     4 Cow             465    423  \n## 5     5 Grey wolf        36.3  120. \n## 6     6 Goat             27.7  115\n\nFit a model of brain vs body weight:\n\n# Construct the model\nmammal_model &lt;- lm(brain ~ body, mammals)\n\n# Check it out\nsummary(mammal_model)\n## \n## Call:\n## lm(formula = brain ~ body, data = mammals)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -810.07  -88.52  -79.64  -13.02 2050.33 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 91.00440   43.55258    2.09   0.0409 *  \n## body         0.96650    0.04766   20.28   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 334.7 on 60 degrees of freedom\n## Multiple R-squared:  0.8727, Adjusted R-squared:  0.8705 \n## F-statistic: 411.2 on 1 and 60 DF,  p-value: &lt; 2.2e-16\n\n\nConstruct two plots that will help us evaluate mammal_model:\n\n\n# Scatterplot of brain weight (y) vs body weight (x)\n# Include a model trend line (i.e. a representation of mammal_model)\n\n\n# Residual plot for mammal_model\n\n\nThese two plots confirm that our model is wrong. What is wrong? That is, which of the LINE assumptions are violated? (NOTE: We again can fix this model by “transforming” one or both of the brain and body variables. We’ll discuss this idea in our next class.)"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#exercise-4-exploring-mammals",
    "href": "template_qmds/05-slr-model-eval-notes.html#exercise-4-exploring-mammals",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Exercise 4: Exploring mammals",
    "text": "Exercise 4: Exploring mammals\nJust for fun, let’s dig into the mammals data. Discuss what you observe:\n\n# Label the points by the animal name!\n# Discuss: What 2 things are new in this code?\nggplot(mammals, aes(x = body, y = brain, label = animal)) + \n    geom_text() + \n    geom_smooth(method = \"lm\", se = FALSE) \n\n\n\n\n\n\n\n\n\n# Zoom in\nggplot(mammals, aes(x = body, y = brain, label = animal)) + \n    geom_text() + \n    lims(y = c(0, 1500), x = c(0, 600))\n\n\n\n\n\n\n\n\n\n# Zoom in more\nggplot(mammals, aes(x = body, y = brain, label = animal)) + \n    geom_text() + \n    lims(y = c(0, 500), x = c(0, 200))"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#exercise-5-is-the-model-strong-developing-r-squared-intuition",
    "href": "template_qmds/05-slr-model-eval-notes.html#exercise-5-is-the-model-strong-developing-r-squared-intuition",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Exercise 5: Is the model strong? Developing R-squared intuition",
    "text": "Exercise 5: Is the model strong? Developing R-squared intuition\nThe R-squared metric is a way to quantify the strength of a model. It measures how much variation in the outcome/response variable can be explained by the variation in the predictors.\nWhere does R-squared come from? Well, it turns out that we can partition the variance of the observed response values into the variability that’s explained by the model (the variance of the predictions) and the variability that’s left unexplained by the model (the variance of the residuals):\n\\[\\text{Var(observed) = Var(predicted) + Var(residuals)}\\]\nStrong models have residuals that don’t deviate far from 0. So the smaller the variance in the residuals (thus larger the variance in the predictions), the stronger the model. Take a look at the picture below and write a few sentences addressing the following:\n\nThe two rows of plots show a stronger and a weaker model. Just by looking at the blue trend line and the dispersion of the points about the line, which row corresponds to the stronger model? How can you tell? Which row would you expect to have a higher correlation?\nWhat is different about the variance of the residuals from the first to the second row?\n\n\nPutting this together, the R-squared compares Var(predicted) to Var(response):\n\\[R^2 = \\frac{\\text{variance of predicted values}}{\\text{variance of observed response values}} = 1 - \\frac{\\text{variance of residuals}}{\\text{variance of observed response values}}\\] ::: {.callout-note collapse=“true”} ## R-squared\n\\[\nR^2 = 1 - \\frac{SSE}{SSTO} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n\\] where \\(y_i\\) are our observed outcomes, \\(i = 1, \\dots, n\\), \\(\\hat{y}_i\\) are our fitted values/predictions, and \\(\\bar{y}\\) is our observed average outcome. :::"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#exercise-6-r-squared-interpretations",
    "href": "template_qmds/05-slr-model-eval-notes.html#exercise-6-r-squared-interpretations",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Exercise 6: R-squared Interpretations",
    "text": "Exercise 6: R-squared Interpretations\nRecall bikemod1 from Exercise 3, where we predicted registered riders by what the temperature felt like on a given day. Use the summary function to look out the model output for bikemod1, and interpret the \\(R^2\\) value for this model, in the context of the problem. (NOTE: \\(R^2\\) is reported in output here as “Multiple R-squared”).\n\n# Get R-squared\nsummary(bike_model)\n## \n## Call:\n## lm(formula = riders_registered ~ temp_feel, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3607.1  -959.2  -153.8   998.2  3304.8 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -667.916    251.608  -2.655  0.00811 ** \n## temp_feel     57.892      3.306  17.514  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1310 on 729 degrees of freedom\n## Multiple R-squared:  0.2961, Adjusted R-squared:  0.2952 \n## F-statistic: 306.7 on 1 and 729 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#exercise-7-further-exploring-r-squared",
    "href": "template_qmds/05-slr-model-eval-notes.html#exercise-7-further-exploring-r-squared",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Exercise 7: Further exploring R-squared",
    "text": "Exercise 7: Further exploring R-squared\nIn this exercise, we’ll look at data from a synthetic dataset called Anscombe’s quartet. Load the data in as follows, and look at the first few rows:\n\ndata(anscombe)\n\n# Look at the first few rows\n\nThe anscombe data is actually 4 datasets in one: x1 and y1 go together, and so forth. Examine the coefficient estimates (in the “Estimate” column of the “Coefficients:” part) and the “Multiple R-squared” value on the second to last line. What do you notice? How do these models compare?\n\nanscombe_mod1 &lt;- lm(y1 ~ x1, data = anscombe)\nanscombe_mod2 &lt;- lm(y2 ~ x2, data = anscombe)\nanscombe_mod3 &lt;- lm(y3 ~ x3, data = anscombe)\nanscombe_mod4 &lt;- lm(y4 ~ x4, data = anscombe)\n\nsummary(anscombe_mod1)\n## \n## Call:\n## lm(formula = y1 ~ x1, data = anscombe)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.92127 -0.45577 -0.04136  0.70941  1.83882 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0001     1.1247   2.667  0.02573 * \n## x1            0.5001     0.1179   4.241  0.00217 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.237 on 9 degrees of freedom\n## Multiple R-squared:  0.6665, Adjusted R-squared:  0.6295 \n## F-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\nsummary(anscombe_mod2)\n## \n## Call:\n## lm(formula = y2 ~ x2, data = anscombe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.9009 -0.7609  0.1291  0.9491  1.2691 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)    3.001      1.125   2.667  0.02576 * \n## x2             0.500      0.118   4.239  0.00218 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.237 on 9 degrees of freedom\n## Multiple R-squared:  0.6662, Adjusted R-squared:  0.6292 \n## F-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\nsummary(anscombe_mod3)\n## \n## Call:\n## lm(formula = y3 ~ x3, data = anscombe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1586 -0.6146 -0.2303  0.1540  3.2411 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0025     1.1245   2.670  0.02562 * \n## x3            0.4997     0.1179   4.239  0.00218 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.236 on 9 degrees of freedom\n## Multiple R-squared:  0.6663, Adjusted R-squared:  0.6292 \n## F-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\nsummary(anscombe_mod4)\n## \n## Call:\n## lm(formula = y4 ~ x4, data = anscombe)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -1.751 -0.831  0.000  0.809  1.839 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0017     1.1239   2.671  0.02559 * \n## x4            0.4999     0.1178   4.243  0.00216 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.236 on 9 degrees of freedom\n## Multiple R-squared:  0.6667, Adjusted R-squared:  0.6297 \n## F-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nNow take a look at the following scatterplots of the 4 pairs of variables. What do you notice? What takeaway can we draw from this exercise?\n\nggplot(anscombe, aes(x = x1, y = y1)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x2, y = y2)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x3, y = y3)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x4, y = y4)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#exercise-8-biased-data-biased-results-example-1",
    "href": "template_qmds/05-slr-model-eval-notes.html#exercise-8-biased-data-biased-results-example-1",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Exercise 8: Biased data, biased results: example 1",
    "text": "Exercise 8: Biased data, biased results: example 1\nDATA ARE NOT NEUTRAL. Data can reflect personal biases, institutional biases, power dynamics, societal biases, the limits of our knowledge, and so on. In turn, biased data can lead to biased analyses. Consider an example.\n\nDo a Google image search for “statistics professor.” What do you observe?\nThese search results are produced by a search algorithm / model. Explain why the data used by this model are not neutral.\nWhat are the potential implications, personal or societal, of the search results produced from this biased data?"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#exercise-9-biased-data-biased-results-example-2",
    "href": "template_qmds/05-slr-model-eval-notes.html#exercise-9-biased-data-biased-results-example-2",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Exercise 9: Biased data, biased results: example 2",
    "text": "Exercise 9: Biased data, biased results: example 2\nConsider the example of a large company that developed a model / algorithm to review the résumés of applicants for software developer & other tech positions. The model then gave each applicant a score indicating their hireability or potential for success at the company. You can think of this model as something like:\n\\[\\text{potential for success } = \\beta_0 + \\beta_1 (\\text{features from the résumé})\\]\nSkim this Reuter’s article about the company’s résumé model.\n\nExplain why the data used by this model are not neutral.\nWhat are the potential implications, personal or societal, of the results produced from this biased data?"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#exercise-10-rigid-data-collection-systems",
    "href": "template_qmds/05-slr-model-eval-notes.html#exercise-10-rigid-data-collection-systems",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Exercise 10: Rigid data collection systems",
    "text": "Exercise 10: Rigid data collection systems\nWhen working with categorical variables, we’ve seen that our units of observation fall into neat groups. Reality isn’t so discrete. For example, check out questions 6 and 9 on page 2 of the 2020 US Census. With your group, discuss the following:\n\nWhat are a couple of issues you see with these questions?\nWhat impact might this type of data collection have on a subsequent analysis of the census responses and the policies it might inform?\nCan you think of a better way to write these questions while still preserving the privacy of respondents?\n\nFOR A DEEPER DISCUSSION: Read Chapter 4 of Data Feminism on “What gets counted counts”."
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#exercise-11-presenting-data-elevating-emotion-and-embodiment",
    "href": "template_qmds/05-slr-model-eval-notes.html#exercise-11-presenting-data-elevating-emotion-and-embodiment",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Exercise 11: Presenting data: “Elevating emotion and embodiment”",
    "text": "Exercise 11: Presenting data: “Elevating emotion and embodiment”\nNote: The following example highlights work done by W.E.B. Du Bois in the late 1800s / early 1900s. His work uses language common to that time period and addresses the topic of slavery.\nThe types of visualizations we’ve been learning in this course are standard practice, hence widely understood. Yet these standard visualizations can also suppress the lived experiences of people represented in the data, hence can miss the larger point. W.E.B. Du Bois (1868–1963), a “sociologist, socialist, historian, civil rights activist, Pan-Africanist, author, writer, and editor”1, was a pioneer in elevating emotion and embodiment in data visualization. For the Paris World Fair of 1900, Du Bois and his team of students from Atlanta University presented 60 data visualizations of the Black experience in America, less than 50 years after the abolishment of slavery. To this end, Du Bois noted that “I wanted to set down its aim and method in some outstanding way which would bring my work to notice by the thinking world.” That is, he wanted to increase the impact of his work by partnering technical visualizations with design that better connects to lived experiences. Check out:\n\nAn article by Allen Hillery (@AlDatavizguy).\nA complete set of the data visualizations provided by Anthony Starks (@ajstarks).\n\nDiscuss your observations. In what ways do you think the W.E.B. Du Bois visualizations might have been more effective at sharing his work than, say, plainer bar charts?\nFOR A DEEPER DISCUSSION AND MORE MODERN EXAMPLES: Read Chapter 3 of Data Feminism on the principle of elevating emotion and embodiment, i.e. the value of “multiple forms of knowledge, including the knowledge that comes from people as living, feeling bodies in the world.”"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#reflection",
    "href": "template_qmds/05-slr-model-eval-notes.html#reflection",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Reflection",
    "text": "Reflection\nWhat has stuck with you most in our exploration of model evaluation? Why\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#done",
    "href": "template_qmds/05-slr-model-eval-notes.html#done",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/05-slr-model-eval-notes.html#footnotes",
    "href": "template_qmds/05-slr-model-eval-notes.html#footnotes",
    "title": "Simple linear regression: model evaluation (Notes)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/W._E._B._Du_Bois↩︎"
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html",
    "href": "template_qmds/03-slr-introduction-notes.html",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nVisualize and describe the relationship between two quantitative variables using a scatterplot\nWrite R code to create a scatterplot and compute the linear correlation between two quantitative variables\nDescribe/identify weak / strong, and positive / negative correlation from a point cloud\nBuild intuition for fitting lines to quantify the relationship between two quantitative variables\n\n\n\n\nChoose either the reading or the videos to go through after class.\n\nReading: Sections 2.8, 3.1-3.3, 3.6 in the STAT 155 Notes\nVideos:\n\nSimple linear regression Part 1: motivation & scatterplots\nSimple linear regression Part 2: correlation\nSimple linear regression Part 3: simple linear regression models\nR Code for Fitting a Linear Model (Time: 11:07)\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#learning-goals",
    "href": "template_qmds/03-slr-introduction-notes.html#learning-goals",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nVisualize and describe the relationship between two quantitative variables using a scatterplot\nWrite R code to create a scatterplot and compute the linear correlation between two quantitative variables\nDescribe/identify weak / strong, and positive / negative correlation from a point cloud\nBuild intuition for fitting lines to quantify the relationship between two quantitative variables"
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#readings-and-videos",
    "href": "template_qmds/03-slr-introduction-notes.html#readings-and-videos",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "",
    "text": "Choose either the reading or the videos to go through after class.\n\nReading: Sections 2.8, 3.1-3.3, 3.6 in the STAT 155 Notes\nVideos:\n\nSimple linear regression Part 1: motivation & scatterplots\nSimple linear regression Part 2: correlation\nSimple linear regression Part 3: simple linear regression models\nR Code for Fitting a Linear Model (Time: 11:07)\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#exercise-1-get-to-know-the-data",
    "href": "template_qmds/03-slr-introduction-notes.html#exercise-1-get-to-know-the-data",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\nCreate a new code chunk by clicking the green “C” button with a green + sign in the top right of the menu bar. In this code chunk, use an appropriate function to look at the first few rows of the data.\nCreate a new code chunk, and use an appropriate function to learn how much data we have (in terms of cases and variables).\nWhat does a case represent?\nNavigate to the FAQ page and read the response to the “How does this site work? Do you just download results from the federations?” question. What do you learn about data quality and completeness from this response?"
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#exercise-2-mutating-our-data",
    "href": "template_qmds/03-slr-introduction-notes.html#exercise-2-mutating-our-data",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "Exercise 2: Mutating our data",
    "text": "Exercise 2: Mutating our data\nStrength-to-weight ratio (SWR) is defined as TotalKg/BodyweightKg. We can use the mutate() function from the dplyr package to create a new variable in our dataframe for SWR using the following code:\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (lifts data is \"fed into\" the mutate() function).\n# When creating a new variable, we often reassign the data frame to itself,\n# which updates the existing columns in lifts with the additional \"new\" column(s)\n# in lifts!\nlifts &lt;- lifts %&gt;% \n    mutate(NEW_VARIABLE_NAME = Age/BestSquatKg)\n## Error in `mutate()`:\n## ℹ In argument: `NEW_VARIABLE_NAME = Age/BestSquatKg`.\n## Caused by error:\n## ! object 'BestSquatKg' not found\n\nAdapt the example above to create a new variable called SWR, where SWR is defined as TotalKg/BodyweightKg."
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#exercise-3-get-to-know-the-outcomeresponse-variable",
    "href": "template_qmds/03-slr-introduction-notes.html#exercise-3-get-to-know-the-outcomeresponse-variable",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "Exercise 3: Get to know the outcome/response variable",
    "text": "Exercise 3: Get to know the outcome/response variable\nLet’s get acquainted with the SWR variable.\n\nConstruct an appropriate plot to visualize the distribution of this variable, and compute appropriate numerical summaries.\nWrite a good paragraph interpreting the plot and numerical summaries."
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#exercise-4-data-visualization---two-quantitative-variables",
    "href": "template_qmds/03-slr-introduction-notes.html#exercise-4-data-visualization---two-quantitative-variables",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "Exercise 4: Data visualization - two quantitative variables",
    "text": "Exercise 4: Data visualization - two quantitative variables\nWe’d like to visualize the relationship between body weight and the strength-to-weight ratio. A scatterplot (or informally, a “point cloud”) allows us to do this! The code below creates a scatterplot of body weight vs. SWR using ggplot().\n\n# scatterplot\n\n# The alpha = 0.5 in geom_point() adds transparency to the points\n# to make them easier to see. You can make this smaller for more transparency\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5)\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error:\n## ! object 'SWR' not found\n\n\nThis is your first bivariate data visualization (visualization for two variables)! What differences do you notice in the code structure when creating a bivariate visualization, compared to univariate visualizations we’ve worked with before?\nWhat similarities do you notice in the code structure?\nDoes there appear to be some sort of pattern in the structure of the point cloud? Describe it, in no more than three sentences! Comment on the direction of the relationship between the two variables (positive? negative?) and the spread of the points (are they dispersed? close together?)."
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#exercise-5-scatterplots---patterns-in-point-clouds",
    "href": "template_qmds/03-slr-introduction-notes.html#exercise-5-scatterplots---patterns-in-point-clouds",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "Exercise 5: Scatterplots - patterns in point clouds",
    "text": "Exercise 5: Scatterplots - patterns in point clouds\nSometimes, it can be easier to see a pattern in a point cloud by adding a smoothing line to our scatterplots. The code below adapts the code in Exercise 4 to do this:\n\n# scatterplot with smoothing line\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth()\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error:\n## ! object 'SWR' not found\n\n\nLook back at your answer to Exercise 4 (c). Does the smoothing line assist you in seeing a pattern, or change your answer at all? Why or why not?\nBased on the scatterplot with the smoothing line added above, does there appear to be a linear relationship between body weight and SWR (i.e. would a straight line do a decent job at summarizing the relationship between these two variables)? Why or why not?"
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#exercise-6-correlation",
    "href": "template_qmds/03-slr-introduction-notes.html#exercise-6-correlation",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "Exercise 6: Correlation",
    "text": "Exercise 6: Correlation\nWe can quantify the linear relationship between two quantitative variables using a numerical summary known as correlation (sometimes known as a “correlation coefficient” or “Pearson’s correlation”). Correlation can range from -1 to 1, where a correlation of 0 indicates that there is no linear relationship between the two quantitative variables.\nBelow is an example of a “Math Box”. You’ll see these occasionally throughout the activities. You are not required to memorize, nor will you be assessed on, anything in the math boxes. If you plan on continuing with Statistics courses at Macalester (or are interested in the math behind everything!), these math boxes are for you!\n\n\n\n\n\n\nCorrelation\n\n\n\n\n\nThe Pearson correlation coefficient, \\(r_{x, y}\\), of \\(x\\) and \\(y\\) is the (almost) average of products of the z-scores of variables \\(x\\) and \\(y\\):\n\\[\nr_{x, y} = \\frac{\\sum z_x z_y}{n - 1}\n\\]\n\n\n\nIn general, we will want to be able to describe (qualitatively) two aspects of correlation:\n\nStrength\n\n\nIs the correlation between x and y strong, or weak, i.e. how closely do the points fit around a line? This has to do with how dispersed our point clouds are.\n\n\nDirection\n\n\nIs the correlation between x and y positive or negative, i.e. does y go “up” when x goes “up” (positive), or does y go “down” when x goes “up” (negative)?\n\nStronger correlations will be further from 0 (closer to -1 or 1), and positive and negative correlations will have the appropriate respective sign (above or below zero).\n\nRather than a smooth trend line, we can force the line we add to our scatterplots to be linear using geom_smooth(method = 'lm'), as below:\n\n\n# scatterplot with linear trend line\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error:\n## ! object 'SWR' not found\n\n\nBased on the above scatterplot, how would you describe the correlation between body weight and SWR, in terms of strength and direction?\nMake a guess as to what numerical value the correlation between body weight and SWR will have, based on your response to part (b)."
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#exercise-7-computing-correlation-in-r",
    "href": "template_qmds/03-slr-introduction-notes.html#exercise-7-computing-correlation-in-r",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "Exercise 7: Computing correlation in R",
    "text": "Exercise 7: Computing correlation in R\nWe can compute the correlation between body weight and SWR using summarize and cor functions:\n\n# correlation\n\n# Note: the order in which you put your two quantitative variables into the cor\n# function doesn't matter! Try switching them around to confirm this for yourself\n# Because of the missing data, we need to include the use = \"complete.obs\" - otherwise the correlation would be computed as NA\nlifts %&gt;%\n    summarize(cor(SWR, BodyweightKg, use = \"complete.obs\"))\n## Error in `summarize()`:\n## ℹ In argument: `cor(SWR, BodyweightKg, use = \"complete.obs\")`.\n## Caused by error:\n## ! object 'SWR' not found\n\nIs the computed correlation close to what you guessed in Exercise 6 part (c)?"
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#exercise-8-limitations-of-correlation",
    "href": "template_qmds/03-slr-introduction-notes.html#exercise-8-limitations-of-correlation",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "Exercise 8: Limitations of correlation",
    "text": "Exercise 8: Limitations of correlation\nWe previously noted that correlation was a numerical summary of the linear relationship between two variables. We’ll now go through some examples of relationships between quantitative variables to demonstrate why it is incredibly important to visualize our data in addition to just computing numerical summaries!\nFor this exercise, we’ll be working with the anscombe dataset, which is built in to R. To load this dataset into our environment, we run the following code:\n\n# load anscombe data\ndata(\"anscombe\")\n\nThe anscombe dataset contains four different pairs of quantitative variables:\n\nx1, y1\nx2, y2\nx3, y3\nx4, y4\n\nAdapt the code we used in Exercise 7 to compute the correlation between each of these four pairs of variables, below:\n\n# correlation between x1, y1\n\n# correlation between x2, y2\n\n# correlation between x3, y3\n\n# correlation between x4, y4\n\n\nWhat do you notice about each of these correlations (if the answer to this isn’t obvious, double-check your code)?\nDescribe these correlations in terms of strength and direction, using only the numerical summary to assist you in your description.\nDraw an example on the white board or at your tables of what you think the point clouds for these pairs of variables might look like. There are only 11 observations, so you can draw all 11 points if you’d like!\nAdapt the code for scatterplots given previously in this activity to make four distinct scatterplots for each pair of quantitative variables in the anscombe dataset. You do not need to add a smooth trend line or a linear trend line to these plots.\n\n\n# scatterplot: x1, y1\n\n# scatterplot: x2, y2\n\n# scatterplot: x3, y3\n\n# scatterplot: x4, y4\n\n\nBased on the correlations you calculated and scatterplots you made, what is the message of this last exercise as it relates to the limits of correlation?"
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#reflection",
    "href": "template_qmds/03-slr-introduction-notes.html#reflection",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "Reflection",
    "text": "Reflection\nMuch of statistics is about making (hopefully) reasonable assumptions in attempt to summarize observed relationships in data. Today we started considering assumptions of linear relationships between quantitative variables.\nReview the learning objectives at the top of this file and today’s activity. How do you imagine assumptions of linearity might be useful in terms of quantifying relationships between quantitative variables? How do you imagine these assumptions could sometimes fall short, or even be unethical in certain cases?\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#exercise-9-lines-of-best-fit",
    "href": "template_qmds/03-slr-introduction-notes.html#exercise-9-lines-of-best-fit",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "Exercise 9: Lines of best fit",
    "text": "Exercise 9: Lines of best fit\nIn this activity, we’ve learned how to fit straight lines to data, to help us visualize the relationship between two quantitative variables. So far, ggplot has chosen the line for us. How does it know which line is “best”, and what does “best” even mean?\nFor this exercise, we’ll consider the relationship between x1 and y1 in the anscombe dataset. Run the following code, which creates a scatterplot with a fitted line to our data using the function geom_abline:\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_point() +\n  geom_abline(slope = 0.4, intercept = 3, col = \"blue\", size = 1)\n\n\n\n\n\n\n\n\nDescribe the line that you see. Do you think the line is “good”? What are you using to define “good”?\nSome things to think about:\n\nHow many points are above the line?\nHow many points are below the line?\nAre the distances of the points above and below the line roughly similar, or is there meaningful difference?\n\nNow we’ll add another line to our plot. Which line do you think is better suited for this data? Why? Be specific!\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_point() +\n  geom_abline(slope = 0.4, intercept = 3, col = \"blue\", size = 1) +\n  geom_abline(slope = 0.5, intercept = 4, col = \"orange\", size = 1)\n\n\n\n\n\n\n\n\nIt’s usually quite simple to note when a line is bad, but more difficult to quantify when a line is a good fit for our data. Consider the following line:\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_point() +\n  geom_abline(slope = -0.5, intercept = 10, col = \"red\", size = 1) \n\n\n\n\n\n\n\n\nIn the next activity, we’ll formalize the principle of least squares, which will give us one particular definition of a line of best fit that is commonly used in statistics! We’ll take advantage of the vertical distances between each point and the fitted line (residuals), which will help us define (mathematically) a line that best fits our data:\n\nlibrary(broom)\nanscombe %&gt;%\n  lm(y1 ~ x1, data = .) %&gt;%\n  augment() %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_segment(aes(xend = x1, yend = .fitted), col = \"red\") +\n  geom_point()"
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#exercise-10-correlation-and-extreme-values",
    "href": "template_qmds/03-slr-introduction-notes.html#exercise-10-correlation-and-extreme-values",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "Exercise 10: Correlation and extreme values",
    "text": "Exercise 10: Correlation and extreme values\nIn this exercise, we’ll explore how correlation changes with the addition of extreme values, or observations. We’ll begin by generating a toy dataset called dat with two quantitative variables, x and y. Run the code below to create the dataset.\nwhile not required, recall that you can look up function documentation in R using the ? in front of a function name to figure out what that function is doing!\n\n# create a toy dataset\nset.seed(1234)\nx &lt;- rnorm(100, mean = 5, sd = 2)\ny &lt;- -3 * x + rnorm(100, sd = 4)\ndat &lt;- data.frame(x = x, y = y)\n\n\nMake a scatterplot of x vs. y.\n\n\n# scatterplot\n\n\nBased on your scatterplot, describe the correlation between x and y in terms of strength and direction.\nGuess the correlation (the numerical value) between x and y.\nCompute the correlation between x and y. Was your guess from part (c) close?\n\n\n# correlation\n\n\nSuppose we observe an additional observation with x = 15 and y = -45. We can create a new data frame, dat_new1, that contains this observation in addition to the original ones as follows:\n\n\n# creating dat_new1\nx1 &lt;- c(x, 15)\ny1 &lt;- c(y, -45)\ndat_new1 &lt;- data.frame(x = x1, y = y1)\n\n\nMake a scatterplot of x vs. y for this new data frame, and compute the correlation between x and y. Did your correlation change very much with the addition of this observation? Hypothesize why or why not.\n\n\n# scatterplot\n\n# correlation\n\n\nSuppose instead of our additional observation having values x = 15 and y = -45, we instead observe x = 15 and y = -15. We can create a new data frame, dat_new2, that contains this observation in addition to the original ones as follows:\n\n\n# creating dat_new1\nx2 &lt;- c(x, 15)\ny2 &lt;- c(y, 45)\ndat_new2 &lt;- data.frame(x = x2, y = y2)\n\n\nMake a scatterplot of x vs. y for this new data frame, and compute the correlation between x and y. Did your correlation change very much with the addition of this observation? Hypothesize why or why not.\n\n\n# scatterplot\n\n# correlation\n\n\nWhat do you think the takeaway message is of this exercise?\n\n\nChallenge Add linear trend lines to your scatterplots from parts (f) and (h). Does this give you any additional insight into why the correlations may have changed in different ways with the addition of a new observation?"
  },
  {
    "objectID": "template_qmds/03-slr-introduction-notes.html#done",
    "href": "template_qmds/03-slr-introduction-notes.html#done",
    "title": "Simple linear regression: Visualization and Introduction (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/01-foundations-welcome-notes.html",
    "href": "template_qmds/01-foundations-welcome-notes.html",
    "title": "Collecting and Summarizing Data (Notes)",
    "section": "",
    "text": "Welcome to our first in-class activity! Today we will collect and summarize some data. Our goals are to get to know the people in this class and to start working with data.\nBy the end of this lesson, you should be able to:\n\nDefine cases and variables\nApply the 5 W’s + H (who, what, when, where, why, and how) to data collection\n\nLinks to related reading(s):\n\nWhat is Data?\nData Context\n\nThis activity is structured a bit differently than the activities for the remainder of the class. In this section, you’ll typically find a mini-lecture, review material, or guided / structural examples, followed by exercises.\nFor today, you’ll have some steps to follow for an interactive, tactile activity at your tables before working through the exercises below together.\n\n\nAt your table, write a one to two word answer to each of the following 7 question(s), each on a separate post-it note (do this individually). If there are two questions: write your answer to the first question on the left-hand side of the post-it note, and your answer to the second question on the bottom of the post-it note.\n\nHow many hours of sleep did you get last night? How many cups of coffee did you drink this morning?\nWhat is your declared or potential major? (If you are a double major, just pick whichever one you think of first.)\nWhat is your class year? (first year, sophomore, junior, senior)\nHow many stats courses have you taken in the past?\nOn a scale of 1 (get me out of here) to 10 (yay!), how excited are you about this course?\nIs it your birthday this semester? (yes/no)\nHow many unread emails do you have in your inbox right now?\nHave you used R/RStudio a lot, a little, or never?\n\n\n\n\nDesignate one person from your table to distribute your group’s answers to the questions to the corresponding “station” around the classroom. Each table should have a number on it, corresponding to the “station”/question.\n\n\n\nFill out an electronic version of these questions. We’ll come back to this in a future class. Wait until everyone in your group is done with this before moving on to the exercises."
  },
  {
    "objectID": "template_qmds/01-foundations-welcome-notes.html#step-1",
    "href": "template_qmds/01-foundations-welcome-notes.html#step-1",
    "title": "Collecting and Summarizing Data (Notes)",
    "section": "",
    "text": "At your table, write a one to two word answer to each of the following 7 question(s), each on a separate post-it note (do this individually). If there are two questions: write your answer to the first question on the left-hand side of the post-it note, and your answer to the second question on the bottom of the post-it note.\n\nHow many hours of sleep did you get last night? How many cups of coffee did you drink this morning?\nWhat is your declared or potential major? (If you are a double major, just pick whichever one you think of first.)\nWhat is your class year? (first year, sophomore, junior, senior)\nHow many stats courses have you taken in the past?\nOn a scale of 1 (get me out of here) to 10 (yay!), how excited are you about this course?\nIs it your birthday this semester? (yes/no)\nHow many unread emails do you have in your inbox right now?\nHave you used R/RStudio a lot, a little, or never?"
  },
  {
    "objectID": "template_qmds/01-foundations-welcome-notes.html#step-2",
    "href": "template_qmds/01-foundations-welcome-notes.html#step-2",
    "title": "Collecting and Summarizing Data (Notes)",
    "section": "",
    "text": "Designate one person from your table to distribute your group’s answers to the questions to the corresponding “station” around the classroom. Each table should have a number on it, corresponding to the “station”/question."
  },
  {
    "objectID": "template_qmds/01-foundations-welcome-notes.html#step-3",
    "href": "template_qmds/01-foundations-welcome-notes.html#step-3",
    "title": "Collecting and Summarizing Data (Notes)",
    "section": "",
    "text": "Fill out an electronic version of these questions. We’ll come back to this in a future class. Wait until everyone in your group is done with this before moving on to the exercises."
  },
  {
    "objectID": "template_qmds/01-foundations-welcome-notes.html#exercise-1",
    "href": "template_qmds/01-foundations-welcome-notes.html#exercise-1",
    "title": "Collecting and Summarizing Data (Notes)",
    "section": "Exercise 1",
    "text": "Exercise 1\nWith your group, in no more than two sentences per question, respond to the questions posed by the 5 W’s + H for the data at your group’s table. If you need a refresher on the 5 W’s + H, check out the related readings posted at the top of this activity!\nWho\n\n  Your answer:\n  \n\nWhat\n\n  Your answer:\n  \n\nWhen\n\n  Your answer:\n  \n\nWhere\n\n  Your answer:\n  \n\nWhy\n\n  Your answer:\n  \n\nHow\n\n  Your answer:"
  },
  {
    "objectID": "template_qmds/01-foundations-welcome-notes.html#exercise-2",
    "href": "template_qmds/01-foundations-welcome-notes.html#exercise-2",
    "title": "Collecting and Summarizing Data (Notes)",
    "section": "Exercise 2",
    "text": "Exercise 2\nMove the post-it notes around to construct a visualization of the responses at your station."
  },
  {
    "objectID": "template_qmds/01-foundations-welcome-notes.html#exercise-3",
    "href": "template_qmds/01-foundations-welcome-notes.html#exercise-3",
    "title": "Collecting and Summarizing Data (Notes)",
    "section": "Exercise 3",
    "text": "Exercise 3\nCalculate at least one numerical summary of the post-it note responses at your station, and record your numerical summary in the response text below. Write a complete sentence, not just the numerical summary alone! This is good practice for summarizing data in more formal writing.\n\n  Your answer:"
  },
  {
    "objectID": "template_qmds/01-foundations-welcome-notes.html#exercise-4",
    "href": "template_qmds/01-foundations-welcome-notes.html#exercise-4",
    "title": "Collecting and Summarizing Data (Notes)",
    "section": "Exercise 4",
    "text": "Exercise 4\nIn no more than three sentences, describe what you learn about from the visual and numerical summaries. Try to write your description in a way that tells an interesting story about the people in this class.\n\n  Your answer:"
  },
  {
    "objectID": "template_qmds/01-foundations-welcome-notes.html#exercise-5",
    "href": "template_qmds/01-foundations-welcome-notes.html#exercise-5",
    "title": "Collecting and Summarizing Data (Notes)",
    "section": "Exercise 5",
    "text": "Exercise 5\nImagine that you took all the post-it notes in this room and organized them into a spreadsheet. What would each row in the underlying data set represent? What would each column of the data set represent? Check out the first related reading, linked at the top of this activity, if you need assistance!\n\n  Your answer:"
  },
  {
    "objectID": "template_qmds/01-foundations-welcome-notes.html#reflection",
    "href": "template_qmds/01-foundations-welcome-notes.html#reflection",
    "title": "Collecting and Summarizing Data (Notes)",
    "section": "Reflection",
    "text": "Reflection\nIn three to four sentences, reflect upon today’s activity. Some reflection prompts are found below:\n\nDo you think the context in which the data was collected influenced the results you found?\nWho would the numerical summaries you calculated potentially be represented of? Could you generalize the information you learned to a broader population (all students at Mac, perhaps), or would you have ethical concerns with generalizing your results?\nWhat (if anything) surprised you about the numerical summaries calculated by your group, or other groups?\nDid your data visualization help you better understand, or discover new things about the data that otherwise would have been difficult to distinguish? Why or why not?\n\n\n  Your answer:"
  },
  {
    "objectID": "template_qmds/01-foundations-welcome-notes.html#done",
    "href": "template_qmds/01-foundations-welcome-notes.html#done",
    "title": "Collecting and Summarizing Data (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Course Schedule",
    "section": "",
    "text": "The (tentative) course schedule below will be filled in throughout the semester. For each day, there will be written on what is due ahead of class time OR whats’ the big thing (like quizzes!). The “Announcements” column is (hopefully) self-explanatory. Any urgent announcements will be made over email (moodle).\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\nEnjoy, no class\n09/03: First day of class! Collecting and Summarizing Data  Before class:  - Read the course syllabus. I will assume that you understand all course policies beginning on Wednesday!  - Familiarize yourself with Moodle and the course website  - Complete Introductions Survey  - Walk through R Resources tab. Try downloading R and RStudio before coming to class!\n09/05: Univariate Visualization & Numerical Summaries  Before class:  - Download R and RStudio, and watch corresponding videos in the R Resources tab  - Complete the Introductions Survey (if you haven’t already!)  - Do either the readings or videos for today’s activity (Links in the Moodle checkpoint quiz)  - Complete today’s checkpoint on Moodle  During class:  - Begin Univariate Foundations Activity\nWelcome (back) to campus!\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n09/08: Simple Linear Regression: Discovery  Before class:  - Finish Univariate Foundations Activity outside of class  - Look through the Study Tips & Slides page on the Course Website – set a plan for yourself to be successful this semester!  - No checkpoint on Moodle for today!  During class:  - Begin SLR: Discovery Activity\n09/10: Simple Linear Regression: Formalization  Before class:  - Finish SLR: Discovery Activity outside of class  - Do either the readings or videos for today’s activity (Links in the Moodle checkpoint quiz)  - Complete today’s checkpoint on Moodle  During class:  - Begin SLR: Formalization Activity\n09/12: Simple Linear Regression: Model Evaluation  Before class:  - Finish SLR: Formalization Activity outside of class  - Do either the readings or videos for today’s activity (Links in the Moodle checkpoint quiz)  - Complete today’s checkpoint on Moodle  During class:  - Begin SLR: Model Evaluation Activity\n- PP#1 is posted on the Practice Problems tab (due Friday 09/12, 11:59pm CT)  - Last day to add/drop is Friday, 09/12\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n09/15: Simple Linear Regression: Transformation  Before class:  - Finish last class activities and CP Quiz on Moodles\n09/17: Simple Linear Regression: Categorcial Variable  Before class:  - Finish last class activities and CP Quiz on Moodles\n09/19: Mulitple Linear Regression Intro  Before class:  - Finish last class activities\n- PP#2 is posted on the Practice Problems tab (due Friday 09/20, 11:59pm CT)\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n09/22: Multiple Linear Regression: Discovery \n09/24: Quiz 01\n09/26: Mulitple Linear Regression Discovery\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n09/29: MLR principles  Before class:  - CP Quiz on Moodles\n**10/01: MLR principles + MLR confounding*  Before class:  - CP Quiz on Moodle\n10/03: MLR confounding\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n10/06: MLR interaction  Before class:  - CP Quiz on Moodles\n10/08: MLR interaction + MLR Model Building  Before class:  - CP Quiz on Moodle\n10/10: MLR Model Building\n- PP#3 is posted on the Practice Problems tab (due Friday 10/10, 11:59pm CT)\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n10/13: Logistic Univariate  Before class:  - CP Quiz on Moodles\n10/15: No class (Fall Break)  - PP#4 is posted on the Practice Problems tab (due Wednesday 10/15 at 11:59pm CT)\n10/10: No class (Fall Break)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n10/20: Bill out of town (Fall break continues!) \n10/22: Multiple Logistic   Before class:  - CP Quiz on Moodles\n10/24: Multiple Logistic + Review \n- PP#5 is posted on the Practice Problems tab (due Saturday, Oct 25 at 11:59pm CT)\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n10/27: Review \n10/29: Quiz 02\n10/31: Project Day\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n11/03: Tools for inference (Sampling-Normal)  Before class:  - CP Quiz on Moodles\n11/05: Tools for inference (Sampling-Normal) + Sampling Dist+CTL+Bootsrap   Before class:  - CP Quiz on Moodles\n11/07: Sampling Dist+CTL+Bootsrap \n- Project Milestone 1 due on Friday, Nov 07 at 11:59pm CT\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n11/10: Confidence intervals  Before class:  - CP Quiz on Moodles\n11/12: Confidence intervals + Hypothesis testing discovery (single coefficient) \n11/14: Hypothesis testing discovery (single coefficient)\n- PP#6 is posted on the Practice Problems tab (due Friday, Nov 14 at 11:59pm CT)\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 12\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n11/17: Hypothesis testing details (single coefficient)  Before class:  - CP Quiz on Moodles\n11/19: Hypothesis testing details (single coefficient) + Hypothesis testing consideration (single coefficient)\n11/21: Hypothesis testing consideration (single coefficient)\n- PP#7 is posted on the Practice Problems tab (due Friday, Nov 21 at 11:59pm CT)\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n11/24: Project Day\n11/26: ThanksGiving\n11/28: ThanksGiving\n- Project milestone 2 is due (on Moodle) Monday, Nov 24 at 11:59pm CT)\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 14\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n12/01: Hypothesis testing (F-test)  Before class:  - CP Quiz on Moodles\n12/03: Hypothesis testing (F-test) + P-values\n12/05: P-values\n- PP#8 is posted on the Practice Problems tab (due Saturday, Dec 06 at 11:59pm CT)\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonday\nWednesday\nFriday\nAnnouncements\n\n\n\n\n12/08: Project Day\n12/10: Project Day\nStudy Day\n- Project Final paper (.pdf and .qmd; one from each group) and Project reflection (each student) is due (on Moodle) Wednesday, Dec 10 at 11:59pm CT)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "",
    "text": "STAT 155: Introduction to Statistical Modeling\nMacalester College, Fall 2025\nStatistics is not just about theories & numbers — it’s about making sense of the world.\n\n\n\n\n\n\n📖 A Thought from H. G. Wells\n\n\n\n\n“Statistical thinking will one day be as necessary for efficient citizenship as the ability to read and write.”\n\n\n\nWelcome to the world of Statistics! In this course, you’ll learn how to analyze data, test research hypotheses, and make predictions in ways that matter.\nInstructor: Md Mutasim Billah  Class meeting times:\n\nSection 03: M/W/F 09:40-10:40am, THTR 002\nSection 04: M/W/F 12:00-01:00pm, THTR 202\n\nInstructor’s drop-in (office) hours:\n\nLocation: My office (OLRI 234)\nTimes: M/W: 01:30-02:30 pm.\nBy Appointment: I’m also happy to meet one-on-one! Shoot me an email and we can arrange it either in-person or over zoom, password: 123456.\nEmail Response Time: I do my best to reply to emails promptly during weekdays.\nPlease note that messages sent after 4:00 pm or on weekends may take longer to receive a response.\n\n\nSTAT 155 Preceptor Office Hours: There is a link to a Google Calendar containing all preceptor office hours available at the top of the course Moodle page!\nR/RStudio Preceptor Office Hours: Available on the MSCS Events google calendar (not to be used for questions about course content, only Data and R-related things!)\n\nThis course website will be updated throughout the semester with new activities, assignments, and announcements, so please bookmark this page if you are enrolled in the course!\nIf you find any typos, bugs, dead links, or have other questions, please email mbillah@macalester.edu"
  },
  {
    "objectID": "activities/25-f-tests.html",
    "href": "activities/25-f-tests.html",
    "title": "F-Tests",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#nested-models",
    "href": "activities/25-f-tests.html#nested-models",
    "title": "F-Tests",
    "section": "Nested Models",
    "text": "Nested Models\n“Nested models” are models where the covariates of one model are a subset of another model. As an example, consider the following models for estimating the association between forced expiratory volume (FEV) and smoking status:\nModel 1:\n\\[\nE[FEV \\mid smoke] = \\beta_0 + \\beta_1 smoke\n\\]\nModel 2:\n\\[\nE[FEV \\mid smoke, age] = \\beta_0 + \\beta_1 smoke + \\beta_2 age\n\\] Here, Model 1 is “nested” inside Model 2, since the covariates included in Model 1 (only smoke) are a subset of those in Model 2 (both smoke and age).\nAn example of non-nested models are…\nModel 3:\n\\[\nE[FEV \\mid smoke, height] = \\beta_0 + \\beta_1 smoke + \\beta_2 height\n\\]\nModel 4:\n\\[\nE[FEV \\mid smoke, sex] = \\beta_0 + \\beta_1 smoke + \\beta_2 sex\n\\]\nHere, even though Model 3 and Model 4 both contain smoke as explanatory variables, neither is nested in the other, since sex is not a part of Model 3, and height is not a part of Model 4.",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#learning-goals",
    "href": "activities/25-f-tests.html#learning-goals",
    "title": "F-Tests",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this activity, you should be able to:\n\nDetermine if one model is nested within another\nDetermine which null and alternative hypotheses require an f-test\nDetermine which f-tests require the use of the anova function in R vs. the overall f-test given in regular regression output\nInterpret the results of an f-test in context",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#readings-and-videos",
    "href": "activities/25-f-tests.html#readings-and-videos",
    "title": "F-Tests",
    "section": "Readings and videos",
    "text": "Readings and videos\nPlease read the following notes and watch the following video before class:\n\nReading: Section 7.3.4 in the STAT 155 Notes\nVideo: F-Tests (script)",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#exercise-1-nested-models",
    "href": "activities/25-f-tests.html#exercise-1-nested-models",
    "title": "F-Tests",
    "section": "Exercise 1: Nested Models",
    "text": "Exercise 1: Nested Models\n\nWhich of the following models are nested in the model \\(E[A \\mid B, C, D] = \\beta_0 + \\beta_1 D + \\beta_2 B + \\beta_3 C + \\beta_4 B * C\\)?\n\n\nModel 1: \\(E[A \\mid B] = \\beta_0 + \\beta_1 B\\)\nModel 2: \\(E[A \\mid B, D] = \\beta_0 + \\beta_1 B + \\beta_2 D\\)\nModel 3: \\(E[B \\mid C] = \\beta_0 + \\beta_1 C\\)\nModel 4: \\(E[A \\mid B, C, D] = \\beta_0 + \\beta_1 B + \\beta_2 C + \\beta_3 D\\)\nModel 5: \\(E[A \\mid B, C, D] = \\beta_0 + \\beta_1 C + \\beta_2 B + \\beta_3 D + \\beta_4 B * D\\)\nModel 6: \\(E[A \\mid D] = \\beta_0 + \\beta_1 D\\)\n\n\nConsider the following models involving variables A, B, C, and D:\n\n\nModel 1: \\(E[A \\mid B] = \\beta_0 + \\beta_1 B\\)\nModel 2: \\(E[A \\mid B, C] = \\beta_0 + \\beta_1 B + \\beta_2 C\\)\nModel 3: \\(E[A \\mid B, C] = \\beta_0 + \\beta_1 B + \\beta_2 C + \\beta_3 BC\\)\nModel 4: \\(E[A \\mid C, D] = \\beta_0 + \\beta_1 C + \\beta_2 D\\)\nModel 5: \\(E[B \\mid A] = \\beta_0 + \\beta_1 A\\)\nModel 6: \\(E[B \\mid A, C] = \\beta_0 + \\beta_1 A + \\beta_2 C + \\beta_3 AC\\)\n\nDetermine for each of the following statements whether that statement is True or False.\n\nModel 1 is nested in Model 2\nModel 1 is nested in Model 3\nModel 1 is nested in Model 4\nModel 2 is nested in Model 3\nModel 3 is nested in Model 2\nModel 2 is nested in Model 6\n\n\nWhat is one (numeric) way to compare nested models? Explain how you would determine which model is “better” based on this metric.",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#exercise-2-f-tests",
    "href": "activities/25-f-tests.html#exercise-2-f-tests",
    "title": "F-Tests",
    "section": "Exercise 2: F-Tests",
    "text": "Exercise 2: F-Tests\nThis exercise involves the MacGrades.csv dataset, which contains a sub-sample (to help preserve anonymity) of every grade assigned to a former Macalester graduating class. For each of the 6414 rows of data, the following information is provided (with a few missing values):\n\nsessionID: A section ID number\nsid: A student ID number\ngrade: The grade obtained, as a numerical value (i.e. an A is a 4, an A- is a 3.67, etc.)\ndept: A department identifier (these have been made ambiguous to maintain anonymity)\nlevel: The course level (e.g. 100-, 200-, 300-, and 600-)\nsem: A semester identifier\nenroll: The section enrollment\niid: An instructor identifier (these have been made ambiguous to maintain anonymity)\n\n\n# Load packages & data\nlibrary(tidyverse)\nMacGrades &lt;- read_csv(\"https://mac-stat.github.io/data/MacGrades.csv\")\n\nNOTE: Questions (a) and (b), since they are exploratory in nature, can suck up a lot of time if you let them! For the sake of getting to the rest of the activity, please spend no more than ~5 minutes on them.\n\nHypothesize two relationships between the variables in the dataset (pick any two relationships you want!). Your response should be written in a paragraph form.\n\n\nResponse Put your response here\n\n\nExplore the relationship between course grades and other variables in the data. Make two visualizations, and describe any patterns you observe.\n\n\n# Exploratory plots\n\n# course grade vs. enrollment\nMacGrades %&gt;%\n  ggplot(aes(enroll, grade)) +\n  geom_jitter() +\n  theme_classic() +\n  ggtitle(\"Course grades by enrollment numbers\")\n\n\n\n\n\n\n\n\n# course grade vs. level\nMacGrades %&gt;%\n  mutate(level = factor(level)) %&gt;%\n  ggplot(aes(y = grade, x = level)) +\n  geom_boxplot() +\n  ggtitle(\"Course grades by course level\")\n\n\n\n\n\n\n\n\n# course grade vs. level (treating grade as categorical)\nMacGrades %&gt;%\n  filter(!is.na(grade)) %&gt;%\n  mutate(level = factor(level),\n         grade = factor(grade)) %&gt;%\n  ggplot(aes(x = level, fill = grade)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_viridis_d(option = \"H\") +\n  theme_classic() +\n  ggtitle(\"Course grades by course level\")\n\n\n\n\n\n\n\n\n\n# course grade vs. semester\nMacGrades %&gt;%\n  filter(!is.na(grade)) %&gt;%\n  mutate(grade = factor(grade)) %&gt;%\n  ggplot(aes(x = sem, fill = grade)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_viridis_d(option = \"H\") +\n  theme_classic() +\n  ggtitle(\"Course grades by semester\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n# Let's do something fancy and check out how grades have changed over time... this will require\n# some string manipulation\n\nMacGrades$year &lt;- MacGrades$sem %&gt;%\n  str_replace(\"FA\", \"\") %&gt;%\n  str_replace(\"SP\", \"\") %&gt;%\n  str_replace(\"S1\", \"\") %&gt;%\n  str_replace(\"S2\", \"\") %&gt;% \n  str_replace(\"IT\", \"\") %&gt;% as.numeric()\n\nMacGrades %&gt;%\n  ggplot(aes(year, grade)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_classic() +\n  ggtitle(\"Course grades by year, with least-squares line\")\n\n\n\n\n\n\n\n\n\nClass Notes!\n\n\nNote that the level variable is currently quantitative. For this activity, we’d like to treat it as categorical. Create a new variable level_cat so that we can consider level categorically in the following analysis.\n\n\n# Make level categorical\nMacGrades &lt;- MacGrades %&gt;%\n  mutate(level = factor(level))\n\n\nSuppose we are interested in the relationship between course level (categorical) and student grades. Using grade as your outcome variable, fit a linear regression model to investigate this question.\n\n\nmod &lt;- lm(grade ~ level, data = MacGrades)\nsummary(mod)\n## \n## Call:\n## lm(formula = grade ~ level, data = MacGrades)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.4776 -0.3492  0.2089  0.5224  0.6508 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3.34924    0.01208 277.166  &lt; 2e-16 ***\n## level200     0.11183    0.01995   5.606 2.17e-08 ***\n## level300     0.09078    0.01949   4.659 3.25e-06 ***\n## level400     0.12835    0.03168   4.052 5.15e-05 ***\n## level600     0.63339    0.13624   4.649 3.41e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5915 on 5704 degrees of freedom\n##   (437 observations deleted due to missingness)\n## Multiple R-squared:  0.01085,    Adjusted R-squared:  0.01016 \n## F-statistic: 15.65 on 4 and 5704 DF,  p-value: 9.713e-13\n\nComment on the nature of the relationship between course level and student grades (this should not be a coefficient interpretation, but instead a description of a general trend, or lack thereof).\n\nState the null and alternative hypotheses associated with the research question in part (d).\n\n\\[\nH_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = 0\n\\]\n\\[\nH_1: \\text{One of } \\beta_1, \\beta_2, \\beta_3, \\beta_4 \\neq 0\n\\]\n\nIn words, the null is that there is no relationship between course level and course grades, and the alternative is that there is some relationship (either positive or negative) between course level and course grades.\n\n\nWhat is the p-value associated with this hypothesis test? Do we have enough evidence to reject the null hypothesis, using a significance threshold of 0.05?\n\n\nThe p-value associated with this hypothesis test is 9.713 x \\(10^{-13}\\). We do have enough evidence to reject the null hypothesis.\n\n\nSuppose we are interested in the relationship between course enrollment and student grades. Again, use grade as your outcome variable, and fit a linear regression model to investigate this question.\n\n\nmod2 &lt;- lm(grade ~ enroll, data = MacGrades)\nsummary(mod2)\n## \n## Call:\n## lm(formula = grade ~ enroll, data = MacGrades)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.4529 -0.3871  0.2265  0.5534  0.8448 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3.4842350  0.0173790 200.485  &lt; 2e-16 ***\n## enroll      -0.0031336  0.0006683  -4.689 2.81e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5934 on 5707 degrees of freedom\n##   (437 observations deleted due to missingness)\n## Multiple R-squared:  0.003838,   Adjusted R-squared:  0.003664 \n## F-statistic: 21.99 on 1 and 5707 DF,  p-value: 2.806e-06\n\n\nState the null and alternative hypotheses associated with the research question in part (g).\n\n\\[\nH_0:\n\\]\n\\[\nH_1:\n\\]\n\nWhat is the p-value associated with this hypothesis test? Do we have enough evidence to reject the null hypothesis, using a significance threshold of 0.05?\n\n\nDo we need to conduct a nested F-test using the anova function to complete our hypothesis testing procedure for the research question posed in part (g)? Explain why or why not.",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#exercise-3-more-f-tests",
    "href": "activities/25-f-tests.html#exercise-3-more-f-tests",
    "title": "F-Tests",
    "section": "Exercise 3: More F-tests",
    "text": "Exercise 3: More F-tests\n\nSuppose we are now interested in the association between course grade and enrollment for classes of the same level. Write a model statement in the form \\(E[Y | X] = ...\\) that will produce a statistical model that will allow us to answer our scientific question. Replace Y and X, where appropriate, with response and predictor variables.\n\n\\[\nE[grade | enroll, level] = \\beta_0 + \\beta_1 enroll + \\beta_2 level200 + \\beta_3 level300 + \\beta_4 level400 + \\beta_5 level600\n\\]\nWhich coefficient(s) in your model is the one that is relevant to your research question?\n\nThe relevant coefficient that answers our scientific question is \\(\\beta_1\\), or the coefficient that corresponds to enrollment.\n\n\nWhat are the relevant null and alternative hypotheses that address the scientific question in part (a)?\n\nThe relevant null and alternative hypotheses are:\n\\[\nH_0: \\beta_1 = 0\n\\] \\[\nH_1: \\beta_1 \\neq 0\n\\] We do not need to conduct an F-test to complete this hypothesis testing procedure, since our hypothesis involves only a single regression coefficient.\n\nFit the model you wrote in part (a), calculate a p-value, and report the results of the hypothesis test in part (b).\n\n\nmod &lt;- lm(grade ~ enroll + level, data = MacGrades)\nsummary(mod)\n## \n## Call:\n## lm(formula = grade ~ enroll + level, data = MacGrades)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.4916 -0.3481  0.1907  0.5162  0.7764 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3.4091632  0.0219143 155.568  &lt; 2e-16 ***\n## enroll      -0.0022628  0.0006907  -3.276 0.001058 ** \n## level200     0.1040873  0.0200701   5.186 2.22e-07 ***\n## level300     0.0743387  0.0201066   3.697 0.000220 ***\n## level400     0.1118660  0.0320492   3.490 0.000486 ***\n## level600     0.6182478  0.1361974   4.539 5.76e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.591 on 5703 degrees of freedom\n##   (437 observations deleted due to missingness)\n## Multiple R-squared:  0.01271,    Adjusted R-squared:  0.01185 \n## F-statistic: 14.69 on 5 and 5703 DF,  p-value: 2.453e-14\n\nWe have statistically significant evidence of a relationship between enrollment and course grade, for courses of the same level (p = 0.001058). We reject the null hypothesis that there is no relationship between enrollment and course grade, adjusting for course level.",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#reflection",
    "href": "activities/25-f-tests.html#reflection",
    "title": "F-Tests",
    "section": "Reflection",
    "text": "Reflection\nF-tests are useful when the null hypothesis you wish to test is such that more than one covariate is simultaneously equal to a specific number (typically zero).",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#exercise-4",
    "href": "activities/25-f-tests.html#exercise-4",
    "title": "F-Tests",
    "section": "Exercise 4",
    "text": "Exercise 4\nRepeat Exercise 3, supposing we are instead interested in the association between course grade and course level for classes of the same enrollment.\nOur model statement is identical to that in Exercise 3, but the relevant coefficients are \\(\\beta_2, \\beta_3, \\beta_4\\), and \\(\\beta_5\\).\nThe relevant null and alternative hypotheses are:\n\\[\nH_0: \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 0\n\\]\n\\[\nH_1: \\text{At least one of } \\beta_2, \\beta_3, \\beta_4, \\beta_5 \\neq 0\n\\]\nWe do need to conduct an F-test to complete this hypothesis testing procedure, since our hypothesis involves more than one regression coefficient.\n\n# Same model as in Question 10, we just now need to do an F-test!\nmod &lt;- lm(grade ~ enroll + level, data = MacGrades)\nsmaller_mod &lt;- lm(grade ~ enroll, data = MacGrades)\n\nanova(smaller_mod, mod)\n## Analysis of Variance Table\n## \n## Model 1: grade ~ enroll\n## Model 2: grade ~ enroll + level\n##   Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)    \n## 1   5707 2009.8                                 \n## 2   5703 1991.9  4    17.904 12.815 2.19e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe have statistically significant evidence of a relationship between course level and course grade, for courses of the same enrollment (\\(p = 2.19 \\times 10^{-10}\\)). We reject the null hypothesis that there is no relationship between course level and course grade, adjusting for enrollment.",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#exercise-5-reference-categories",
    "href": "activities/25-f-tests.html#exercise-5-reference-categories",
    "title": "F-Tests",
    "section": "Exercise 5: Reference categories",
    "text": "Exercise 5: Reference categories\nOur final research question pertains to whether or not there is a relationship between course grade and department. Again, use course grade as the outcome variable in your linear regression model.\n\nState the null and alternative hypotheses in colloquial language associated with the relevant hypothesis test.\n\nH0:There is no relationship between course grade and department.\nH1:There is some relationship between course grade and department.\n\nFit a linear regression model, and conduct your hypothesis testing procedure to answer the research question posed in this Exercise. State your conclusions accordingly (you do not need to interpret any regression coefficients, just state and interpret the results of your hypothesis test!).\n\n\nmod &lt;- lm(grade ~ dept, data = MacGrades)\nsummary(mod)\n## \n## Call:\n## lm(formula = grade ~ dept, data = MacGrades)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.5268 -0.2749  0.1475  0.4515  0.9039 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3.5000000  0.4088676   8.560   &lt;2e-16 ***\n## deptb       -0.2536066  0.4155163  -0.610    0.542    \n## deptB       -0.2614286  0.4278948  -0.611    0.541    \n## deptC        0.0268398  0.4106338   0.065    0.948    \n## deptd       -0.1825995  0.4098240  -0.446    0.656    \n## deptD        0.0617623  0.4105399   0.150    0.880    \n## depte       -0.0162500  0.4122607  -0.039    0.969    \n## deptE        0.1391667  0.4416275   0.315    0.753    \n## deptF       -0.2251373  0.4104679  -0.548    0.583    \n## deptg        0.0956522  0.4176615   0.229    0.819    \n## deptG       -0.3303306  0.4105537  -0.805    0.421    \n## deptH       -0.0307813  0.4120495  -0.075    0.940    \n## depti       -0.1474011  0.4111711  -0.358    0.720    \n## deptI        0.0011538  0.4243020   0.003    0.998    \n## deptj       -0.0625248  0.4108867  -0.152    0.879    \n## deptJ       -0.2815172  0.4116777  -0.684    0.494    \n## deptk        0.0124521  0.4104311   0.030    0.976    \n## deptK       -0.2431624  0.4123474  -0.590    0.555    \n## deptL        0.0468000  0.4169648   0.112    0.911    \n## deptm        0.0224798  0.4099682   0.055    0.956    \n## deptM       -0.4039440  0.4099067  -0.985    0.324    \n## deptn        0.1487363  0.4111080   0.362    0.718    \n## deptN        0.1670000  0.4139469   0.403    0.687    \n## depto       -0.3616667  0.4255629  -0.850    0.395    \n## deptO        0.0066856  0.4100242   0.016    0.987    \n## deptp       -0.0139370  0.4120744  -0.034    0.973    \n## deptP       -0.1140000  0.4249077  -0.268    0.788    \n## deptq        0.0001132  0.4104076   0.000    1.000    \n## deptQ       -0.0644094  0.4120744  -0.156    0.876    \n## deptR       -0.0381579  0.4110139  -0.093    0.926    \n## depts       -0.0154839  0.4218507  -0.037    0.971    \n## deptS       -0.0247500  0.4139469  -0.060    0.952    \n## deptt       -0.0126923  0.4166562  -0.030    0.976    \n## deptT       -0.2733962  0.4165106  -0.656    0.512    \n## deptU       -0.1136842  0.4298486  -0.264    0.791    \n## deptV       -0.0242500  0.4189646  -0.058    0.954    \n## deptW       -0.0507164  0.4100863  -0.124    0.902    \n## deptX       -0.1189865  0.4116209  -0.289    0.773    \n## deptY        0.0814894  0.4174763   0.195    0.845    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5782 on 5670 degrees of freedom\n##   (437 observations deleted due to missingness)\n## Multiple R-squared:  0.06037,    Adjusted R-squared:  0.05407 \n## F-statistic: 9.586 on 38 and 5670 DF,  p-value: &lt; 2.2e-16\n\n\nAre any of the individual department p-values significant?\n\nWhat do these p-values tell us, and why is this not contradictory to your answer in part (b)?",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#exercise-6-additional-exercise",
    "href": "activities/25-f-tests.html#exercise-6-additional-exercise",
    "title": "F-Tests",
    "section": "Exercise 6: Additional Exercise",
    "text": "Exercise 6: Additional Exercise\n\n# Read in mushroom data\nlibrary(tidyverse)\nlibrary(ggmosaic)\nmushrooms &lt;- read_csv(\"https://Mac-STAT.github.io/data/mushrooms.csv\")\n\n\nmod &lt;- glm(poisonous ~ cap_shape + gill_size, data = mushrooms, family = binomial)\nsummary(mod)\n## \n## Call:\n## glm(formula = poisonous ~ cap_shape + gill_size, family = binomial, \n##     data = mushrooms)\n## \n## Coefficients:\n##                   Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)       -2.30082    0.16000 -14.380  &lt; 2e-16 ***\n## cap_shapeconical  13.89690  441.37172   0.031    0.975    \n## cap_shapeconvex    1.45948    0.16476   8.858  &lt; 2e-16 ***\n## cap_shapeflat      1.65157    0.16517   9.999  &lt; 2e-16 ***\n## cap_shapeknobbed   1.39075    0.19087   7.286 3.18e-13 ***\n## cap_shapesunken  -15.23523  156.04855  -0.098    0.922    \n## gill_sizenarrow    2.96999    0.07634  38.904  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 11252  on 8123  degrees of freedom\n## Residual deviance:  8376  on 8117  degrees of freedom\n## AIC: 8390\n## \n## Number of Fisher Scoring iterations: 13\n\nDo we have an F-test row at the end?\nIf we want to test the hypothesis which addresses whether or not the log(odds) / odds of being poisonous differ for all cap shaped mushrooms, after adjusting for gill size.\n\nmod_small &lt;- glm(poisonous ~ gill_size, data = mushrooms, family = binomial)\nanova(mod_small, mod, test=\"LRT\")\n## Analysis of Deviance Table\n## \n## Model 1: poisonous ~ gill_size\n## Model 2: poisonous ~ cap_shape + gill_size\n##   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n## 1      8122     8659.7                          \n## 2      8117     8376.0  5   283.67 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#exercise-1-nested-models-1",
    "href": "activities/25-f-tests.html#exercise-1-nested-models-1",
    "title": "F-Tests",
    "section": "Exercise 1: Nested Models",
    "text": "Exercise 1: Nested Models\n\nModels 1, 2, 4 and 6.\n\n\n\nModel 1 is nested in Model 2 TRUE\nModel 1 is nested in Model 3 TRUE\nModel 1 is nested in Model 4 FALSE\nModel 2 is nested in Model 3 TRUE\nModel 3 is nested in Model 2 FALSE\nModel 2 is nested in Model 6 FALSE\n\n\nYou could compare the Adjusted \\(R^2\\) values from each model, and note that the one with a higher adjusted \\(R^2\\) is better by this metric. Multiple \\(R^2\\) would not be a good metric, because the larger model (within the nesting structure) will always have a higher \\(R^2\\) value.",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#exercise-2-f-tests-1",
    "href": "activities/25-f-tests.html#exercise-2-f-tests-1",
    "title": "F-Tests",
    "section": "Exercise 2: F-Tests",
    "text": "Exercise 2: F-Tests\n\nIt is reasonable to assume that course grade varies by department as well as course level and instructor. Certain instructors may grade more strictly (or curve more) than others, and similarly, this can vary across department due to cultural norms within the department. As the level of a course gets higher, I would expect grades to perhaps get lower, since courses with higher numbers are expected to be more difficult. Then again, students perhaps “care” more about such courses, and may put in more effort to get a higher grade. I doubt semester plays a significant role in determining course grades, though it is possible that Fall semester first-years or Spring semester seniors have worse grades, on average. We don’t have course year as a variable in our data, so we would be unable to examine this relationship. As enrollment in a course goes up, I would expect grades to decrease, since professors have less time to dedicate to individual students when course enrollment is high.\nExplore the relationship between course grades and other variables in the data. Make at least four visualizations, and describe any patterns you observe.\n\n\n# Exploratory plots\n\n# course grade vs. enrollment\nMacGrades %&gt;%\n  ggplot(aes(enroll, grade)) +\n  geom_jitter() +\n  theme_classic() +\n  ggtitle(\"Course grades by enrollment numbers\")\n\n\n\n\n\n\n\n\n# course grade vs. level\nMacGrades %&gt;%\n  mutate(level = factor(level)) %&gt;%\n  ggplot(aes(y = grade, x = level)) +\n  geom_boxplot() +\n  ggtitle(\"Course grades by course level\")\n\n\n\n\n\n\n\n\n# course grade vs. level (treating grade as categorical)\nMacGrades %&gt;%\n  filter(!is.na(grade)) %&gt;%\n  mutate(level = factor(level),\n         grade = factor(grade)) %&gt;%\n  ggplot(aes(x = level, fill = grade)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_viridis_d(option = \"H\") +\n  theme_classic() +\n  ggtitle(\"Course grades by course level\")\n\n\n\n\n\n\n\n\n# course grade vs. semester\nMacGrades %&gt;%\n  filter(!is.na(grade)) %&gt;%\n  mutate(grade = factor(grade)) %&gt;%\n  ggplot(aes(x = sem, fill = grade)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_viridis_d(option = \"H\") +\n  theme_classic() +\n  ggtitle(\"Course grades by semester\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n# Let's do something fancy and check out how grades have changed over time... this will require\n# some string manipulation\n\nMacGrades$year &lt;- MacGrades$sem %&gt;%\n  str_replace(\"FA\", \"\") %&gt;%\n  str_replace(\"SP\", \"\") %&gt;%\n  str_replace(\"S1\", \"\") %&gt;%\n  str_replace(\"S2\", \"\") %&gt;% \n  str_replace(\"IT\", \"\") %&gt;% as.numeric()\n\nMacGrades %&gt;%\n  ggplot(aes(year, grade)) +\n  geom_jitter() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_classic() +\n  ggtitle(\"Course grades by year, with least-squares line\")\n\n\n\n\n\n\n\n\nIn general, course grades seem to be associated with enrollment numbers. Specifically, when enrollments are greater than 50, we see very few students receiving a course grade lower than a 2.0, which is different than when enrollments are fewer than 50 students. There does not appear to be a clear relationship between course grade and course level, with the exception of 600-level courses. In these cases, every student received either an A or A-. It does seem like the proportion of students who received lower than a 2.67 is greater for 100-level courses than the other course levels.\n\n\n\n\n# Make level categorical\nMacGrades &lt;- MacGrades %&gt;%\n  mutate(level = factor(level))\n\n\n\n\n\nmod &lt;- lm(grade ~ level, data = MacGrades)\nsummary(mod)\n## \n## Call:\n## lm(formula = grade ~ level, data = MacGrades)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.4776 -0.3492  0.2089  0.5224  0.6508 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3.34924    0.01208 277.166  &lt; 2e-16 ***\n## level200     0.11183    0.01995   5.606 2.17e-08 ***\n## level300     0.09078    0.01949   4.659 3.25e-06 ***\n## level400     0.12835    0.03168   4.052 5.15e-05 ***\n## level600     0.63339    0.13624   4.649 3.41e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5915 on 5704 degrees of freedom\n##   (437 observations deleted due to missingness)\n## Multiple R-squared:  0.01085,    Adjusted R-squared:  0.01016 \n## F-statistic: 15.65 on 4 and 5704 DF,  p-value: 9.713e-13\n\nWe observe that as course level goes up, course grades also tend to increase on average.\n\nState the null and alternative hypotheses associated with the research question in part (d).\n\n\\[\nH_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = 0\n\\]\n\\[\nH_1: \\text{One of } \\beta_1, \\beta_2, \\beta_3, \\beta_4 \\neq 0\n\\] In words, the null is that there is no relationship between course level and course grades, and the alternative is that there is some relationship (either positive or negative) between course level and course grades.\n\nThe p-value associated with this hypothesis test is 9.713 x \\(10^{-13}\\). We do have enough evidence to reject the null hypothesis.\n\n\n\nmod &lt;- lm(grade ~ enroll, data = MacGrades)\nsummary(mod)\n## \n## Call:\n## lm(formula = grade ~ enroll, data = MacGrades)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.4529 -0.3871  0.2265  0.5534  0.8448 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3.4842350  0.0173790 200.485  &lt; 2e-16 ***\n## enroll      -0.0031336  0.0006683  -4.689 2.81e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5934 on 5707 degrees of freedom\n##   (437 observations deleted due to missingness)\n## Multiple R-squared:  0.003838,   Adjusted R-squared:  0.003664 \n## F-statistic: 21.99 on 1 and 5707 DF,  p-value: 2.806e-06\n\n\n\n\n\\[\nH_0: \\beta_1 = 0\n\\]\n\\[\nH_1: \\beta_1 \\neq 0\n\\]\n\nThe p-value associated with this hypothesis test is 2.806 x \\(10^{-06}\\). We do have enough evidence to reject the null hypothesis. Note that this p-value could be obtained from either the overall model fit or from the individual coefficient for enroll (they are the same). They may be ever so slightly different when there are few observations in your dataset, but when there are a lot, they will be exactly identical.\n\n\nWe do not need to conduct an F-test, because our hypothesis test involves only a single regression coefficient, and therefore is readily obtained from the summary output of our linear model in R.",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#exercise-3-more-f-tests-1",
    "href": "activities/25-f-tests.html#exercise-3-more-f-tests-1",
    "title": "F-Tests",
    "section": "Exercise 3: More F-tests",
    "text": "Exercise 3: More F-tests\n\n\n\n\\[\nE[grade | enroll, level] = \\beta_0 + \\beta_1 enroll + \\beta_2 level200 + \\beta_3 level300 + \\beta_4 level400 + \\beta_5 level600\n\\]\nThe relevant coefficient that answers our scientific question is \\(\\beta_1\\), or the coefficient that corresponds to enrollment.\n\n\n\nThe relevant null and alternative hypotheses are:\n\\[\nH_0: \\beta_1 = 0\n\\] \\[\nH_1: \\beta_1 \\neq 0\n\\] We do not need to conduct an F-test to complete this hypothesis testing procedure, since our hypothesis involves only a single regression coefficient.\n\n\n\n\nmod &lt;- lm(grade ~ enroll + level, data = MacGrades)\nsummary(mod)\n## \n## Call:\n## lm(formula = grade ~ enroll + level, data = MacGrades)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.4916 -0.3481  0.1907  0.5162  0.7764 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3.4091632  0.0219143 155.568  &lt; 2e-16 ***\n## enroll      -0.0022628  0.0006907  -3.276 0.001058 ** \n## level200     0.1040873  0.0200701   5.186 2.22e-07 ***\n## level300     0.0743387  0.0201066   3.697 0.000220 ***\n## level400     0.1118660  0.0320492   3.490 0.000486 ***\n## level600     0.6182478  0.1361974   4.539 5.76e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.591 on 5703 degrees of freedom\n##   (437 observations deleted due to missingness)\n## Multiple R-squared:  0.01271,    Adjusted R-squared:  0.01185 \n## F-statistic: 14.69 on 5 and 5703 DF,  p-value: 2.453e-14\n\nWe have statistically significant evidence of a relationship between enrollment and course grade, for courses of the same level (p = 0.001058). We reject the null hypothesis that there is no relationship between enrollment and course grade, adjusting for course level.",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#exercise-4-1",
    "href": "activities/25-f-tests.html#exercise-4-1",
    "title": "F-Tests",
    "section": "Exercise 4",
    "text": "Exercise 4\nOur model statement is identical to that in Exercise 3, but the relevant coefficients are \\(\\beta_2, \\beta_3, \\beta_4\\), and \\(\\beta_5\\).\nThe relevant null and alternative hypotheses are:\n\\[\nH_0: \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 0\n\\]\n\\[\nH_1: \\text{At least one of } \\beta_2, \\beta_3, \\beta_4, \\beta_5 \\neq 0\n\\]\nWe do need to conduct an F-test to complete this hypothesis testing procedure, since our hypothesis involves more than one regression coefficient.\n\n# Same model as in Question 10, we just now need to do an F-test!\nmod &lt;- lm(grade ~ enroll + level, data = MacGrades)\nsmaller_mod &lt;- lm(grade ~ enroll, data = MacGrades)\n\nanova(smaller_mod, mod)\n## Analysis of Variance Table\n## \n## Model 1: grade ~ enroll\n## Model 2: grade ~ enroll + level\n##   Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)    \n## 1   5707 2009.8                                 \n## 2   5703 1991.9  4    17.904 12.815 2.19e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWe have statistically significant evidence of a relationship between course level and course grade, for courses of the same enrollment (\\(p = 2.19 \\times 10^{-10}\\)). We reject the null hypothesis that there is no relationship between course level and course grade, adjusting for enrollment.",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/25-f-tests.html#exercise-5-reference-categories-1",
    "href": "activities/25-f-tests.html#exercise-5-reference-categories-1",
    "title": "F-Tests",
    "section": "Exercise 5: Reference categories",
    "text": "Exercise 5: Reference categories\n\n\n\nH0: There is no relationship between course grade and department.\nH1: There is some relationship between course grade and department.\n\n\n\n\nmod &lt;- lm(grade ~ dept, data = MacGrades)\nsummary(mod)\n## \n## Call:\n## lm(formula = grade ~ dept, data = MacGrades)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.5268 -0.2749  0.1475  0.4515  0.9039 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3.5000000  0.4088676   8.560   &lt;2e-16 ***\n## deptb       -0.2536066  0.4155163  -0.610    0.542    \n## deptB       -0.2614286  0.4278948  -0.611    0.541    \n## deptC        0.0268398  0.4106338   0.065    0.948    \n## deptd       -0.1825995  0.4098240  -0.446    0.656    \n## deptD        0.0617623  0.4105399   0.150    0.880    \n## depte       -0.0162500  0.4122607  -0.039    0.969    \n## deptE        0.1391667  0.4416275   0.315    0.753    \n## deptF       -0.2251373  0.4104679  -0.548    0.583    \n## deptg        0.0956522  0.4176615   0.229    0.819    \n## deptG       -0.3303306  0.4105537  -0.805    0.421    \n## deptH       -0.0307813  0.4120495  -0.075    0.940    \n## depti       -0.1474011  0.4111711  -0.358    0.720    \n## deptI        0.0011538  0.4243020   0.003    0.998    \n## deptj       -0.0625248  0.4108867  -0.152    0.879    \n## deptJ       -0.2815172  0.4116777  -0.684    0.494    \n## deptk        0.0124521  0.4104311   0.030    0.976    \n## deptK       -0.2431624  0.4123474  -0.590    0.555    \n## deptL        0.0468000  0.4169648   0.112    0.911    \n## deptm        0.0224798  0.4099682   0.055    0.956    \n## deptM       -0.4039440  0.4099067  -0.985    0.324    \n## deptn        0.1487363  0.4111080   0.362    0.718    \n## deptN        0.1670000  0.4139469   0.403    0.687    \n## depto       -0.3616667  0.4255629  -0.850    0.395    \n## deptO        0.0066856  0.4100242   0.016    0.987    \n## deptp       -0.0139370  0.4120744  -0.034    0.973    \n## deptP       -0.1140000  0.4249077  -0.268    0.788    \n## deptq        0.0001132  0.4104076   0.000    1.000    \n## deptQ       -0.0644094  0.4120744  -0.156    0.876    \n## deptR       -0.0381579  0.4110139  -0.093    0.926    \n## depts       -0.0154839  0.4218507  -0.037    0.971    \n## deptS       -0.0247500  0.4139469  -0.060    0.952    \n## deptt       -0.0126923  0.4166562  -0.030    0.976    \n## deptT       -0.2733962  0.4165106  -0.656    0.512    \n## deptU       -0.1136842  0.4298486  -0.264    0.791    \n## deptV       -0.0242500  0.4189646  -0.058    0.954    \n## deptW       -0.0507164  0.4100863  -0.124    0.902    \n## deptX       -0.1189865  0.4116209  -0.289    0.773    \n## deptY        0.0814894  0.4174763   0.195    0.845    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5782 on 5670 degrees of freedom\n##   (437 observations deleted due to missingness)\n## Multiple R-squared:  0.06037,    Adjusted R-squared:  0.05407 \n## F-statistic: 9.586 on 38 and 5670 DF,  p-value: &lt; 2.2e-16\n\nWe have statistically significant evidence of a relationship between department and course grades at a significance level of 0.05 (p-value &lt; 2.2 x \\(10^{-16}\\)). We reject the null hypothesis that there is no relationship between course grade and department.\n\n\n\nNone of the individual p-values for department are significant! These p-values tell us about whether or not there is a statistically significant difference in course grades between each respective department and the reference department (Department “A”). This doesn’t contradict our answer to part (b) because there are different hypothesis tests that answer different questions!",
    "crumbs": [
      "F-Tests"
    ]
  },
  {
    "objectID": "activities/23-hypothesis-testing-details.html",
    "href": "activities/23-hypothesis-testing-details.html",
    "title": "Hypothesis Testing- Details",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Hypothesis Testing- Details"
    ]
  },
  {
    "objectID": "activities/23-hypothesis-testing-details.html#learning-goals",
    "href": "activities/23-hypothesis-testing-details.html#learning-goals",
    "title": "Hypothesis Testing- Details",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nApply the procedure for a formal hypothesis test\nArticulate how we can formalize a research question as a testable, statistical hypothesis",
    "crumbs": [
      "Hypothesis Testing- Details"
    ]
  },
  {
    "objectID": "activities/23-hypothesis-testing-details.html#readings-and-videos",
    "href": "activities/23-hypothesis-testing-details.html#readings-and-videos",
    "title": "Hypothesis Testing- Details",
    "section": "Readings and videos",
    "text": "Readings and videos\nPlease complete the following reading or videos before class:\n\nReading: Section 7.3 (stop when you get to Section 7.3.4) in the STAT 155 Notes\nVideo 1: Introduction to Statistical Inference\nVideo 2: Hypothesis Testing Framework\nVideo 3: Hypothesis Testing Procedure",
    "crumbs": [
      "Hypothesis Testing- Details"
    ]
  },
  {
    "objectID": "activities/23-hypothesis-testing-details.html#exercise-1",
    "href": "activities/23-hypothesis-testing-details.html#exercise-1",
    "title": "Hypothesis Testing- Details",
    "section": "Exercise 1",
    "text": "Exercise 1\nResearch Question: Can we predict whether or not a mushroom is poisonous based on the shape of its cap?\nFor this exercise, we will look at data from various species of gilled mushrooms in the Agaricus and Lepiota Family. We have information on whether a mushroom is poisonous (TRUE if it is, FALSE if it’s edible), the shape of its cap (cap_shape, a categorical variable with 6 categories), the texture of its cap surface (cap_surface, a categorical variable with 4 categories), and the size of its gills (gill_size, a categorical variable with two categories)\n\n# Load the data & packages\nlibrary(tidyverse)\nmushrooms &lt;- read_csv(\"https://Mac-STAT.github.io/data/mushrooms.csv\")\n\nmushrooms &lt;- mushrooms %&gt;%\n  mutate(cap_shape = relevel(as.factor(cap_shape), ref=\"flat\")) %&gt;%\n  dplyr::select(poisonous, cap_shape)\n\nhead(mushrooms)\n## # A tibble: 6 × 2\n##   poisonous cap_shape\n##   &lt;lgl&gt;     &lt;fct&gt;    \n## 1 TRUE      convex   \n## 2 FALSE     convex   \n## 3 FALSE     bell     \n## 4 TRUE      convex   \n## 5 FALSE     convex   \n## 6 FALSE     convex\n\n\nPart a\nOne of the most poisonous species of mushrooms is the Amanita phalloides or “Death Cap” mushroom, which typically has a flat cap shape when mature. Based on this anecdote, we hypothesize that species of mushrooms with flat caps in general associated with the likelihood of being poisonous.\nFirst, let’s translate this question to an appropriate null and alternative hypothesis that we can compare with a formal hypothesis test. Remember that poisonous is a binary outcome, so we need to frame our null and alternative hypotheses in terms of odds (i.e., Odds(poisionous | flat cap) = P(poisonous|flat cap)/P(edible | flat cap)).\n\n\\(H_0\\): Odds(poisonous | flat cap) = 1\n\\(H_a\\): Odds(poisonous | flat cap) ≠ 1\n\n\n\nPart b\n\nFit a logistic regression model to investigate whether cap_shape is associated with a mushroom being poisonous. (Note that in the setup code above, we have forced the reference category for the cap_shape predictor to be flat; without this, the reference category by default would be set as bell, which is the first category when sorted alphabetically).\n\n\nmushroom_mod1 &lt;- glm(poisonous ~ cap_shape, data=mushrooms, family=\"binomial\")\n\ncoef(mushroom_mod1)\n##      (Intercept)    cap_shapebell cap_shapeconical  cap_shapeconvex \n##      -0.02538207      -2.10483179      14.59144985      -0.10609804 \n## cap_shapeknobbed  cap_shapesunken \n##       0.99296610     -14.54068570\n\n\n\nPart c\nProvide an appropriate interpretation of the intercept coefficient on the odds scale. Based on this interpretation, do you believe mushrooms with flat caps are more likely to be poisonous, or more likely to be edible?\n\nexp(coef(mushroom_mod1))\n##      (Intercept)    cap_shapebell cap_shapeconical  cap_shapeconvex \n##     9.749373e-01     1.218662e-01     2.172632e+06     8.993365e-01 \n## cap_shapeknobbed  cap_shapesunken \n##     2.699229e+00     4.842397e-07\n\n\nThe odds of a flat-capped mushroom being poisonous are 0.975:1–that is, mushrooms with flat caps are very slightly less likely to be poisonous than they are edible.\n\n\n\nPart d\nLet’s look at the full model summary:\n\nsummary(mushroom_mod1)\n## \n## Call:\n## glm(formula = poisonous ~ cap_shape, family = \"binomial\", data = mushrooms)\n## \n## Coefficients:\n##                   Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)       -0.02538    0.03563  -0.712   0.4762    \n## cap_shapebell     -2.10483    0.15677 -13.426   &lt;2e-16 ***\n## cap_shapeconical  14.59145  441.37169   0.033   0.9736    \n## cap_shapeconvex   -0.10610    0.04866  -2.180   0.0292 *  \n## cap_shapeknobbed   0.99297    0.08557  11.604   &lt;2e-16 ***\n## cap_shapesunken  -14.54069  156.04846  -0.093   0.9258    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 11252  on 8123  degrees of freedom\n## Residual deviance: 10702  on 8118  degrees of freedom\n## AIC: 10714\n## \n## Number of Fisher Scoring iterations: 13\n\nReport and interpret the test statistic for the intercept term (our coefficient of interest):\n\n\nThe test statistic is -0.712—this means that the coefficient estimate of interest is 0.712 standard errors away from (specifically, below) the null value of 0 (note that this is on the log-odds scale).\n\n\n\n\nPart e\n\nReport and interpret the p-value for the intercept term.\nBased on this p-value and a significance level of 0.05, do we have evidence that mushrooms with flat caps are more likely to be poisonous than edible?\n\n\nThe p-value for the intercept term is 0.4762.\n\n\nInterpretation: If the null hypothesis were true (i.e., the odds of being poisonous were 1), the probability of seeing a test statistic as or more extreme than |-0.712| is 0.4762. Because the p-value is greater than our significance level of 0.05, we have no evidence to suggest that a flat-capped mushroom is more or less likely to be poisonous.\n\n\n\nPart f\nNow suppose we are interested in whether the odds of being poisonous are different for mushrooms with other cap shapes.\nBy hand, calculate the odds of being poisonous for mushrooms with knobbed caps, conical caps, and sunken caps (remember that the non-exponentiated coefficients represent a difference in log-odds compared to the reference category):\n\nodds(poisonous | knobbed cap) =\n\n\nodds(poisonous | conical cap) =\n\n\nodds(poisonous | sunken cap) =\n\n\n\nPart g\nBased on these odds, which of the 4 mushroom cap shapes we’ve investigated (flat, knobbed, conical, and sunken) do you believe is the best indicator that it’s edible? Which cap shape do you expect is most likely to be poisonous?\n\nYour answer\n\n\n\nPart h\nLet’s get the full model summary again:\n\nsummary(mushroom_mod1)\n## \n## Call:\n## glm(formula = poisonous ~ cap_shape, family = \"binomial\", data = mushrooms)\n## \n## Coefficients:\n##                   Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)       -0.02538    0.03563  -0.712   0.4762    \n## cap_shapebell     -2.10483    0.15677 -13.426   &lt;2e-16 ***\n## cap_shapeconical  14.59145  441.37169   0.033   0.9736    \n## cap_shapeconvex   -0.10610    0.04866  -2.180   0.0292 *  \n## cap_shapeknobbed   0.99297    0.08557  11.604   &lt;2e-16 ***\n## cap_shapesunken  -14.54069  156.04846  -0.093   0.9258    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 11252  on 8123  degrees of freedom\n## Residual deviance: 10702  on 8118  degrees of freedom\n## AIC: 10714\n## \n## Number of Fisher Scoring iterations: 13\n\nNow report and interpret the p-values for the coefficients corresponding to cap_shapeknobbed, cap_shapeconical, and cap_shapesunken:\n\ncap_shapeknobbed: Our null hypothesis is that the odds ratio between flat-capped and knob-capped mushrooms is 1 (i.e., the odds of a knob-capped mushroom being poisonous are equal to the odds of a flat-capped mushroom being poisonous). If we assume the null hypothesis is true, then the probability of seeing a test statistic as or more extreme than |11.60| is (&lt;2e-16). Because the p-value is far below our significance level of 0.05, we take this as strong evidence that knob-capped mushrooms are much more likely to be poisonous than flat-capped mushrooms.\n\n\n** Your Response on cap_shapeconical **\n\n\n** Your Response on cap_shapesunken **\n\n\n\nPart i\nBased on the model summary output in part h above, if you were given a plate of mushrooms with different cap shapes and had to pick one to eat, which one would you choose? Which cap shape would you absolutely avoid at all costs? Are your decisions guided by the coefficient estimates, the p-values, or both?\n\nYour answer\n\n\n\nPart j\nLet’s look at the data a slightly different way, using a 6x2 table of counts:\n\nmushrooms %&gt;% \n  mutate(cap_shape=as.factor(cap_shape),\n         poisonous=as.factor(poisonous)) %&gt;%\n  dplyr::count(cap_shape, poisonous, .drop=FALSE) %&gt;% \n  pivot_wider(names_from=poisonous, values_from=n, names_prefix=\"Poisonous = \")\n## # A tibble: 6 × 3\n##   cap_shape `Poisonous = FALSE` `Poisonous = TRUE`\n##   &lt;fct&gt;                   &lt;int&gt;              &lt;int&gt;\n## 1 flat                     1596               1556\n## 2 bell                      404                 48\n## 3 conical                     0                  4\n## 4 convex                   1948               1708\n## 5 knobbed                   228                600\n## 6 sunken                     32                  0\n\nNow, if you were given a plate of mushrooms with different cap shapes and had to pick one shape to eat and one to absolutely avoid, would you choose the same shapes? Why or why not?\n\nYour answer",
    "crumbs": [
      "Hypothesis Testing- Details"
    ]
  },
  {
    "objectID": "activities/23-hypothesis-testing-details.html#exercise-2",
    "href": "activities/23-hypothesis-testing-details.html#exercise-2",
    "title": "Hypothesis Testing- Details",
    "section": "Exercise 2",
    "text": "Exercise 2\nFor this exercise, let’s return to the fish dataset from a previous activity.\n\nfish &lt;- read_csv(\"https://Mac-STAT.github.io/data/Mercury.csv\")\n\nhead(fish)\n## # A tibble: 6 × 5\n##   River  Station Length Weight Concen\n##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1 Lumber       0   47     1616   1.6 \n## 2 Lumber       0   48.7   1862   1.5 \n## 3 Lumber       0   55.7   2855   1.7 \n## 4 Lumber       0   45.2   1199   0.73\n## 5 Lumber       0   44.7   1320   0.56\n## 6 Lumber       0   43.8   1225   0.51\n\nResearch question: We believe the length of a fish (measured in centimeters) is causally associated with its mercury concentration (measured in parts per million [ppm]). We suspect that the river a fish is sampled from may be a confounder, since differences in the river environment may causally influence both the average length of fish (e.g. due to differences in water temperature or food availability) as well as mercury concentration (e.g. due to differences between the two rivers in mercury pollution levels).\n\nPart a\nFit a linear regression model that can be used to answer our research question.\n\nmod_fish1 &lt;- lm(Concen ~ Length + River, data=fish)\nsummary(mod_fish1)\n## \n## Call:\n## lm(formula = Concen ~ Length + River, data = fish)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.19298 -0.36849 -0.07677  0.30905  1.84773 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  -1.194229   0.216287  -5.521 1.26e-07 ***\n## Length        0.057657   0.005213  11.061  &lt; 2e-16 ***\n## RiverWacamaw  0.142027   0.089496   1.587    0.114    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5779 on 168 degrees of freedom\n## Multiple R-squared:  0.431,  Adjusted R-squared:  0.4243 \n## F-statistic: 63.63 on 2 and 168 DF,  p-value: &lt; 2.2e-16\n\n\n\nPart b\nInterpret the coefficient estimate, test statistic, and p-value for the RiverWacamaw coefficient. Assume we have specified a significance level of 0.05.\n\nResponse\n\n\n\nPart c\nSuppose we now want to determine if the causal effect of fish length on mercury concentration differs according to the river a fish was sampled from.\nFirst, modify the code chunk below to visualize the 3-way relationship between the Concen, Length, and River variables.\n\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen, colour = River)) + \n  geom_point()+\n  geom_smooth(method=\"lm\", se=F)\n\n\n\n\n\n\n\n\nNext, fit an appropriate linear regression model with an interaction term to investigate this question.\n\nmod_fish2 &lt;- lm(Concen ~ Length * River, data=fish)\nsummary(mod_fish2)\n## \n## Call:\n## lm(formula = Concen ~ Length * River, data = fish)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.27784 -0.35402 -0.08314  0.30650  1.94304 \n## \n## Coefficients:\n##                      Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)         -0.623875   0.325576  -1.916   0.0570 .  \n## Length               0.043185   0.008085   5.341 2.99e-07 ***\n## RiverWacamaw        -0.826291   0.426529  -1.937   0.0544 .  \n## Length:RiverWacamaw  0.024326   0.010483   2.321   0.0215 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5705 on 167 degrees of freedom\n## Multiple R-squared:  0.4488, Adjusted R-squared:  0.4389 \n## F-statistic: 45.33 on 3 and 167 DF,  p-value: &lt; 2.2e-16\n\n\n\nPart d\nInterpret the coefficient estimate, test statistic, and p-value for the RiverWacamaw:Length interaction term in this revised model (mod_fish2). Assume we’ve set a significance level of 0.05.\n\nResponse\n\n\n\nPart e\nInterpret the coefficient estimate, test statistic, and p-value for the RiverWacamaw coefficient in this revised model (mod_fish2). (again, you can assume we’ve set a significance level of 0.05).\n\n\nPart f (CHALLENGE)\nSuppose another researcher runs the same model we fit in part c above (mod_fish2), but they claim that a more appropriate alternative hypothesis should be Beta_1 &lt; 0, (and not Beta_1 ≠ 0, as is assumed by default when running a regression model). Because of this, they reported a smaller p-value for the coefficient, and claim that the Wacamaw River has a lower baseline mercury concentration (i.e., when Length = 0cm).\nWhat is the p-value they would have reported for the RiverWacamaw coefficient in mod_fish2?\n\nResponse\n\nWhat is a potential ethical problem with the other researcher’s claim that the alternative hypothesis should be Beta_1 &lt; 0?\n\nResponse\n\n\n\nPart g (CHALLENGE)\nYou point out to the other researcher that the intercept and RiverWacamaw coefficients are both negative, so whatever difference in mercury concentration between the two rivers your model predicts “at baseline” is not useful or meaningful–you cannot have a fish that is 0cm long, nor a mercury concentration &lt;0ppm.\nYou propose that a more appropriate model should transform the Length variable in some way to make the intercept more interpretable. Create a new variable named Length_adj with this transformation and use it to re-fit the model:\n\nmod_fish3 &lt;- lm(Concen ~ Length_adj*River, data=fish)\n## Error in eval(predvars, data, env): object 'Length_adj' not found\nsummary(mod_fish3)\n## Error: object 'mod_fish3' not found\n\nCompare the output of this model to that of mod_fish2. What happened to the estimate, test statistic, and p-value for the RiverWacamaw coefficient? How does this affect your conclusion? How about the other researcher’s conclusion?\n\nResponse",
    "crumbs": [
      "Hypothesis Testing- Details"
    ]
  },
  {
    "objectID": "activities/23-hypothesis-testing-details.html#exercise-1-1",
    "href": "activities/23-hypothesis-testing-details.html#exercise-1-1",
    "title": "Hypothesis Testing- Details",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nPart a\nOne of the most poisonous species of mushrooms is the Amanita phalloides or “Death Cap” mushroom, which typically has a flat cap shape when mature. Based on this anecdote, we hypothesize that species of mushrooms with flat caps in general may be more likely to be poisonous than edible.\nFirst, let’s translate this question to an appropriate null and alternative hypothesis that we can compare with a formal hypothesis test. Remember that poisonous is a binary outcome, so we need to frame our null and alternative hypotheses in terms of odds (i.e., Odds(poisonous | flat cap) = P(poisonous|flat cap)/P(edible | flat cap)).\n\n\\(H_0\\): Odds(poisonous | flat cap) = 1\n\\(H_a\\): Odds(poisonous | flat cap) ≠ 1\n\n\n\nPart b\n\nFit a logistic regression model to investigate whether cap_shape is associated with a mushroom being poisonous. (Note that in the setup code chunk above, we have forced the reference category for the cap_shape predictor to be flat; otherwise, the reference category by default would be set as bell, which is the first category when sorted alphabetically).\n\n\nmushroom_mod1 &lt;- glm(poisonous ~ cap_shape, data=mushrooms, family=\"binomial\")\n\ncoef(mushroom_mod1)\n##      (Intercept)    cap_shapebell cap_shapeconical  cap_shapeconvex \n##      -0.02538207      -2.10483179      14.59144985      -0.10609804 \n## cap_shapeknobbed  cap_shapesunken \n##       0.99296610     -14.54068570\n\n\n\nPart c\nProvide an appropriate interpretation of the intercept coefficient on the odds scale. Based on this interpretation, do you believe mushrooms with flat caps are more likely to be poisonous, or more likely to be edible?\n\nexp(coef(mushroom_mod1))\n##      (Intercept)    cap_shapebell cap_shapeconical  cap_shapeconvex \n##     9.749373e-01     1.218662e-01     2.172632e+06     8.993365e-01 \n## cap_shapeknobbed  cap_shapesunken \n##     2.699229e+00     4.842397e-07\n\n\nThe odds of a flat-capped mushroom being poisonous are 0.975:1–that is, mushrooms with flat caps are very slightly less likely to be poisonous than they are edible.\n\n\n\nPart d\nLet’s look at the full model summary:\n\nsummary(mushroom_mod1)\n## \n## Call:\n## glm(formula = poisonous ~ cap_shape, family = \"binomial\", data = mushrooms)\n## \n## Coefficients:\n##                   Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)       -0.02538    0.03563  -0.712   0.4762    \n## cap_shapebell     -2.10483    0.15677 -13.426   &lt;2e-16 ***\n## cap_shapeconical  14.59145  441.37169   0.033   0.9736    \n## cap_shapeconvex   -0.10610    0.04866  -2.180   0.0292 *  \n## cap_shapeknobbed   0.99297    0.08557  11.604   &lt;2e-16 ***\n## cap_shapesunken  -14.54069  156.04846  -0.093   0.9258    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 11252  on 8123  degrees of freedom\n## Residual deviance: 10702  on 8118  degrees of freedom\n## AIC: 10714\n## \n## Number of Fisher Scoring iterations: 13\n\nReport and interpret the test statistic for the intercept term (our coefficient of interest):\n\nThe test statistic is -0.712—this means that the coefficient estimate of interest is 0.712 standard errors away from (specifically, below) the null value of 0 (note that this is on the log-odds scale).\n\n\n\nPart e\n\nReport and interpret the p-value for the intercept term.\nBased on this p-value and a significance level of 0.05, do we have evidence that mushrooms with flat caps are more likely to be poisonous than edible?\n\n\nThe p-value for the intercept term is 0.4762.\n\n\nInterpretation: If the null hypothesis were true (i.e., the odds of being poisonous were 1), the probability of seeing a test statistic as or more extreme than |-0.712| is 0.4762. Because the p-value is greater than our significance level of 0.05, we have no evidence to suggest that a flat-capped mushroom is more or less likely to be poisonous.\n\n\n\nPart f\nNow suppose we are interested in whether the odds of being poisonous are different for mushrooms with other cap shapes.\nCalculate the odds of being poisonous for mushrooms with knobbed caps, conical caps, and sunken caps (remember that the non-exponentiated coefficients represent a difference in log-odds compared to the reference category):\n\n# knobbed\nexp(-0.025+0.992)\n## [1] 2.630042\n\n#conical\nexp(-0.025+14.59)\n## [1] 2115919\n\n#sunken\nexp(-0.025-14.54)\n## [1] 4.726078e-07\n\n\n\nPart g\nBased on these odds, which of the 4 mushroom cap shapes we’ve investigated (flat, knobbed, conical, and sunken) do you believe is the best indicator that it’s edible? Which cap shape do you expect is most likely to be poisonous?\n\nUsing only the coefficient estimates, it appears that mushrooms with a sunken cap shape appear to be most likely to be edible, as the odds they are poisonous are approximately \\(4.7 \\times 10^{-7}\\) to 1. Mushrooms with conical caps appear to be most likely to be poisonous (odds of being poisonous are &gt;2 million to 1).\n\n\n\nPart h\nLet’s get the full model summary again:\n\nsummary(mushroom_mod1)\n## \n## Call:\n## glm(formula = poisonous ~ cap_shape, family = \"binomial\", data = mushrooms)\n## \n## Coefficients:\n##                   Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)       -0.02538    0.03563  -0.712   0.4762    \n## cap_shapebell     -2.10483    0.15677 -13.426   &lt;2e-16 ***\n## cap_shapeconical  14.59145  441.37169   0.033   0.9736    \n## cap_shapeconvex   -0.10610    0.04866  -2.180   0.0292 *  \n## cap_shapeknobbed   0.99297    0.08557  11.604   &lt;2e-16 ***\n## cap_shapesunken  -14.54069  156.04846  -0.093   0.9258    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 11252  on 8123  degrees of freedom\n## Residual deviance: 10702  on 8118  degrees of freedom\n## AIC: 10714\n## \n## Number of Fisher Scoring iterations: 13\n\nNow report and interpret the p-values for the coefficients corresponding to cap_shapeknobbed, cap_shapeconical, and cap_shapesunken:\n\ncap_shapeknobbed: Our null hypothesis is that the odds ratio between flat-capped and knob-capped mushrooms is 1 (i.e., the odds of a knob-capped mushroom being poisonous are equal to the odds of a flat-capped mushroom being poisonous). If we assume the null hypothesis is true, then the probability of seeing a test statistic as or more extreme than |11.60| is 3.91e-31. Because the p-value is far below our significance level of 0.05, we take this as strong evidence that knob-capped mushrooms are much more likely to be poisonous than flat-capped mushrooms.\n\n\ncap_shapeconical: Our null hypothesis is that the odds ratio between flat-capped and cone-capped mushrooms is 1 (i.e., the odds of a cone-capped mushroom being poisonous are equal to the odds of a flat-capped mushroom being poisonous). If we assume the null hypothesis is true, then the probability of seeing a test statistic as or more extreme than |0.03| is 0.974 (i.e., we are very likely to see a test statistic more extreme than |-0.03| if in fact there were no difference between flat-capped and cone-cap mushrooms in odds of being poisonous). Because the p-value is far above our significance level of 0.05, we do not have evidence that the odds of a cone-capped mushroom being poisonous differ from the odds of a flat-capped mushroom being poisonous.\n\n\ncap_shapesunken: Our null hypothesis is that the odds ratio between flat-capped and sunken-cap mushrooms is 1 (i.e., the odds of a sunken-cap mushroom being poisonous are equal to the odds of a flat-capped mushroom being poisonous). If we assume the null hypothesis is true, then the probability of seeing a test statistic as or more extreme than |-0.09| is 0.926 (i.e., we are very likely to see a test statistic more extreme than |-0.09| if in fact there were no difference between flat-capped and sunken-cap mushrooms in odds of being poisonous). Because the p-value is far above our significance level of 0.05, we do not have evidence that the odds of a sunken-capped mushroom being poisonous differ from the odds of a flat-capped mushroom being poisonous.\n\n\n\nPart i\nBased on the model summary output in part h above, if you were given a plate of mushrooms with different cap shapes and had to pick one to eat, which one would you choose? Which cap shape would you absolutely avoid at all costs? Are your decisions guided by the coefficient estimates, the p-values, or both?\n\nAnswers may vary–if only considering coefficient estimates, then cone-shaped caps are most likely to be poisonous and sunken-shaped caps are most likely to be edible. But if we only look at p-values, then knob-shaped caps have the strongest evidence that they are more likely to be poisonous, and bell-shaped caps have the strongest evidence that they are more likely to be edible.\n\n\n\nPart j\nLet’s look at the data a slightly different way, using a 6x2 table of counts:\n\nmushrooms %&gt;% \n  mutate(cap_shape=as.factor(cap_shape),\n         poisonous=as.factor(poisonous)) %&gt;%\n  dplyr::count(cap_shape, poisonous, .drop=FALSE) %&gt;% \n  pivot_wider(names_from=poisonous, values_from=n, names_prefix=\"Poisonous = \")\n## # A tibble: 6 × 3\n##   cap_shape `Poisonous = FALSE` `Poisonous = TRUE`\n##   &lt;fct&gt;                   &lt;int&gt;              &lt;int&gt;\n## 1 flat                     1596               1556\n## 2 bell                      404                 48\n## 3 conical                     0                  4\n## 4 convex                   1948               1708\n## 5 knobbed                   228                600\n## 6 sunken                     32                  0\n\nNow, if you were given a plate of mushrooms with different cap shapes and had to pick one shape to eat and one to absolutely avoid, would you choose the same shapes? Why or why not?\n\nPersonally, I’d stick with the sunken-shaped caps for eating. Even though our model suggests there’s no evidence to believe they are less likely to be poisonous, 0 out of 32 in the sample are poisonous, which seems like the least risky choice. However, I’d tend to avoid the knob-capped mushrooms more than the cone-capped mushrooms—even though the latter are all poisonous in the sample, there were only 4 observations, so it’s possible that due to sampling variation, the odds of being poisonous for cone-capped mushrooms is lower than that of knob-capped mushrooms (where we have many more observations).",
    "crumbs": [
      "Hypothesis Testing- Details"
    ]
  },
  {
    "objectID": "activities/23-hypothesis-testing-details.html#exercise-2-1",
    "href": "activities/23-hypothesis-testing-details.html#exercise-2-1",
    "title": "Hypothesis Testing- Details",
    "section": "Exercise 2",
    "text": "Exercise 2\nFor this exercise, let’s return to the fish dataset from the previous activity (Activity 22).\n\nfish &lt;- read_csv(\"https://Mac-STAT.github.io/data/Mercury.csv\")\n\nhead(fish)\n## # A tibble: 6 × 5\n##   River  Station Length Weight Concen\n##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1 Lumber       0   47     1616   1.6 \n## 2 Lumber       0   48.7   1862   1.5 \n## 3 Lumber       0   55.7   2855   1.7 \n## 4 Lumber       0   45.2   1199   0.73\n## 5 Lumber       0   44.7   1320   0.56\n## 6 Lumber       0   43.8   1225   0.51\n\nResearch question: We believe the length of a fish (measured in centimeters) is causally associated with its mercury concentration (measured in parts per million [ppm]). We suspect that the river a fish is sampled from may be a confounder, since differences in the river environment may causally influence both the average length of fish (e.g. due to differences in water temperature or food availability) as well as mercury concentration (e.g. due to differences between the two rivers in mercury pollution levels).\n\nPart a\nFit a linear regression model that can be used to answer our research question.\n\nmod_fish1 &lt;- lm(Concen ~ Length + River, data=fish)\nsummary(mod_fish1)\n## \n## Call:\n## lm(formula = Concen ~ Length + River, data = fish)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.19298 -0.36849 -0.07677  0.30905  1.84773 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  -1.194229   0.216287  -5.521 1.26e-07 ***\n## Length        0.057657   0.005213  11.061  &lt; 2e-16 ***\n## RiverWacamaw  0.142027   0.089496   1.587    0.114    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5779 on 168 degrees of freedom\n## Multiple R-squared:  0.431,  Adjusted R-squared:  0.4243 \n## F-statistic: 63.63 on 2 and 168 DF,  p-value: &lt; 2.2e-16\n\n\n\nPart b\nInterpret the coefficient estimate, test statistic, and p-value for the RiverWacamaw coefficient. Assume we have specified a significance level of 0.05.\n\ncoefficient: Holding fish length constant, we estimate the average mercury concentration among fish in the Wacamaw River to be 0.14ppm higher than fish in the Lumber River.\n\n\nTest statistic: The estimate we observe (0.14) is 1.587 standard errors higher than the null value of a 0ppm difference in mercury concentration.\n\n\np-value: Assuming the null hypothesis is true and there is no actual difference in mercury concentration among the two fish populations (adjusting for fish length), the probability of observing a test statistic as or more extreme than |1.587| is 0.114. Because 0.114 &gt; 0.05, we do not have sufficient evidence to reject the null hypothesis, and conclude that the average mercury concentration does not differ between the two rivers.\n\n\n\nPart c\nSuppose we now want to determine if the causal effect of fish length on mercury concentration differs according to the river a fish was sampled from.\nFirst, modify the code chunk below to visualize the 3-way relationship between the Concen, Length, and River variables.\n\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen, colour = River)) + \n  geom_point()+\n  geom_smooth(method=\"lm\", se=F)\n\n\n\n\n\n\n\n\nNext, fit an appropriate linear regression model with an interaction term to investigate this question.\n\nmod_fish2 &lt;- lm(Concen ~ Length * River, data=fish)\nsummary(mod_fish2)\n## \n## Call:\n## lm(formula = Concen ~ Length * River, data = fish)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.27784 -0.35402 -0.08314  0.30650  1.94304 \n## \n## Coefficients:\n##                      Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)         -0.623875   0.325576  -1.916   0.0570 .  \n## Length               0.043185   0.008085   5.341 2.99e-07 ***\n## RiverWacamaw        -0.826291   0.426529  -1.937   0.0544 .  \n## Length:RiverWacamaw  0.024326   0.010483   2.321   0.0215 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5705 on 167 degrees of freedom\n## Multiple R-squared:  0.4488, Adjusted R-squared:  0.4389 \n## F-statistic: 45.33 on 3 and 167 DF,  p-value: &lt; 2.2e-16\n\n\n\nPart d\nInterpret the coefficient estimate, test statistic, and p-value for the RiverWacamaw:Length interaction term in this revised model (mod_fish2). Assume we’ve set a significance level of 0.05.\n\ncoefficient: First, we interpret the Length coefficient–that is, among fish in the Lumber river, we expect that a 1cm increase in length is associated with a 0.043ppm increase in mercury concentration. The interaction coefficient tells us the expected change in that relationship when considering fish in the Wacamaw River instead: we expect an additional 0.024ppm increase in mercury concentration associated with a 1cm increase in length (i.e., in the Wacamaw River, we expect mercury concentration to increase by 0.067ppm per 1cm increase in fish length).\n\n\nTest statistic: The estimate we observe (0.024326) is 2.321 standard errors higher than the null value of zero.\n\n\np-value: Assuming the null hypothesis is true and there is no difference in the relationship between fish length and mercury between the 2 rivers, the probability of observing a test statistic as or more extreme than |2.321| is 0.02. Because 0.02 &lt; 0.05, we take this as evidence to reject the null hypothesis, and conclude that the effect of fish length on mercury concentration does differ slightly between the two rivers.\n\n\n\nPart e\nInterpret the coefficient estimate, test statistic, and p-value for the RiverWacamaw coefficient in this revised model (mod_fish2). (again, you can assume we’ve set a significance level of 0.05).\n\ncoefficient: Visully, the RiverWacamaw coefficient represents the difference in the y-intercepts for the best fit lines for the Lumber and Wacamaw Rivers in the part d plot. Interpretation: Among fish that are 0cm long, average fish mercury concentrations are 0.826ppm lower in the Wacamaw River than in the Lumber River.\n\n\nTest statistic: The estimate we observe (-0.826291) is 1.937 standard errors lower than the null value of 0.\n\n\np-value: Assuming the null hypothesis is true, the probability of observing a test statistic as or more extreme than |-1.937| is 0.0544. Because 0.0544 &gt; 0.05, we do not have evidence to reject the null.\n\n\n\nPart f (CHALLENGE)\nSuppose another researcher runs the same model we fit in part c above (mod_fish2), but they claim that a more appropriate alternative hypothesis should be Beta_1 &lt; 0, (and not Beta_1 ≠ 0, as is assumed by default when running a regression model). Because of this, they reported a smaller p-value for the coefficient, and claim that the Wacamaw River has a lower baseline mercury concentration (i.e., when Length = 0cm).\nWhat is the p-value they would have reported for the RiverWacamaw coefficient in mod_fish2?\n\n0.0544/2 = 0.0272 (we divide the “two-tailed” p-value in half to obtain the p-value for a “one-tailed” test)\n\nWhat is a potential ethical problem with the other researcher’s claim that the alternative hypothesis should be Beta_1 &lt; 0?\n\nIt is possible that the researchers had a particular reason or incentive to publish evidence in support of their hypothesis (some potential reasons are that scientific journals are generally less interested in publishing results, a financial conflict of interests, or favoring a pet hypothesis). They could have first looked at the results of a “two-tailed” statistical test and since the p-value is very close to the traditional significance threshold of 0.05, come up with a post-hoc rationalization to perform a hypothesis test resulting in a “statistically significant” p-value. This unethical practice is known in the field as “p-hacking.”\n\n\n\nPart g (CHALLENGE)\nYou point out to the other researcher that the intercept and RiverWacamaw coefficients are both negative, so whatever difference in mercury concentration between the two rivers your model predicts “at baseline” is not useful or meaningful–you cannot have a fish that is 0cm long, nor a mercury concentration &lt;0ppm.\nYou propose that a more appropriate model should transform the Length variable in some way to make the intercept more interpretable. Create a new variable named Length_adj with this transformation and use it to re-fit the model:\n\nfish &lt;- fish %&gt;%\n  mutate(Length_adj=Length-min(Length))\n\nmod_fish3 &lt;- lm(Concen ~ Length_adj*River, data=fish)\nsummary(mod_fish3)\n## \n## Call:\n## lm(formula = Concen ~ Length_adj * River, data = fish)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.27784 -0.35402 -0.08314  0.30650  1.94304 \n## \n## Coefficients:\n##                          Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)              0.464384   0.132896   3.494 0.000609 ***\n## Length_adj               0.043185   0.008085   5.341 2.99e-07 ***\n## RiverWacamaw            -0.213287   0.176778  -1.207 0.229321    \n## Length_adj:RiverWacamaw  0.024326   0.010483   2.321 0.021520 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.5705 on 167 degrees of freedom\n## Multiple R-squared:  0.4488, Adjusted R-squared:  0.4389 \n## F-statistic: 45.33 on 3 and 167 DF,  p-value: &lt; 2.2e-16\n\nCompare the output of this model to that of mod_fish2. What happened to the estimate, standard error, test statistic, and p-value for the RiverWacamaw coefficient? How does this affect your conclusion? How about the other researcher’s conclusion?\n\nThe RiverWacamaw coefficient increased (and became closer to 0). The standard error decreased, but the test statistic decreased in magnitude and the p-value increased.\n\n\nWhat happened with this transformation is that the vertical axis got shifted so that the new “zero” was at 25.2cm (the minimum fish length in the data). At this point there is a smaller difference between the Lumber and Wacamaw River lines. However, as we saw above, there does seem to be a true modest difference in the slopes of these lines so there are larger differences between the 2 rivers at larger fish lengths.",
    "crumbs": [
      "Hypothesis Testing- Details"
    ]
  },
  {
    "objectID": "activities/21-confidence-intervals.html",
    "href": "activities/21-confidence-intervals.html",
    "title": "Confidence Interval",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Confidence Interval"
    ]
  },
  {
    "objectID": "activities/21-confidence-intervals.html#learning-goals",
    "href": "activities/21-confidence-intervals.html#learning-goals",
    "title": "Confidence Interval",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nConstruct (approximate) confidence intervals by hand using the 68-95-99.7 rule\nConstruct exact confidence intervals in R\nInterpret confidence intervals in context by referring to the coefficient of interest\nUse confidence intervals to make statements about whether there appear to be true population relationships, changes, and differences",
    "crumbs": [
      "Confidence Interval"
    ]
  },
  {
    "objectID": "activities/21-confidence-intervals.html#readings-and-videos",
    "href": "activities/21-confidence-intervals.html#readings-and-videos",
    "title": "Confidence Interval",
    "section": "Readings and videos",
    "text": "Readings and videos\nPlease complete the following reading before class.\n\nReading: Section 7 Introduction, Section 7.1, Section 7.2 (stop when you get to 7.2.4.3 Confidence Intervals for Prediction) in the STAT 155 Notes\n\nOptionally you can use the following videos as a companion to the reading (not in place of the reading):\n\nVideo 1: Introduction to Confidence Intervals\nVideo 2: Confidence Intervals: Construction and Interpretation",
    "crumbs": [
      "Confidence Interval"
    ]
  },
  {
    "objectID": "activities/21-confidence-intervals.html#exercise-1",
    "href": "activities/21-confidence-intervals.html#exercise-1",
    "title": "Confidence Interval",
    "section": "Exercise 1",
    "text": "Exercise 1\nResearch question: Is the relationship between wind speed (windspeed) (in miles per hour) and number of riders (riders_total) different across weekdays and weekends?\n\nPart a\nConstruct and interpret a visualization that would address this question.\n\nggplot(bikes, aes(x = windspeed, y = riders_total, col = weekend)) + \n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    theme_classic() +\n    labs(x = \"Windspeed (miles per hour)\", y = \"Total daily riders\")\n\n\n\n\n\n\n\n\n\nOverall, windier days seem to have less riders (negative slope). The slope for weekends seems slightly steeper than for weekdays, but overall weekdays and weekends have similar slopes.\n\n\n\nPart b\nFit a regression model that would address our research question. (Should it be a linear or a logistic regression model?) Interpret only the coefficient of interest.\n\nmod_bikes &lt;- lm(riders_total ~ windspeed*weekend, data = bikes)\nsummary(mod_bikes)\n## \n## Call:\n## lm(formula = riders_total ~ windspeed * weekend, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4523.2 -1317.9   -46.9  1443.3  4715.7 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)            5560.31     219.07  25.382  &lt; 2e-16 ***\n## windspeed               -79.47      15.97  -4.976 8.09e-07 ***\n## weekendTRUE             200.56     409.72   0.489    0.625    \n## windspeed:weekendTRUE   -26.82      29.56  -0.907    0.365    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1885 on 727 degrees of freedom\n## Multiple R-squared:  0.05721,    Adjusted R-squared:  0.05332 \n## F-statistic:  14.7 on 3 and 727 DF,  p-value: 2.638e-09\n\n\nWe need to fit a linear regression model (because outcome is quantitative) with an interaction term to answer this question. The interaction coefficient is of interest.\n\n\nInterpretation of interaction coefficient: The average decrease in ridership associated with a 1 mph increase in wind speed is 26.82 rides/mph lower on weekends than for weekdays. Put another way, on weekdays, a 1 mph increase in wind speed is associated with a decrease of 79.47 riders. On weekends, that decrease is 106.29 riders.\n\n\n\nPart c\n\nConstruct an approximate 95% confidence interval (CI) for the coefficient of interest by hand using the 68-95-99.7 rule.\nCompare your confidence interval to the one given by confint() which gives an exact confidence interval. (The columns give the lower and upper ends of the CI for each coefficient.)\nInterpret the exact confidence interval in context.\nIs zero in the interval? Do we have evidence for a real difference in the windspeed-riders relationship across weekends and weekdays?\n\n\n# By hand using 68-95-99.7 rule\n-26.82 - 2*29.56 \n## [1] -85.94\n-26.82 + 2*29.56 \n## [1] 32.3\n\n# By hand using 1.96, which is closer to the exact normal distribution quantile to use\n\n\n# Using confint()\nconfint(mod_bikes, level = 0.95)\n##                            2.5 %     97.5 %\n## (Intercept)           5130.23243 5990.38552\n## windspeed             -110.81588  -48.11605\n## weekendTRUE           -603.82472 1004.93649\n## windspeed:weekendTRUE  -84.84192   31.21156\n\n\nOur manual calculation is pretty close to the CI given by confint().\n\n\nInterpretation in context: Preferred interpretation: It is plausible that the true population difference in the relationship between riders and wind speed comparing weekends to weekdays ranges from an average decrease of 84 riders/mph to an average increase of 31.21 riders/mph.\n\n\nNot as preferred interpretation (but you’ll see this wording across disciplines): We are 95% confident that the difference in riders vs. wind speed slopes between weekends and weekdays is between -84 riders/mph to +31.21 riders/mph. (The instructors don’t like this interpretation as much because saying “95% confident” is rather vague. We are confident about the interval construction process across random samples, and this interpretation doesn’t make that clear.)\n\n\nZero is in the CI. This means that the difference in slopes could plausibly be zero. Therefore we do not have evidence for a real difference in the windspeed-riders relationship across weekends and weekdays.\n\n\nClass Notes\n\n\n\nPart d\nLet’s see if these results agree when looking at adjusted R-squared.\nFit another regression model that does not have the coefficient of interest from your Part b model. Compare the adjusted R-squared values between this model and the Part b model. Explain your findings.\n\nmod_bikes_noint &lt;- lm(riders_total ~ windspeed +\n                        weekend, data = bikes)\nsummary(mod_bikes_noint)\n## \n## Call:\n## lm(formula = riders_total ~ windspeed + weekend, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4563.0 -1323.1   -67.4  1445.2  4645.8 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  5659.76     189.64  29.845  &lt; 2e-16 ***\n## windspeed     -87.29      13.44  -6.497 1.52e-10 ***\n## weekendTRUE  -143.87     154.07  -0.934    0.351    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1885 on 728 degrees of freedom\n## Multiple R-squared:  0.05614,    Adjusted R-squared:  0.05355 \n## F-statistic: 21.65 on 2 and 728 DF,  p-value: 7.346e-10\nsummary(mod_bikes)\n## \n## Call:\n## lm(formula = riders_total ~ windspeed * weekend, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4523.2 -1317.9   -46.9  1443.3  4715.7 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)            5560.31     219.07  25.382  &lt; 2e-16 ***\n## windspeed               -79.47      15.97  -4.976 8.09e-07 ***\n## weekendTRUE             200.56     409.72   0.489    0.625    \n## windspeed:weekendTRUE   -26.82      29.56  -0.907    0.365    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1885 on 727 degrees of freedom\n## Multiple R-squared:  0.05721,    Adjusted R-squared:  0.05332 \n## F-statistic:  14.7 on 3 and 727 DF,  p-value: 2.638e-09\n\n\nThe adjusted R-squared for the interaction model was 0.05332, compared to 0.05355 for the model without the interaction. Adding the interaction term actually decreased the adjusted R-squared, suggesting that it didn’t really improve the model. This agrees with what our CI interpretation: zero was a plausible value for the difference in slopes. If zero is a plausible value for the difference in slopes, allowing the slopes to be different in our model might not be necessary.",
    "crumbs": [
      "Confidence Interval"
    ]
  },
  {
    "objectID": "activities/21-confidence-intervals.html#exercise-2",
    "href": "activities/21-confidence-intervals.html#exercise-2",
    "title": "Confidence Interval",
    "section": "Exercise 2",
    "text": "Exercise 2\nResearch question: How different is holiday ridership from non-holidays, after accounting for confounding factors?\n\nPart a\nWe believe that weather category (weather_cat), temperature (temp_actual), and wind speed (windspeed) confound the relationship of interest.\n\nConstruct visualizations that allow you how each potential confounder relates to riders_total and to holiday.\n\n\n# weather category\nggplot(bikes, aes(x = weather_cat, y = riders_total)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n# temperature\nggplot(bikes, aes(x = temp_actual, y = riders_total)) +\n    geom_point() +\n    geom_smooth()\n\n\n\n\n\n\n\n\n# windspeed \nggplot(bikes, aes(x = windspeed, y = riders_total)) +\n    geom_point() +\n    geom_smooth()\n\n\n\n\n\n\n\n\n# holiday & weather category\nggplot(bikes, aes(x = holiday, fill = weather_cat)) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n# holiday & temperature\nggplot(bikes, aes(x = holiday, y = temp_actual)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n# holiday & windspeed \nggplot(bikes, aes(x = holiday, y = windspeed)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nThe visualizations support that weather_cat, temp_actual, and windspeed are causes of ridership, but only weather_cat and temp_actual seem to have noticeable differences between holidays and non-holidays.\n\n\n\nPart b\nBased on your Part a explorations, fit an appropriate regression model to answer our research question. Interpret only the coefficient of interest.\nA note about scientific notation in R: Sometimes you may see numbers with the letter e in the middle. This is R’s way of expressing scientific notation. Whenever you see e, replace that with 10 to the power of .... So:\n\n1.234e+02 is 1.234 x 10^2 = 123.4\n1.234e-02 is 1.234 x 10^(-2) = 0.01234\n\n\nmod_bikes_smaller &lt;- lm(riders_total ~ holiday + weather_cat + temp_actual, data = bikes)\nmod_bikes_larger &lt;- lm(riders_total ~ holiday + weather_cat + temp_actual + windspeed, data = bikes)\n\nsummary(mod_bikes_smaller)\n## \n## Call:\n## lm(formula = riders_total ~ holiday + weather_cat + temp_actual, \n##     data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4266.2 -1134.0   -86.4  1010.7  3505.5 \n## \n## Coefficients:\n##                    Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)       -1088.971    283.764  -3.838 0.000135 ***\n## holidayyes         -702.377    316.924  -2.216 0.026985 *  \n## weather_catcateg2  -585.725    113.314  -5.169 3.04e-07 ***\n## weather_catcateg3 -2601.400    319.947  -8.131 1.84e-15 ***\n## temp_actual          85.855      3.948  21.749  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1430 on 726 degrees of freedom\n## Multiple R-squared:  0.4584, Adjusted R-squared:  0.4554 \n## F-statistic: 153.6 on 4 and 726 DF,  p-value: &lt; 2.2e-16\nsummary(mod_bikes_larger)\n## \n## Call:\n## lm(formula = riders_total ~ holiday + weather_cat + temp_actual + \n##     windspeed, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3866.9 -1084.7  -124.9  1034.5  3585.4 \n## \n## Coefficients:\n##                    Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)        -310.590    329.483  -0.943   0.3462    \n## holidayyes         -696.442    312.833  -2.226   0.0263 *  \n## weather_catcateg2  -605.826    111.940  -5.412 8.47e-08 ***\n## weather_catcateg3 -2453.431    317.532  -7.727 3.69e-14 ***\n## temp_actual          83.101      3.945  21.067  &lt; 2e-16 ***\n## windspeed           -46.013     10.256  -4.486 8.43e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1411 on 725 degrees of freedom\n## Multiple R-squared:  0.4731, Adjusted R-squared:  0.4694 \n## F-statistic: 130.2 on 5 and 725 DF,  p-value: &lt; 2.2e-16\n\nClear confounders from Part a include weather_cat and temp_actual. windspeed might be a precision variable because it don’t seem to be very different between holidays and non-holidays. We try models with just the confounders and with confounders+precision variable.\nThe coefficient on holiday is of interest.\nmod_bikes_smaller interpretation: Among days that have the same weather category and temperature, holidays have 702 fewer riders on average than non-holidays.\nmod_bikes_larger interpretation: Among days that have the same weather category, temperature, and wind speed, holidays have 696 fewer riders on average than non-holidays.\n\n\nPart c\n\nUse confint() to construct a 95% confidence interval for the coefficient of interest.\nInterpret this confidence interval in context.\nIs zero in the interval? Do we have evidence for a real holiday effect on ridership?\n\n\nconfint(mod_bikes_smaller, level = 0.95)\n##                         2.5 %      97.5 %\n## (Intercept)       -1646.06797  -531.87497\n## holidayyes        -1324.57498   -80.17969\n## weather_catcateg2  -808.18647  -363.26341\n## weather_catcateg3 -3229.53220 -1973.26782\n## temp_actual          78.10468    93.60462\nconfint(mod_bikes_larger, level = 0.95)\n##                         2.5 %      97.5 %\n## (Intercept)        -957.44394   336.26452\n## holidayyes        -1310.60962   -82.27534\n## weather_catcateg2  -825.59030  -386.06140\n## weather_catcateg3 -3076.82251 -1830.03909\n## temp_actual          75.35708    90.84545\n## windspeed           -66.14848   -25.87694",
    "crumbs": [
      "Confidence Interval"
    ]
  },
  {
    "objectID": "activities/21-confidence-intervals.html#exercise-3",
    "href": "activities/21-confidence-intervals.html#exercise-3",
    "title": "Confidence Interval",
    "section": "Exercise 3",
    "text": "Exercise 3\nThe Western Collaborative Group Study (WCGS) was designed in order to investigate a possible link between Type A behavior and coronary heart disease (CHD), and to develop a framework to select patients for intervention in order to decrease risk of CHD. The study contained 3154 cis men between the ages of 39 and 59 in California who had no history of CHD. They were enrolled in the study in 1960 and 1961, underwent a medical examination and covered their medical history, and they were re-examined annually for interim cardiovascular history.\nA full codebook is available here. We will focus on the following variables:\n\nchd: Presence (1) or absence (0) of CHD over followup (outcome)\ntabp: Presence (1) or absence (0) of Type A behavior (main variable of interest)\nage: Age at time of enrollment in the study (years)\nsbp: Systolic blood pressure\ndbp: Diastolic blood pressure\nchol: Cholesterol (mg/dL)\nncigs: Number of cigarettes smoked per day\narcus: Presence (1) or absence (0) of arcus senilis (a colored ring around the cornea made up of lipids like cholesterol and believed to be a risk factor for CHD)\nbmi: BMI = weight * 703 / height^2\n\nResearch question: Is there a causal effect of Type A/B personality on developing coronary heart disease?\n\nwcgs &lt;- read_csv(\"https://mac-stat.github.io/data/wcgs.csv\")\n\n\nPart a\nWe believe that the following variables are confounders of the relationship between Type A/B personality tabp and coronary heart disease (CHD): age + sbp + dbp + chol + ncigs + arcus + bmi.\nFit a regression model that would address our research question. (Should it be a linear or a logistic regression model?) Interpret only the coefficient of interest.\n\ntypea_mod &lt;- glm(chd ~ tabp + age + sbp + dbp + chol + ncigs + arcus + bmi, data = wcgs, family = \"binomial\")\nsummary(typea_mod)\n## \n## Call:\n## glm(formula = chd ~ tabp + age + sbp + dbp + chol + ncigs + arcus + \n##     bmi, family = \"binomial\", data = wcgs)\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -1.225e+01  9.898e-01 -12.378  &lt; 2e-16 ***\n## tabp         6.670e-01  1.458e-01   4.576 4.74e-06 ***\n## age          5.897e-02  1.230e-02   4.794 1.64e-06 ***\n## sbp          1.824e-02  6.408e-03   2.846  0.00443 ** \n## dbp         -5.797e-04  1.086e-02  -0.053  0.95743    \n## chol         1.045e-02  1.519e-03   6.879 6.04e-12 ***\n## ncigs        2.131e-02  4.287e-03   4.971 6.67e-07 ***\n## arcus        2.219e-01  1.436e-01   1.545  0.12238    \n## bmi          5.841e-02  2.714e-02   2.152  0.03141 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1769.2  on 3139  degrees of freedom\n## Residual deviance: 1572.6  on 3131  degrees of freedom\n##   (14 observations deleted due to missingness)\n## AIC: 1590.6\n## \n## Number of Fisher Scoring iterations: 6\n\n\nWe need to fit a logistic regression model because the chd outcome is binary. We include tabp as the main predictor of interest and all of the other confounding variables. We need to exponentiate the coefficient so that we’re interpreting on the odds scale rather than the log odds scale.\n\n\nInterpretation of exp(tabp): Among men of the same age, systolic and diastolic blood pressure, cholesterol levels, smoking habits, history of arcus sinilis, and BMI, those with Type A personality have 1.95 times the odds of CHD than those without Type A personality.\n\n\n\nPart b\n\nConstruct a 95% confidence interval for the odds ratio of interest using the following code.\nInterpret the confidence interval in context.\nIs 1 contained in the interval? Why is 1 a relevant value to look for here?\n\n\nconfint(typea_mod, level = 0.95) %&gt;% exp()\n##                    2.5 %       97.5 %\n## (Intercept) 6.706495e-07 3.257156e-05\n## tabp        1.469222e+00 2.603660e+00\n## age         1.035479e+00 1.086677e+00\n## sbp         1.005561e+00 1.031169e+00\n## dbp         9.783446e-01 1.020911e+00\n## chol        1.007521e+00 1.013538e+00\n## ncigs       1.012938e+00 1.030126e+00\n## arcus       9.399791e-01 1.651412e+00\n## bmi         1.004943e+00 1.117811e+00\n\n\nPreferred interpretation: Among men of the same age, systolic and diastolic blood pressure, cholesterol levels, smoking habits, history of arcus sinilis, and BMI, it is plausible that those with Type A personality have 1.47 to 2.60 times the odds of CHD than those without Type A personality.\n\n\n1 is not in the CI. 1 is a relevant value to consider for ratios because if the odds ratio is 1, then the (adjusted) odds of CHD is the same in those with Type A and Type B personality. There seems to be a positive relationship between Type A personality and CHD in this study.\n\n\n\nPart c\n(On your own time)\nThe data context in this exercise has a fraught history with the smoking industry. Read this article for some context about how the Type A personality came to be defined and studied. (One big takeaway: The smoking industry had a large incentive to find something to blame health problems on other than smoking!)",
    "crumbs": [
      "Confidence Interval"
    ]
  },
  {
    "objectID": "activities/21-confidence-intervals.html#exercise-4",
    "href": "activities/21-confidence-intervals.html#exercise-4",
    "title": "Confidence Interval",
    "section": "Exercise 4",
    "text": "Exercise 4\nFor each of the following MISINTERPRETATIONS of a 95% confidence interval (a,b), explain why the statement is a misinterpretation.\n\nMisinterpretation 1: “There is a 95% probability that the population parameter is within (a,b).”\n\nResponse: The population parameter is not random. It is either in the interval or not, so the probability is 1 or 0. The 95% means that 95% of random samples (that are representative of the population of interest) are expected to contain the true population parameter—“95% confidence” is describing confidence in the interval construction process.\n\nMisinterpretation 2: “There is a 5% probability that the population parameter is not within (a,b).”\n\nResponse: This is incorrect for the same reason as the first misinterpretation.\n\nMisinterpretation 3: “There is a 95% chance that the sample estimate in (a,b).”\n\nResponse: The sample estimate is always in the interval by construction.",
    "crumbs": [
      "Confidence Interval"
    ]
  },
  {
    "objectID": "activities/21-confidence-intervals.html#reflection",
    "href": "activities/21-confidence-intervals.html#reflection",
    "title": "Confidence Interval",
    "section": "Reflection",
    "text": "Reflection\nHow are you feeling about your ability to translate research questions into appropriate statistical investigations and addressing those questions using output from those investigations? What has gotten easier? What remains challenging?\n\nResponse:",
    "crumbs": [
      "Confidence Interval"
    ]
  },
  {
    "objectID": "activities/21-confidence-intervals.html#exercise-1-1",
    "href": "activities/21-confidence-intervals.html#exercise-1-1",
    "title": "Confidence Interval",
    "section": "Exercise 1",
    "text": "Exercise 1\nResearch question: Is the relationship between wind speed (windspeed) (in miles per hour) and number of riders (riders_total) different across weekdays and weekends?\n\nPart a\nConstruct and interpret a visualization that would address this question.\n\nResponse: Overall, windier days seem to have less riders (negative slope). The slope for weekends seems slightly steeper than for weekdays, but overall weekdays and weekends have similar slopes.\n\n\nggplot(bikes, aes(x = windspeed, y = riders_total, col = weekend)) + \n    geom_point(alpha = 0.2) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    theme_classic() +\n    labs(x = \"Windspeed (miles per hour)\", y = \"Total daily riders\")\n\n\n\n\n\n\n\n\n\n\nPart b\nFit a regression model that would address our research question. (Should it be a linear or a logistic regression model?) Interpret only the coefficient of interest.\n\nResponse: We need to fit a linear regression model (because outcome is quantitative) with an interaction term to answer this question. The interaction coefficient is of interest.\nInterpretation of interaction coefficient: The average decrease in ridership associated with a 1 mph increase in wind speed is 26.82 rides/mph lower on weekends than for weekdays. Put another way, on weekdays, a 1 mph increase in wind speed is associated with a decrease of 79.47 riders. On weekends, that decrease is 106.29 riders.\n\n\nmod_bikes &lt;- lm(riders_total ~ windspeed*weekend, data = bikes)\nsummary(mod_bikes)\n## \n## Call:\n## lm(formula = riders_total ~ windspeed * weekend, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4523.2 -1317.9   -46.9  1443.3  4715.7 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)            5560.31     219.07  25.382  &lt; 2e-16 ***\n## windspeed               -79.47      15.97  -4.976 8.09e-07 ***\n## weekendTRUE             200.56     409.72   0.489    0.625    \n## windspeed:weekendTRUE   -26.82      29.56  -0.907    0.365    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1885 on 727 degrees of freedom\n## Multiple R-squared:  0.05721,    Adjusted R-squared:  0.05332 \n## F-statistic:  14.7 on 3 and 727 DF,  p-value: 2.638e-09\n\n\n\nPart c\n\nConstruct an approximate 95% confidence interval (CI) for the coefficient of interest by hand using the 68-95-99.7 rule.\nCompare your confidence interval to the one given by confint() which gives an exact confidence interval. (The columns give the lower and upper ends of the CI for each coefficient.)\nInterpret the exact confidence interval in context.\nIs zero in the interval? Do we have evidence for a real difference in the windspeed-riders relationship across weekends and weekdays?\n\n\nResponse:\n\nOur manual calculation is pretty close to the CI given by confint().\nInterpretation in context:\n\nPreferred interpretation: It is plausible that the true population difference in the relationship between riders and wind speed comparing weekends to weekdays ranges from an average decrease of 84 riders/mph to an average increase of 31.21 riders/mph.\nNot as preferred interpretation (but you’ll see this wording across disciplines): We are 95% confident that the difference in riders vs. wind speed slopes between weekends and weekdays is between -84 riders/mph to +31.21 riders/mph. (The instructors don’t like this interpretation as much because saying “95% confident” is rather vague. We are confident about the interval construction process across random samples, and this interpretation doesn’t make that clear.)\n\nZero is in the CI. This means that the difference in slopes could plausibly be zero. Therefore we do not have evidence for a real difference in the windspeed-riders relationship across weekends and weekdays.\n\n\n\n# By hand\n-26.82 - 2*29.56\n## [1] -85.94\n-26.82 + 2*29.56\n## [1] 32.3\n\n# By hand using 1.96, which is closer to the exact normal distribution quantile to use\n-26.82 - 1.96*29.56\n## [1] -84.7576\n-26.82 + 1.96*29.56\n## [1] 31.1176\n\n# Using confint()\nconfint(mod_bikes, level = 0.95)\n##                            2.5 %     97.5 %\n## (Intercept)           5130.23243 5990.38552\n## windspeed             -110.81588  -48.11605\n## weekendTRUE           -603.82472 1004.93649\n## windspeed:weekendTRUE  -84.84192   31.21156\n\n\n\nPart d\nLet’s see if these results agree when looking at adjusted R-squared.\nFit another regression model that does not have the coefficient of interest from your Part b model. Compare the adjusted R-squared values between this model and the Part b model. Explain your findings.\n\nResponse:\n\nThe adjusted R-squared for the interaction model was 0.05332, compared to 0.05355 for the model without the interaction.\nAdding the interaction term actually decreased the adjusted R-squared, suggesting that it didn’t really improve the model.\nThis agrees with what our CI interpretation: zero was a plausible value for the difference in slopes. If zero is a plausible value for the difference in slopes, allowing the slopes to be different in our model might not be necessary.\n\n\n\nmod_bikes_noint &lt;- lm(riders_total ~ windspeed+weekend, data = bikes)\nsummary(mod_bikes)\n## \n## Call:\n## lm(formula = riders_total ~ windspeed * weekend, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4523.2 -1317.9   -46.9  1443.3  4715.7 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)            5560.31     219.07  25.382  &lt; 2e-16 ***\n## windspeed               -79.47      15.97  -4.976 8.09e-07 ***\n## weekendTRUE             200.56     409.72   0.489    0.625    \n## windspeed:weekendTRUE   -26.82      29.56  -0.907    0.365    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1885 on 727 degrees of freedom\n## Multiple R-squared:  0.05721,    Adjusted R-squared:  0.05332 \n## F-statistic:  14.7 on 3 and 727 DF,  p-value: 2.638e-09\nsummary(mod_bikes_noint)\n## \n## Call:\n## lm(formula = riders_total ~ windspeed + weekend, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4563.0 -1323.1   -67.4  1445.2  4645.8 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  5659.76     189.64  29.845  &lt; 2e-16 ***\n## windspeed     -87.29      13.44  -6.497 1.52e-10 ***\n## weekendTRUE  -143.87     154.07  -0.934    0.351    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1885 on 728 degrees of freedom\n## Multiple R-squared:  0.05614,    Adjusted R-squared:  0.05355 \n## F-statistic: 21.65 on 2 and 728 DF,  p-value: 7.346e-10",
    "crumbs": [
      "Confidence Interval"
    ]
  },
  {
    "objectID": "activities/21-confidence-intervals.html#exercise-2-1",
    "href": "activities/21-confidence-intervals.html#exercise-2-1",
    "title": "Confidence Interval",
    "section": "Exercise 2",
    "text": "Exercise 2\nResearch question: How different is holiday ridership from non-holidays, after accounting for confounding factors?\n\nPart a\nWe believe that weather category (weather_cat), temperature (temp_actual), and wind speed (windspeed) confound the relationship of interest.\n\nConstruct visualizations that allow you how each potential confounder relates to riders_total and to holiday.\n\n\nggplot(bikes, aes(x = weather_cat, y = riders_total)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nggplot(bikes, aes(x = temp_actual, y = riders_total)) +\n    geom_point() +\n    geom_smooth()\n\n\n\n\n\n\n\n\nggplot(bikes, aes(x = windspeed, y = riders_total)) +\n    geom_point() +\n    geom_smooth()\n\n\n\n\n\n\n\n\n\nggplot(bikes, aes(x = holiday, fill = weather_cat)) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nggplot(bikes, aes(x = holiday, y = temp_actual)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nggplot(bikes, aes(x = holiday, y = windspeed)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nPart b\nBased on your Part a explorations, fit an appropriate regression model to answer our research question. Interpret only the coefficient of interest.\nA note about scientific notation in R: Sometimes you may see numbers with the letter e in the middle. This is R’s way of expressing scientific notation. Whenever you see e, replace that with 10 to the power of .... So:\n\n1.234e+02 is 1.234 x 10^2 = 123.4\n1.234e-02 is 1.234 x 10^(-2) = 0.01234\n\n\nResponse: Clear confounders from Part a include weather_cat and temp_actual. windspeed might be a precision variable because it don’t seem to be very different between holidays and non-holidays. We try models with just the confounders and with confounders+precision variable. Because temperature has a curved relationships with riders, we include a squared term.\nThe coefficient on holiday is of interest.\nmod_bikes_smaller interpretation: Among days that have the same weather category and temperature, holidays have 731 fewer riders on average than non-holidays.\nmod_bikes_larger interpretation: Among days that have the same weather category, temperature, and wind speed, holidays have 725 fewer riders on average than non-holidays.\n\n\nbikes_new &lt;- bikes %&gt;% \n    mutate(\n        temp_actual_squared = temp_actual^2\n    )\nmod_bikes_smaller &lt;- lm(riders_total ~ holiday + weather_cat + temp_actual_squared, data = bikes_new)\nmod_bikes_larger &lt;- lm(riders_total ~ holiday + weather_cat + temp_actual_squared + windspeed, data = bikes_new)\n\nsummary(mod_bikes_smaller)\n## \n## Call:\n## lm(formula = riders_total ~ holiday + weather_cat + temp_actual_squared, \n##     data = bikes_new)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4225.5 -1200.7  -111.8  1057.8  3608.9 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)          1.872e+03  1.660e+02  11.280  &lt; 2e-16 ***\n## holidayyes          -7.311e+02  3.263e+02  -2.240   0.0254 *  \n## weather_catcateg2   -5.708e+02  1.168e+02  -4.885 1.27e-06 ***\n## weather_catcateg3   -2.571e+03  3.297e+02  -7.799 2.18e-14 ***\n## temp_actual_squared  5.980e-01  2.973e-02  20.116  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1472 on 726 degrees of freedom\n## Multiple R-squared:  0.4257, Adjusted R-squared:  0.4225 \n## F-statistic: 134.5 on 4 and 726 DF,  p-value: &lt; 2.2e-16\nsummary(mod_bikes_larger)\n## \n## Call:\n## lm(formula = riders_total ~ holiday + weather_cat + temp_actual_squared + \n##     windspeed, data = bikes_new)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -4050.0 -1109.6  -120.6  1068.0  3699.8 \n## \n## Coefficients:\n##                       Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)          2.577e+03  2.281e+02  11.294  &lt; 2e-16 ***\n## holidayyes          -7.246e+02  3.222e+02  -2.249   0.0248 *  \n## weather_catcateg2   -5.924e+02  1.155e+02  -5.131 3.71e-07 ***\n## weather_catcateg3   -2.422e+03  3.272e+02  -7.403 3.70e-13 ***\n## temp_actual_squared  5.770e-01  2.973e-02  19.405  &lt; 2e-16 ***\n## windspeed           -4.692e+01  1.057e+01  -4.439 1.05e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1454 on 725 degrees of freedom\n## Multiple R-squared:  0.4409, Adjusted R-squared:  0.437 \n## F-statistic: 114.3 on 5 and 725 DF,  p-value: &lt; 2.2e-16\n\n\n\nPart c\n\nUse confint() to construct a 95% confidence interval for the coefficient of interest.\nInterpret this confidence interval in context.\nIs zero in the interval? Do we have evidence for a real holiday effect on ridership?\n\n\nResponse: We’ll focus on the CI from mod_bikes_smaller since the CI from mod_bikes_larger is pretty similar.\n\nInterpretation in context:\n\nPreferred interpretation: It is plausible that the true population difference in average holiday ridership vs. average non-holiday ridership is from 1371.8 to 90.5 fewer rides on holidays (among days of the same weather category and temperature).\nNot as preferred interpretation: We are 95% confident that the population difference in holiday vs non-holiday ridership is between -1371.8 to -90.4532521.\n\nZero is not in the CI which means that the difference between holidays and non-holidays (among days of the same weather category and temperature) is not plausibly zero. We do have evidence for a true holiday effect.\n\n\n\nconfint(mod_bikes_smaller, level = 0.95)\n##                             2.5 %        97.5 %\n## (Intercept)          1546.3798240  2198.1156306\n## holidayyes          -1371.8105006   -90.4532521\n## weather_catcateg2    -800.1540608  -341.3738083\n## weather_catcateg3   -3218.3124545 -1923.8150951\n## temp_actual_squared     0.5396379     0.6563642\nconfint(mod_bikes_larger, level = 0.95)\n##                            2.5 %        97.5 %\n## (Intercept)          2128.817226  3024.6176444\n## holidayyes          -1357.222742   -92.0407887\n## weather_catcateg2    -819.136285  -365.7464372\n## weather_catcateg3   -3064.923766 -1780.0402827\n## temp_actual_squared     0.518586     0.6353312\n## windspeed             -67.677428   -26.1684414",
    "crumbs": [
      "Confidence Interval"
    ]
  },
  {
    "objectID": "activities/21-confidence-intervals.html#exercise-3-1",
    "href": "activities/21-confidence-intervals.html#exercise-3-1",
    "title": "Confidence Interval",
    "section": "Exercise 3",
    "text": "Exercise 3\nThe Western Collaborative Group Study (WCGS) was designed in order to investigate a possible link between Type A behavior and coronary heart disease (CHD), and to develop a framework to select patients for intervention in order to decrease risk of CHD. The study contained 3154 cis men between the ages of 39 and 59 in California who had no history of CHD. They were enrolled in the study in 1960 and 1961, underwent a medical examination and covered their medical history, and they were re-examined annually for interim cardiovascular history.\nA full codebook is available here. We will focus on the following variables:\n\nchd: Presence (1) or absence (0) of CHD over followup (outcome)\ntabp: Presence (1) or absence (0) of Type A behavior (main variable of interest)\nage: Age at time of enrollment in the study (years)\nsbp: Systolic blood pressure\ndbp: Diastolic blood pressure\nchol: Cholesterol (mg/dL)\nncigs: Number of cigarettes smoked per day\narcus: Presence (1) or absence (0) of arcus senilis (a colored ring around the cornea made up of lipids like cholesterol and believed to be a risk factor for CHD)\nbmi: BMI = weight * 703 / height^2\n\nResearch question: Is there a causal effect of Type A/B personality on developing coronary heart disease?\n\nwcgs &lt;- read_csv(\"https://mac-stat.github.io/data/wcgs.csv\")\n\n\nPart a\nWe believe that the following variables are confounders of the relationship between Type A/B personality tabp and coronary heart disease (CHD): age + sbp + dbp + chol + ncigs + arcus + bmi.\nFit a regression model that would address our research question. (Should it be a linear or a logistic regression model?) Interpret only the coefficient of interest.\n\nResponse: We need to fit a logistic regression model because the chd outcome is binary. We include tabp as the main predictor of interest and all of the other confounding variables. We need to exponentiate the coefficient so that we’re interpreting on the odds scale rather than the log odds scale.\nInterpretation of exp(tabp): Among men of the same age, systolic and diastolic blood pressure, cholesterol levels, smoking habits, history of arcus sinilis, and BMI, those with Type A personality have 1.95 times the odds of CHD than those without Type A personality.\n\n\ntypea_mod &lt;- glm(chd ~ tabp + age + sbp + dbp + chol + ncigs + arcus + bmi, data = wcgs, family = \"binomial\")\nsummary(typea_mod)\n## \n## Call:\n## glm(formula = chd ~ tabp + age + sbp + dbp + chol + ncigs + arcus + \n##     bmi, family = \"binomial\", data = wcgs)\n## \n## Coefficients:\n##               Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept) -1.225e+01  9.898e-01 -12.378  &lt; 2e-16 ***\n## tabp         6.670e-01  1.458e-01   4.576 4.74e-06 ***\n## age          5.897e-02  1.230e-02   4.794 1.64e-06 ***\n## sbp          1.824e-02  6.408e-03   2.846  0.00443 ** \n## dbp         -5.797e-04  1.086e-02  -0.053  0.95743    \n## chol         1.045e-02  1.519e-03   6.879 6.04e-12 ***\n## ncigs        2.131e-02  4.287e-03   4.971 6.67e-07 ***\n## arcus        2.219e-01  1.436e-01   1.545  0.12238    \n## bmi          5.841e-02  2.714e-02   2.152  0.03141 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1769.2  on 3139  degrees of freedom\n## Residual deviance: 1572.6  on 3131  degrees of freedom\n##   (14 observations deleted due to missingness)\n## AIC: 1590.6\n## \n## Number of Fisher Scoring iterations: 6\ncoef(typea_mod)\n##   (Intercept)          tabp           age           sbp           dbp \n## -1.225217e+01  6.670497e-01  5.897199e-02  1.823715e-02 -5.797328e-04 \n##          chol         ncigs         arcus           bmi \n##  1.045016e-02  2.131124e-02  2.218750e-01  5.840654e-02\nexp(coef(typea_mod))\n##  (Intercept)         tabp          age          sbp          dbp         chol \n## 4.774751e-06 1.948480e+00 1.060746e+00 1.018404e+00 9.994204e-01 1.010505e+00 \n##        ncigs        arcus          bmi \n## 1.021540e+00 1.248415e+00 1.060146e+00\n\n\n\nPart b\n\nConstruct a 95% confidence interval for the odds ratio of interest using the following code.\nInterpret the confidence interval in context.\nIs 1 contained in the interval? Why is 1 a relevant value to look for here?\n\n\nResponse:\n\nInterpretation in context:\n\nPreferred interpretation: Among men of the same age, systolic and diastolic blood pressure, cholesterol levels, smoking habits, history of arcus sinilis, and BMI, it is plausible that those with Type A personality have 1.47 to 2.60 times the odds of CHD than those without Type A personality.\n\n1 is not in the CI. 1 is a relevant value to consider for ratios because if the odds ratio is 1, then the (adjusted) odds of CHD is the same in those with Type A and Type B personality. There seems to be a positive relationship between Type A personality and CHD in this study.\n\n\n\nconfint(typea_mod, level = 0.95) %&gt;% exp()\n##                    2.5 %       97.5 %\n## (Intercept) 6.706495e-07 3.257156e-05\n## tabp        1.469222e+00 2.603660e+00\n## age         1.035479e+00 1.086677e+00\n## sbp         1.005561e+00 1.031169e+00\n## dbp         9.783446e-01 1.020911e+00\n## chol        1.007521e+00 1.013538e+00\n## ncigs       1.012938e+00 1.030126e+00\n## arcus       9.399791e-01 1.651412e+00\n## bmi         1.004943e+00 1.117811e+00\n\n\n\nPart c\n(On your own time)\nThe data context in this exercise has a fraught history with the smoking industry. Read this article for some context about how the Type A personality came to be defined and studied. (One big takeaway: The smoking industry had a large incentive to find something to blame health problems on other than smoking!)",
    "crumbs": [
      "Confidence Interval"
    ]
  },
  {
    "objectID": "activities/21-confidence-intervals.html#exercise-4-1",
    "href": "activities/21-confidence-intervals.html#exercise-4-1",
    "title": "Confidence Interval",
    "section": "Exercise 4",
    "text": "Exercise 4\nFor each of the following MISINTERPRETATIONS of a 95% confidence interval (a,b), explain why the statement is a misinterpretation.\n\nMisinterpretation 1: “There is a 95% probability that the population parameter is within (a,b).”\n\nResponse: The population parameter is not random. It is either in the interval or not, so the probability is 1 or 0. The 95% means that 95% of random samples (that are representative of the population of interest) are expected to contain the true population parameter—“95% confidence” is describing confidence in the interval construction process.\n\nMisinterpretation 2: “There is a 5% probability that the population parameter is not within (a,b).”\n\nResponse: This is incorrect for the same reason as the first misinterpretation.\n\nMisinterpretation 3: “There is a 95% chance that the sample estimate in (a,b).”\n\nResponse: The sample estimate is always in the interval by construction.",
    "crumbs": [
      "Confidence Interval"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html",
    "href": "activities/18-sampling-normal.html",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#learning-goals",
    "href": "activities/18-sampling-normal.html#learning-goals",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Learning goals",
    "text": "Learning goals\n\nRecognize the difference between a population parameter and a sample estimate.\nReview the Normal probability model, a tool we’ll need to turn information in our sample data into inferences about the broader population.\nExplore the ideas of randomness, sampling distributions, and standard error through a class experiment. (We’ll define these more formally in the next class.)",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#readings-and-videos",
    "href": "activities/18-sampling-normal.html#readings-and-videos",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Readings and videos",
    "text": "Readings and videos\nPlease do the following videos and reading before class.\n\nReading: Section 6 Introduction, and Section 6.6 in the STAT 155 Notes\nVideo 1: exploration vs inference\nVideo 2: Normal probability model",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-1-using-the-normal-model",
    "href": "activities/18-sampling-normal.html#exercise-1-using-the-normal-model",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 1: Using the Normal model",
    "text": "Exercise 1: Using the Normal model\nSuppose that the speeds of cars on a highway, in miles per hour, can be reasonably represented by the Normal model with a mean of 55mph and a standard deviation of 5mph from car to car:\n\\[\nX \\sim N(55, 5^2)\n\\]\n\nshaded_normal(mean = 55, sd = 5)\n\n\n\n\n\n\n\n\n\nProvide the (approximate) range of the middle 68% of speeds, and shade in the corresponding region on your Normal curve. NOTE: a is the lower end of the range and b is the upper end.\n\n\nshaded_normal(mean = 55, sd = 5, a = ___, b = ___)\n## Error in parse(text = input): &lt;text&gt;:1:39: unexpected input\n## 1: shaded_normal(mean = 55, sd = 5, a = __\n##                                           ^\n\n\nUse the 68-95-99.7 rule to estimate the probability that a car’s speed exceeds 60mph.\n\n\nYour response here\n\n\n# Visualize\nshaded_normal(mean = 55, sd = 5, a = 60)\n\n\n\n\n\n\n\n\n\nWhich of the following is the correct range for the probability that a car’s speed exceeds 67mph? Explain your reasoning.\n\n\nless than 0.0015\nbetween 0.0015 and 0.025\nbetween 0.025 and 0.16\ngreater than 0.16\n\n\nExplain your reasoning here\n\n\n# Visualize\nshaded_normal(mean = 55, sd = 5, a = 67)",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-2-z-scores",
    "href": "activities/18-sampling-normal.html#exercise-2-z-scores",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 2: Z-scores",
    "text": "Exercise 2: Z-scores\nInherently important to all of our calculations above is how many standard deviations a value “X” is from the mean.\nThis distance is called a Z-score and can be calculated as follows:\n\\[\n\\text{Z-score} = \\frac{X - \\text{mean}}{\\text{sd}}\n\\]\nFor example (from Exercise 1), if I’m traveling 40 miles an hour, my Z-score is -3. That is, my speed is 3 standard deviations below the average speed:\n\n(40 - 55) / 5\n## [1] -3\n\n\nConsider 2 other drivers. Both drivers are speeding. Who do you think is speeding more, relative to the distributions of speed in their area?\n\nDriver A is traveling at 60mph on the highway where speeds are N(55, 5^2) and the speed limit is 55mph.\nDriver B is traveling at 36mph on a residential road where speeds are N(30, 3^2) and the speed limit is 30mph.\n\n\n\nPut your best guess (hypothesis) here\n\n\nCalculate the Z-scores for Drivers A and B.\n\n\n# Driver A\n\n\n# Driver B\n\n\nNow, based on the Z-scores, who is speeding more? NOTE: The below plots might provide some insights.\n\n\n# Driver A\nshaded_normal(mean = 55, sd = 5) + \n  geom_vline(xintercept = 60)\n\n\n\n\n\n\n\n\n# Driver B\nshaded_normal(mean = 30, sd = 3) + \n  geom_vline(xintercept = 36)  \n\n\n\n\n\n\n\n\n\nYour response here",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-3-parameter-vs-estimate",
    "href": "activities/18-sampling-normal.html#exercise-3-parameter-vs-estimate",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 3: Parameter vs estimate",
    "text": "Exercise 3: Parameter vs estimate\nIt’s important to note that this dataset is by no means a comprehensive collection of films and their review scores–it does not contain every film that was released from 2014-2015, nor films released outside of that date range. The review scores are also frozen in time–all of these films have almost certainly accumulated additional reviews since the data were first collected.\nHowever, our stated goal is to make inferences about the overarching relationship between critic reviews and user reviews for all films (relatedly, we may want to use our model to make predictions about how user reviews are affected by critic reviews for films that may not even exist yet!). Can we actually make these inferences/predictions about a potentially infinite collection of films when all we have is a fairly limited subset of these?\n\nPopulations and Samples\nThis question points to two of the most important concepts in the field of statistical inference: populations and samples. Statisticians have many different ways of defining a population (depending on the questions they are asking), but for the purposes of this exercise, we can think of the population as the set of all possible films and all possible review scores that have been or could be catalogued on RottenTomatoes.\nOur dataset of 146 films is considered a sample of this population. A sample is simply a subset of observations taken from that population.\n\n\n\n\n\n\nSampling Criteria\n\n\n\n\n\nWhen we take a sample of data from a population, there is always some set of criteria used to determine how a sample is taken. This could be as simple as “we randomly selected 1% of all films catalogued on RottenTomatoes as of 4/1/2025”, or a more complex set of specific criteria (for this dataset, the sample was taken by selecting all films that had tickets for sale on Fandango on 8/24/2015, then further filtering to include films that have a Rotten Tomatoes rating, a RT User rating, a Metacritic score, a Metacritic User score, an IMDb score, and at least 30 fan reviews on Fandango.)\n\n\n\n\n\n“True” parameters versus estimates\nIn order to conduct statistical inference using linear regression, we must assume that there is some true, underlying, fixed intercept and slope \\(\\beta_0\\) and \\(\\beta_1\\), that describe the true linear relationship in the overall population that we’re interested in.\nIf we are modeling the relationship between UserScore and CriticScore on RottenTomatoes, The “true” underlying model we assume is thus:\n\\[\nUserScore_i = \\beta_0 + \\beta_1 CriticScore_i + e_i\n\\]\nHowever, the “true” values of \\(\\beta_0\\) and \\(\\beta_1\\) are typically impossible to know, because knowing them requires access to our entire population of interest (in this case, the review scores for every film that has been or will be released). When we fit a regression model using the sample that we do have, we are actually obtaining estimates of those true population parameters (note the notation change of putting a \\(\\hat{ }\\) on top of the Betas, to indicate that this is an estimate):\n\\[\nE[UserScore \\mid CriticScore] = \\hat{\\beta}_0 + \\hat{\\beta}_1 CriticScore\n\\]\nwhere our estimates are given by our model as \\(\\hat{\\beta}_0 = 32.3%\\), \\(\\hat{\\beta}_1 = 0.52%\\)\nFor the sake of this activity, let’s assume that these estimates are identical to the true population parameters.\n🚩🚩🚩 HOWEVER, be very careful not to make this assumption in other models you encounter.For this dataset, recall the specific sampling criteria that were used, which means these 146 films likely aren’t representative of the full population of films we’re interested in. This means that the estimates we obtained probably don’t match the true population parameters–they may or may not be close, but we don’t know for certain! 🚩🚩🚩\nBelow, we’ll simulate how parameter estimates are impacted by taking different samples. You’ll each take a random sample of 10 films in the dataset, and we’ll see if we can recover the presumed population parameters (i.e., the coefficient estimates we obtained from our model using all 146 films that were initially sampled).\nFirst, fill in your intuition below:\n\nDo you think every student will get the same set of 10 films?\n\n\nYour response here\n\n\nDo you think that your coefficient estimates will be the same as your neighbors’?\n\n\nYour responses here",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-4-random-sampling",
    "href": "activities/18-sampling-normal.html#exercise-4-random-sampling",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 4: Random sampling",
    "text": "Exercise 4: Random sampling\n\nUse the sample_n() function to take a random sample of 2 films\n\n\n# Try running the following chunk A FEW TIMES\nsample_n(fandango, size = 2, replace = FALSE)\n## # A tibble: 2 × 3\n##   film                           userscore_rt criticscore_rt\n##   &lt;chr&gt;                                 &lt;int&gt;          &lt;int&gt;\n## 1 Me and Earl and The Dying Girl           89             81\n## 2 Seymour: An Introduction                 87            100\n\nReflect:\n\nHow do your results compare to your neighbors’?\n\n\nYour response here\n\n\nWhat is the role of size = 2? HINT: Remember you can look at function documentation by running ?sample_n in the console!\n\n\nYour response here\n\n\nWhat is the role of replace = FALSE? HINT: Remember you can look at function documentation by running ?sample_n in the console!\n\n\nYour response here\n\n\nNow, “set the seed” to 155 and re-try your sampling.\n\n\n# Try running the following FULL chunk A FEW TIMES\nset.seed(155)\nsample_n(fandango, size = 2, replace = FALSE)\n## # A tibble: 2 × 3\n##   film            userscore_rt criticscore_rt\n##   &lt;chr&gt;                  &lt;int&gt;          &lt;int&gt;\n## 1 Unbroken                  70             51\n## 2 Project Almanac           46             34\n\nWhat changed?\n\nYour response here",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-5-take-your-own-sample",
    "href": "activities/18-sampling-normal.html#exercise-5-take-your-own-sample",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 5: Take your own sample",
    "text": "Exercise 5: Take your own sample\nThe underlying random number generator plays a role in the random sample we happen to get. If we set.seed(some positive integer) before taking a random sample, we’ll get the same results.\nThis reproducibility is important:\n\nwe get the same results every time we render our qmd\nwe can share our work with others & ensure they get our same answers\nit wouldn’t be great if you submitted your work to, say, a journal and weren’t able to back up / confirm / reproduce your results!\n\nFollow the chunks below to obtain and use your own unique sample.\n\n# DON'T SKIP THIS STEP! \n# Set the random number seed to the digits of your own phone number (just the numbers)\nset.seed(123)\n\n# Take a sample of 10 films\nmy_sample &lt;- sample_n(fandango, size = 10, replace = FALSE)\nmy_sample                       \n## # A tibble: 10 × 3\n##    film                                     userscore_rt criticscore_rt\n##    &lt;chr&gt;                                           &lt;int&gt;          &lt;int&gt;\n##  1 Unbroken                                           70             51\n##  2 Terminator Genisys                                 60             26\n##  3 Testament of Youth                                 79             81\n##  4 About Elly                                         86             97\n##  5 Kumiko, The Treasure Hunter                        63             87\n##  6 Two Days, One Night                                78             97\n##  7 Spy                                                82             93\n##  8 The SpongeBob Movie: Sponge Out of Water           55             78\n##  9 Spare Parts                                        83             52\n## 10 Paddington                                         81             98\n\n\n# Plot the relationship of UserScore with CriticScore among your sample\nmy_sample %&gt;% \n  ggplot(aes(y = userscore_rt, x = criticscore_rt)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n# Model the relationship among your sample\nmy_model &lt;- lm(userscore_rt ~ criticscore_rt, data = my_sample)\ncoef(summary(my_model))[,1]\n##    (Intercept) criticscore_rt \n##     58.8478387      0.1954232",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-6-sampling-variation",
    "href": "activities/18-sampling-normal.html#exercise-6-sampling-variation",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 6: Sampling variation",
    "text": "Exercise 6: Sampling variation\nRecall that we are assuming the population parameters are equal to the estimates we obtained from the model we fit using the initial sample of 146 films:\n\\[\nE[UserScore \\mid CriticScore] = 32.3 + 0.52 CriticScore\n\\]\nLet’s explore how our sample estimates of these parameters varied from student to student:\n\n# Import the experiment results\nlibrary(gsheet)\nresults &lt;- gsheet2tbl('https://docs.google.com/spreadsheets/d/11OT1VnLTTJasp5BHSKulgJiCbSLiutv8mKDOfvvXZSo/edit?usp=sharing')\nhead(results)\n## # A tibble: 6 × 4\n##   Timestamp          sample_intercept sample_slope section\n##   &lt;chr&gt;                         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;  \n## 1 3/27/2025 14:07:04             20          0.7   01     \n## 2 3/27/2025 14:07:12             30          0.4   01     \n## 3 3/27/2025 14:09:44             40          0.3   02     \n## 4 4/1/2025 10:48:54              42.0        0.424 02     \n## 5 4/1/2025 10:52:58              34.2        0.541 01     \n## 6 4/1/2025 10:55:04              30.2        0.543 02\n\nPlot each student’s sample estimate of the model line (gray). How do these compare to the assumed population model (red)?\n\nfandango %&gt;% \n  ggplot(aes(y = userscore_rt, x = criticscore_rt)) +\n  geom_abline(data = results, aes(intercept = sample_intercept, slope = sample_slope, linetype=section), color = \"gray\") + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-7-sample-intercepts",
    "href": "activities/18-sampling-normal.html#exercise-7-sample-intercepts",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 7: Sample intercepts",
    "text": "Exercise 7: Sample intercepts\nLet’s focus on just the sample estimates of the intercept parameter:\n\nresults %&gt;% \n  ggplot(aes(x = sample_intercept)) + \n  geom_density() + \n  geom_vline(xintercept = 32.3, color = \"red\")\n\n\n\n\n\n\n\n\nComment on the shape, center, and spread of these sample estimates and how they relate to the (assumed) population intercept (red line).\n\nThe intercepts are roughly normal, centered around the intercept of the larger sample (32.3), and range from roughly 3.517 to 50.278",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-8-slopes-finish-after-the-class",
    "href": "activities/18-sampling-normal.html#exercise-8-slopes-finish-after-the-class",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 8: Slopes [Finish after the Class]",
    "text": "Exercise 8: Slopes [Finish after the Class]\nSuppose we were to construct a density plot of the sample estimates of the criticscore_rt coefficient (i.e. the slopes).\n\nIntuitively, what shape do you think this plot will have?\n\n\nYour response here\n\n\nIntuitively, around what value do you think this plot will be centered?\n\n\nYour response here\n\n\nCheck your intuition:\n\n\nresults %&gt;% \n  ggplot(aes(x = sample_slope)) + \n  geom_density() + \n  geom_vline(xintercept = 0.52, color = \"red\")\n\n\n\n\n\n\n\n\n\nThinking back to the 68-95-99.7 rule, visually approximate the standard deviation among the sample slopes.\n\n\nYour response here",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-9-standard-error-finish-after-the-class",
    "href": "activities/18-sampling-normal.html#exercise-9-standard-error-finish-after-the-class",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 9: Standard error [Finish after the Class]",
    "text": "Exercise 9: Standard error [Finish after the Class]\nYou’ve likely observed that the typical or mean slope estimate is roughly equal to the (assumed) population slope parameter of 0.52:\n\nresults %&gt;% \n  summarize(mean(sample_slope))\n## # A tibble: 1 × 1\n##   `mean(sample_slope)`\n##                  &lt;dbl&gt;\n## 1                0.560\n\nThus the standard deviation of the slope estimates measures how far we might expect an estimate to fall from the (assumed) population slope parameter.\nThat is, it measures the typical or standard error in our sample estimates:\n\nresults %&gt;% \n  summarize(sd(sample_slope))\n## # A tibble: 1 × 1\n##   `sd(sample_slope)`\n##                &lt;dbl&gt;\n## 1              0.194\n\n\nRecall your sample estimate of the slope. How far is it from the population slope, 0.52?\n\n\nHow many standard errors does your estimate fall from the population slope? That is, what’s your Z-score?\n\n\nReflecting upon your Z-score, do you think your sample estimate was one of the “lucky” ones, or one of the “unlucky” ones?\n\n\nYour response here",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-1-using-the-normal-model-1",
    "href": "activities/18-sampling-normal.html#exercise-1-using-the-normal-model-1",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 1: Using the Normal model",
    "text": "Exercise 1: Using the Normal model\n\n.\n\n\nshaded_normal(mean = 55, sd = 5, a = 50, b = 60)\n\n\n\n\n\n\n\n\n\n16% (32/2)\nbetween 0.0015 and 0.025",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-2-z-scores-1",
    "href": "activities/18-sampling-normal.html#exercise-2-z-scores-1",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 2: Z-scores",
    "text": "Exercise 2: Z-scores\n\nintuition\n.\n\n\n# Driver A\n(60 - 55) / 5\n## [1] 1\n\n# Driver B\n(36 - 30) / 3\n## [1] 2\n\n\nB, they are 2 standard deviations above the mean (the speed limit)",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-3-parameter-vs-estimate-1",
    "href": "activities/18-sampling-normal.html#exercise-3-parameter-vs-estimate-1",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 3: Parameter vs estimate",
    "text": "Exercise 3: Parameter vs estimate\nintuition",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-4-random-sampling-1",
    "href": "activities/18-sampling-normal.html#exercise-4-random-sampling-1",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 4: Random sampling",
    "text": "Exercise 4: Random sampling\n\n# Observe that the 2 films change every time & differ from your neighbors' samples\nsample_n(fandango, size = 2, replace = FALSE)\n## # A tibble: 2 × 3\n##   film                                userscore_rt criticscore_rt\n##   &lt;chr&gt;                                      &lt;int&gt;          &lt;int&gt;\n## 1 The Woman In Black 2 Angel of Death           25             22\n## 2 Get Hard                                      48             29\n\n\n# Observe that the 2 films are the same every time & are the same as your neighbors' samples\nset.seed(155)\nsample_n(fandango, size = 2, replace = FALSE)\n## # A tibble: 2 × 3\n##   film            userscore_rt criticscore_rt\n##   &lt;chr&gt;                  &lt;int&gt;          &lt;int&gt;\n## 1 Unbroken                  70             51\n## 2 Project Almanac           46             34",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-5-take-your-own-sample-1",
    "href": "activities/18-sampling-normal.html#exercise-5-take-your-own-sample-1",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 5: Take your own sample",
    "text": "Exercise 5: Take your own sample\nwill vary from student to student",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-6-sampling-variation-1",
    "href": "activities/18-sampling-normal.html#exercise-6-sampling-variation-1",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 6: Sampling variation",
    "text": "Exercise 6: Sampling variation\nThe sample estimates vary around the population model:\n\n# Import the experiment results\nlibrary(gsheet)\nresults &lt;- gsheet2tbl('https://docs.google.com/spreadsheets/d/11OT1VnLTTJasp5BHSKulgJiCbSLiutv8mKDOfvvXZSo/edit?usp=sharing')\n\nfandango %&gt;% \n  ggplot(aes(y = userscore_rt, x = criticscore_rt)) +\n  geom_abline(data = results, aes(intercept = sample_intercept, slope = sample_slope, linetype=section), color = \"gray\") + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-7-sample-intercepts-1",
    "href": "activities/18-sampling-normal.html#exercise-7-sample-intercepts-1",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 7: Sample intercepts",
    "text": "Exercise 7: Sample intercepts\nThe intercepts are roughly normal, centered around the intercept of the larger sample (32.3), and range from roughly 3.517 to 50.278:\n\nresults %&gt;% \n  ggplot(aes(x = sample_intercept)) + \n  geom_density() + \n  geom_vline(xintercept = 32.3, color = \"red\")",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-8-slopes",
    "href": "activities/18-sampling-normal.html#exercise-8-slopes",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 8: Slopes",
    "text": "Exercise 8: Slopes\n\nintuition\nintuition\nCheck your intuition:\n\n\nresults %&gt;% \n  ggplot(aes(x = sample_slope)) + \n  geom_density() + \n  geom_vline(xintercept = 0.52, color = \"red\")\n\n\n\n\n\n\n\n\n\nWill vary, but should roughly be 0.19.",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/18-sampling-normal.html#exercise-9-standard-error",
    "href": "activities/18-sampling-normal.html#exercise-9-standard-error",
    "title": "Tools for Inference: Sampling-Normal",
    "section": "Exercise 9: Standard error",
    "text": "Exercise 9: Standard error\n\nFor example, suppose my estimate were 0.7:\n\n\n0.7 - 0.52\n## [1] 0.18\n\n\nFor example, suppose my estimate were 0.7. Then my Z-score is (0.7 - 0.52) / 0.19 = 0.9473684\nThis is somewhat subjective. But we’ll learn that if your estimate is within 2 sd of the actual slope, i.e. your Z-score is between -2 and 2, you’re pretty “lucky”.",
    "crumbs": [
      "Tools for Inference: Sampling-Normal"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html",
    "href": "activities/15+16-Logistic-Regression-I.html",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#readings-and-videos",
    "href": "activities/15+16-Logistic-Regression-I.html#readings-and-videos",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Readings and videos",
    "text": "Readings and videos\nGo through the following reading or videos before class:\n\nReading: Sections 2.5, Section 6.2 introduction, and Sections 4.1-4.3 in the STAT 155 Notes\nVideos:\n\nProb vs. Odds vs. Log Odds (script)\nCalculating Probability and Odds from 2x2 Tables (script)\nLogistic regression (slides)\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder.",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-1-exploring-first-steps-enrollment-and-gestational-age",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-1-exploring-first-steps-enrollment-and-gestational-age",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 1: Exploring First Steps enrollment and Gestational Age",
    "text": "Exercise 1: Exploring First Steps enrollment and Gestational Age\nA baby born prior to 37 weeks is considered premature. In figuring out whether we have evidence that the First Steps program is associated with better birth outcomes than those not in the First Steps program, we can look at whether the individuals in the program are more likely to have preterm babies.\nBelow, we make a 2x2 table in R:\n\n# 2x2 Table: preterm vs. First Steps\nfirststeps %&gt;% \n    count(preterm, firstep)\n## # A tibble: 4 × 3\n##   preterm firstep     n\n##   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n## 1 No            0  1879\n## 2 No            1   343\n## 3 Yes           0   218\n## 4 Yes           1    60\n\nYou may be wondering why this is called a 2x2 table, when it looks as though the table has four rows and three columns. The data can be re-arranged (and usually is, in a formal report) as follows…\n\ntable(firststeps$preterm, firststeps$firstep)\n##      \n##          0    1\n##   No  1879  343\n##   Yes  218   60\n\n… but it’s much cleaner to code up the original way!\n\nHow many birth parents were enrolled in the First Steps program? Which rows did you use to calculate this number?\n\n\n343 + 60 = 403 parents were enrolled in the First Steps program. I used both rows of the table where firstep = 1.\n\n\nWhat percentage of people in the study were enrolled in the First Steps program? Recall: there were 2500 participants! You can confirm this by adding up the entire third column of the table\n\n\n16.12% of people in the study were enrolled in First Steps!\n\n\n403 / 2500\n## [1] 0.1612\n\n\nHow many birth parents who were enrolled in First Steps had a premature baby?\n\n\n60 birth parents\n\n\nWhat percentage of birth parents in First Steps had a premature baby? Think carefully about the numerator and denominator you use to calculate this!\n\n\n14.89% (60 / 403) of birth parents in First Steps had a premature baby.\n\n\n60 / 403\n## [1] 0.1488834\n\n\nWhat percentage of birth parents who had a premature baby were enrolled in First Steps? Think carefully about the numerator and denominator you use to calculate this!\n\nThe total number of birth parents who had a premature baby was 218 + 60 = 278. Of those. 60 were enrolled in First Steps. Therefore, 21.58% (60/278) of birth parents who had a premature baby were enrolled in First Steps.\n\n60/278\n## [1] 0.2158273\n\nCongratulations! If you’ve made it to this point, you already intuitively know what marginal and conditional probabilities are. Formally,\n\na marginal probability, denoted \\(P(A)\\) for an event \\(A\\), is the probability that \\(A\\) occurs overall. You calculated the marginal probability that people were enrolled in First Steps in part (b)! In this case, the denominator used to calculate the probability was the total number of people in the study.\na conditional probability, denoted \\(P(A | B)\\) for events \\(A\\) and \\(B\\), is the probability that \\(A\\) occurs given that event \\(B\\) occurs. You calculated the conditional probability that a premature baby was born given that a parent was in First Steps in part (d)! In this case, the denominator used to calculate the probability was the total number of birth parents in the First Steps program. You also calculated a conditional probability in part (e).\n\nUsing formal probability notation, write the probabilities you calculated in parts (b), (d), and (e) as\n\\[\nb. P(\\text{First Steps}) = .1612\n\\]\n\\[\nd. P(\\text{Preterm} | \\text{First Steps}) = .1488\n\\]\n\\[\ne. P(\\text{First Steps} | \\text{Preterm}) = .2158\n\\]\nNote that the conditional probabilities calculated in parts (d) and (e) are not the same! This is because which event you condition on alters the denominator, and the event you’re interested in alters the numerator.\n\nTo determine if gestational age differed by enrollment in First Steps, we’ll want to calculate the conditional probability that a baby is born prematurely given First Steps enrollment (Done!.1488), and given that a parent is not enrolled in First Steps. Use the 2x2 table to calculated this conditional probability.\n\n\\[\nP(\\text{Preterm} | \\text{Not in First Steps}) = 218 / (218 + 1879) = 0.103958\n\\]\n\nA ratio of conditional probabilities, where the conditioning event is the same for both, tells us how many times more likely an event is to occur for one group compared to another. Calculate how many times more likely a birth parent enrolled in First Steps is to have a premature baby compared to birth parents not enrolled in First Steps.\n\n\\[\n\\frac{(\\text{Preterm} | \\text{First Steps})}{P(\\text{Preterm} | \\text{Not in First Steps})} = .1488 / 0.103958 = 1.43\n\\]\n\nWrite a two-sentence summary, appropriate for a general audience, summarizing your results in terms of a ratio of probabilities. Does gestational age appear to differ greatly by First Steps enrollment? What does this imply about the effectiveness of the First Steps program, if anything?\n\n\nParents in this study in the First Steps program are 1.43 times more likely to have a premature birth than those not enrolled in the First Steps program, indicating that gestational age does differ by First Steps enrollment. This implies that enrollment in the First Steps program may not be associated with better birth outcomes, as measured by gestational age. [ Might not be a fair comparison! Ideally, we would compare birth outcomes from mothers in the First Steps program to the birth outcomes from those same mothers not in the First Steps program, to determine if the program made a positive impact.]\n\n\nTo go along with your summary, let’s make a visualization! There are three basic options for visualization two categorical variables. All are perfectly valid, but some may be more useful to read than others, and display different information.\n\nYou’ll see one other fancier option (called a mosaic plot) in the next activity.\n\n# Side-by-side bar chart\nfirststeps %&gt;%\n  ggplot(aes(firstep, fill = preterm)) +\n  geom_bar(position = \"dodge\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n# Stacked bar chart\nfirststeps %&gt;%\n  ggplot(aes(firstep, fill = preterm)) +\n  geom_bar() +\n  theme_classic()\n\n\n\n\n\n\n\n\n# Stacked relative frequency bar chart\nfirststeps %&gt;%\n  ggplot(aes(firstep, fill = preterm)) +\n  geom_bar(position = \"fill\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nBonus Question: Which of the above three plots allows you to directly see the conditional probabilities we calculated previously?\n\nPlace your answer here",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-2-exploring-first-steps-enrollment-and-low-birthweights",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-2-exploring-first-steps-enrollment-and-low-birthweights",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 2: Exploring First Steps enrollment and Low birthweights",
    "text": "Exercise 2: Exploring First Steps enrollment and Low birthweights\nAnother birth outcome we can consider when comparing those enrolled in the First Steps program to those not enrolled is birth weight. A baby is considered to have low birth weight when birth weight is less than 2500 grams.\n\nFill in the code below to make a table comparing low_bwt to firsteps.\n\n\n# 2x2 Table: low_bwt vs. First Steps\nfirststeps %&gt;%\n  count(low_bwt, firstep)\n## # A tibble: 4 × 3\n##   low_bwt firstep     n\n##   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n## 1 low           0   102\n## 2 low           1    25\n## 3 not low       0  1995\n## 4 not low       1   378\n\n\nUsing the table from part (a), calculate the following conditional probabilities:\n\n\\[\nP(\\text{Low birth weight} | \\text{First Steps}) = 25 / (25 + 378) = 0.062\n\\]\n\\[\nP(\\text{Normal birth weight} | \\text{First Steps}) = 378 / (25 + 378) = 0.938\n\\]\n\\[\nP(\\text{Low birth weight} | \\text{Not in First Steps}) = 102 / (102 + 1995) = 0.049\n\\]\n\\[\nP(\\text{Normal birth weight} | \\text{Not in First Steps}) = 1995 / (102 + 1995) = 0.951\n\\]\nAn additional numerical summary that is often useful when working with indicator variables is odds. Odds are defined as\n\\[\nOdds = \\frac{p}{1 - p}\n\\]\nwhere \\(p\\) is the probability that an event occurs. Therefore, if we know \\(p\\), we can calculate the odds that an event happens! Similarly, if we know the odds, we can calculate \\(p\\) using\n\\[\np = Odds / (1 + Odds)\n\\]\nWe can also calculate odds from our 2x2 (or 3x2, 4x2, …) tables. In colloquial terms, probabilities are “yes”’s over “total”’s, and odds are “yes”’s over “no’s”. In pseudo-math:\n\\[\np = \\frac{Yes}{Total}, \\quad Odds = \\frac{Yes}{No}\n\\] We’ll see why odds are especially useful when we have binary outcome variables in a regression model in the next activity. For now, note that they’re also commonly used in lots of contexts: sports, gambling, case-control studies, etc.\n\nUsing your answer to part (b), calculate the following odds\n\n\\[\nOdds(\\text{Low birth weight} | \\text{First Steps}) = 0.062 / (1 - 0.062) = 0.06609808\n\\]\n\\[\nOdds(\\text{Normal birth weight} | \\text{First Steps}) = 0.938 / (1 - 0.938) = 15.12903\n\\]\n\\[\nOdds(\\text{Low birth weight} | \\text{Not in First Steps}) = 0.049 / (1 - 0.049) = 0.05152471\n\\] \\[\nOdds(\\text{Normal birth weight} | \\text{Not in First Steps}) = 0.951 / (1 - 0.951) = 19.40816\n\\]\n\nA ratio of odds (called an odds ratio, unsurprisingly) tells us how many times higher or greater the odds are that an event occurs, comparing one group to another. This might sound irritatingly circular. The key here is that while odds ratios do allow us to compare binary/indicator outcomes from one group to one another, they do not tell us how much more likely an event is to occur comparing those same groups. This is distinct from ratios of probabilities!\n\nCalculate the ratio of the odds of having a low-birth-weight baby, comparing those in the First Steps program to those not in the First Steps program (i.e., how many times higher/lower is the odds of having a low-birth-weight baby among those in First Steps as compared to those not in First Steps?)\n\n0.06609808 / 0.05152471\n## [1] 1.282842\n\n\nWrite a two-sentence summary, appropriate for a general audience, summarizing your results in terms of an odds ratio. Does birth weight appear to differ greatly by First Steps enrollment? What does this imply about the effectiveness of the First Steps program, if anything?\n\n\nThe odds of having a low birth weight baby are 1.28 times higher for those enrollment in First Steps compared to those not in First Steps. Just as in Exercise 1, this implies that the First Steps program may not be associated with improved birth outcomes (with the same caveats as given in the answer to 1 (h)).\n\n\nTo go along with your summary, add code below to make one of the three visualization options we tried out in Exercise 1.\n\n\n# Stacked relative frequency bar chart (with some fancy aesthetics)\nfirststeps %&gt;%\n  mutate(Birthweight = low_bwt %&gt;% str_to_title()) %&gt;%\n  ggplot(aes(firstep, fill = Birthweight)) +\n  geom_bar(position = \"fill\") +\n  theme_classic() +\n  scale_fill_viridis_d(option = \"H\") +\n  labs(x = \"First Steps\", title = \"Birthweight by First Steps Enrollment\") +\n  scale_x_continuous(breaks = c(0,1), labels = c(\"Not Enrolled\", \"Enrolled\"))",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-3-conditional-vs.-marginal-probabilities",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-3-conditional-vs.-marginal-probabilities",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 3: Conditional vs. Marginal probabilities",
    "text": "Exercise 3: Conditional vs. Marginal probabilities\nSuppose we select a person at random from the entire global population. For each of the following probabilities, which do you think is bigger? Explain your reasoning.\n\nP(lung cancer) or P(lung cancer | smoker)\n\n\nYour response here\n\n\nP(likes McDonald’s) or P(likes McDonald’s | vegetarian)\n\n\nYour response here\n\n\nP(smart | Mac grad) or P(Mac grad | smart)\n\n\nYour response here",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-4-probability-practice",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-4-probability-practice",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 4: Probability practice",
    "text": "Exercise 4: Probability practice\nLet’s explore whether birthweight of a baby varies by whether or not it was the first child that a mother had, and whether this relationship differs by First Steps enrollment. We make a table below:\n\nfirststeps %&gt;%\n  count(firstchild, low_bwt, firstep)\n## # A tibble: 8 × 4\n##   firstchild low_bwt firstep     n\n##   &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n## 1 No         low           0    43\n## 2 No         low           1    13\n## 3 No         not low       0  1080\n## 4 No         not low       1   198\n## 5 Yes        low           0    59\n## 6 Yes        low           1    12\n## 7 Yes        not low       0   915\n## 8 Yes        not low       1   180\n\n\nWhat is the probability that a mother enrolled in First steps who is having their first child, has a baby who is born at a low birthweight? Calculate your answer, and write it using formal probability notation.\n\n\nP(___ | ___) = ?\n\n\nWhat is the probability that a mother not enrolled in First steps who is having their first child, has a baby who is born at a low birthweight? Calculate your answer, and write it using formal probability notation.\n\n\nP(___ | ___) = ?\n\n\nWhat is the probability that a mother’s first child has a low birthweight? Calculate your answer, and write it using formal probability notation.\n\n\nP(___ | ___) = ?\n\n\nHow many times more likely is a child to be born at a low birthweight, comparing children who are the first born to those not first born?\n\n\nP(___ | ) / P( | ___) = ?",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-1-exploring-age",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-1-exploring-age",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 1: Exploring age",
    "text": "Exercise 1: Exploring age\nDid younger passengers tend to have higher survival rates than older passengers?\nVisualizing the relationship between a binary response and a quantitative predictor can be tricky. We will take a few approaches here.\n\nCreate a boxplot where one box corresponds to the age distribution of survivors and the second to that of non-survivors.\nCreate density plots with separate colors for the survivors and non-survivors.\nThe remainder of the code below creates a plot of the fraction who survived at each age. (Since we have a large data set and multiple (though sometimes not many) observations at most ages, we can manually calculate the survival fraction.\n\nAfter inspecting the plots, summarize what you learn.\n\n# Create a boxplot\n# Note that you'll need to force R to view Survived as a binary categorical variable by using x = factor(Survived) instead of just x = Survived in the aes() part of your plot\n\nggplot(titanic, aes(x = factor(Survived), y = Age)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n# Can flip the boxplot on its side too\nggplot(titanic, aes(y = factor(Survived), x = Age)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n# Create a density plot (you'll need to use factor(Survived) again)\nggplot(titanic, aes(x = Age, color = factor(Survived))) +\n    geom_density()\n\n\n\n\n\n\n\n\n# Use the code below to create a plot of the fraction who survived at each age\ntitanic_summ &lt;- titanic %&gt;% \n    group_by(Age) %&gt;%\n    summarize(frac_survived = mean(Survived))\n\nggplot(titanic_summ, aes(x = Age, y = frac_survived)) +\n    geom_point() +\n    geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\nThe boxplot doesn’t clearly indicate a difference in the age distributions across survivors and non-survivors, but we do notice from the density plot that there is a greater density of younger passengers among the survivors. We also see from the last plot that younger passengers tend to have a higher survival chance.",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-2-exploring-sex-and-ticket-class",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-2-exploring-sex-and-ticket-class",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 2: Exploring sex and ticket class",
    "text": "Exercise 2: Exploring sex and ticket class\nWere males or females more likely to survive? Did 1st class passengers tend to survive more than 2nd and 3rd class passengers?\nThe code below creates plots that allow us to explore how Sex and PClass relate to survival. The first two plots are standard bar plots that use color to indicate what fraction of each group survived. The last two plots are mosaic plots that are much like the standard bar plots, but the width of the bars reflects the distribution of the x-axis variable. (The widest bar is the most prevalent category.)\nSummarize what you learn about the relationship between sex, ticket class, and survival.\n\n# Standard bar plots\nggplot(titanic, aes(x = Sex, fill = factor(Survived))) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nggplot(titanic, aes(x = PClass, fill = factor(Survived))) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n# Mosaic plots\nggplot(data = titanic %&gt;% mutate(Survived = as.factor(Survived))) +\n    geom_mosaic(aes(x = product(Sex), fill = Survived))\n\n\n\n\n\n\n\n\nggplot(data = titanic %&gt;% mutate(Survived = as.factor(Survived))) +\n    geom_mosaic(aes(x = product(PClass), fill = Survived))\n\n\n\n\n\n\n\n\n\nFemales were more likely to survive than males. 1st class was most likely to survive, followed by 2nd then 3rd class.",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-3-linear-regression-model",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-3-linear-regression-model",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 3: Linear regression model",
    "text": "Exercise 3: Linear regression model\nFor now we will focus on exploring the relationship between (ticket) class and survival.\nLet’s tabulate survival across classes. We can tabulate across two variables by providing both variables to count():\n\ntitanic %&gt;% \n    count(PClass, Survived)\n## # A tibble: 7 × 3\n##   PClass Survived     n\n##   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n## 1 1st           0   129\n## 2 1st           1   193\n## 3 2nd           0   160\n## 4 2nd           1   119\n## 5 3rd           0   573\n## 6 3rd           1   138\n## 7 &lt;NA&gt;          0     1\n\n\nUsing the count() output, we can fill in the following contingency table:\n\n\n\n\nClass\nDied\nSurvived\nTotal\n\n\n\n\n1st Class\n129\n193\n322\n\n\n2nd Class\n160\n119\n279\n\n\n3rd Class\n573\n138\n711\n\n\nTotal\n862\n450\n1312\n\n\n\n\nUsing the table, we can estimate the followings:\n\nthe probability of surviving among 1st class passengers: 193/322 = 0.599\nthe probability of surviving among 2nd class passengers: 119/279 = 0.427\nthe probability of surviving among 3rd class passengers: 138/711 = 0.194\nthe difference in the probability of surviving, comparing 2nd class passengers to 1st class passengers (i.e., how much lower is the probability of 2nd class passengers as compared to 1st class passengers?): 119/279 - 193/322 = -0.173\nthe difference in the probability of surviving, comparing 3rd class passengers to 1st class passengers (i.e., how much lower is the probability of 3rd class passengers as compared to 1st class passengers?): 138/711 - 193/322 = -0.405\n\nAfter fitting the linear regression model below, write out the model formula using correct notation. Explain carefully what it means to talk about the expected/average value of a binary variable.\n\n\nlin_mod &lt;- lm(Survived ~ PClass, data = titanic)\nsummary(lin_mod)\n## \n## Call:\n## lm(formula = Survived ~ PClass, data = titanic)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.5994 -0.1941 -0.1941  0.4006  0.8059 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.59938    0.02468  24.284  &lt; 2e-16 ***\n## PClass2nd   -0.17286    0.03623  -4.772 2.03e-06 ***\n## PClass3rd   -0.40529    0.02975 -13.623  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4429 on 1309 degrees of freedom\n##   (1 observation deleted due to missingness)\n## Multiple R-squared:  0.1315, Adjusted R-squared:  0.1302 \n## F-statistic: 99.09 on 2 and 1309 DF,  p-value: &lt; 2.2e-16\n\nThis model can be written as: \\(E[Survived | PClass] = \\beta_0 + \\beta_1 PClass2nd + \\beta_2 PClass3rd\\). - In the context of a binary variable, the expected value/average is the same as the probability that the variable equals one. To see an example of this, calculate the average of this list of 0’s and 1’s: (0,0,1,1,0,1,0,1). Now calculate the proportion of 1’s. What do you notice? - This means that we can also write this model as follows: \\(P[Survived = 1 | PClass] = \\beta_0 + \\beta_1 PClass2nd + \\beta_2 PClass3rd\\)\n\nWrite an interpretation of each of the coefficients in your linear regression model. How do your coefficient estimates compare to your answers in part b?\n\nThe coefficient estimates are the differences in probability from part b! - (Intercept): the estimated probability of survival for passengers in 1st class is 0.599 (59.9%) - PClass2nd: the difference in the estimated probability of survival comparing passengers in 1st class to passengers in 2nd class is 0.173 (17.3%), where passengers in 1st class have the higher estimated survival probability - OR… comparing passengers in 1st class to passengers in 2nd class, the difference in the proportion of passengers that survived is 0.173 (17.3%), with 1st class having a higher proportion of passengers that survived - OR… the probability of survival is 17.3% lower among passengers in 2nd class than it is among passengers in 1st class - PClass3rd: the difference in the estimated probability of survival comparing passengers in 1st class to passengers in 3rd class is 0.405 (40.5%), where passengers in 1st class have the higher estimated survival probability",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-4-logistic-regression-model-categorical-predictor",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-4-logistic-regression-model-categorical-predictor",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 4: Logistic regression model (categorical predictor)",
    "text": "Exercise 4: Logistic regression model (categorical predictor)\n\nRefer back to your contingency table from Exercise 3a. Using your table, estimate the following:\n\nthe odds of surviving among 1st class passengers\nthe odds of surviving among 2nd class passengers\nthe odds of surviving among 3rd class passengers\nthe ratio of the odds of surviving, comparing 2nd class passengers to 1st class passengers (i.e., how many times higher/lower is the odds of survival among 2nd class passengers as compared to 1st class passengers?)\nthe ratio of the odds of surviving, comparing 3rd class passengers to 1st class passengers\n\n\nHints: Remember that we can also calculate odds from our 2x2 (or 3x2, 4x2, …) tables. In colloquial terms, probabilities are “yes”’s over “total”’s, and odds are “yes”’s over “no’s”. In pseudo-math:\n\\[\np = \\frac{Yes}{Total}, \\quad Odds = \\frac{Yes}{No}\n\\]\n\n\n\nClass\nDied\nSurvived\nTotal\n\n\n\n\n1st Class\n129\n193\n322\n\n\n2nd Class\n160\n119\n279\n\n\n3rd Class\n573\n138\n711\n\n\nTotal\n862\n450\n1312\n\n\n\n\n\nthe odds of surviving among 1st class passengers: 193/129 = 1.496\nthe odds of surviving among 2nd class passengers: 119/160 = 0.744\nthe odds of surviving among 3rd class passengers: 138/573 = 0.241\nthe ratio of the odds of surviving, comparing 2nd class passengers to 1st class passengers (i.e., how many times higher/lower is the odds of survival among 2nd class passengers as compared to 1st class passengers?): (119/160)/(193/129) = 0.497\nthe ratio of the odds of surviving, comparing 3rd class passengers to 1st class passengers: (138/573)/(193/129) = 0.161\n\n\n\nAfter fitting the logistic regression model below, write out the model formula using correct notation.\n\n\nlog_mod &lt;- glm(Survived ~ PClass, data = titanic, family = \"binomial\")\ncoef(summary(log_mod))\n##               Estimate Std. Error    z value     Pr(&gt;|z|)\n## (Intercept)  0.4028778  0.1137246   3.542574 3.962427e-04\n## PClass2nd   -0.6989281  0.1660923  -4.208071 2.575600e-05\n## PClass3rd   -1.8265098  0.1480705 -12.335410 5.839072e-35\n\n\\(\\log(Odds[Survived = 1 | PClass]) = \\beta_0 + \\beta_1 PClass2nd + \\beta_2 PClass3rd\\)\n\nWrite an interpretation of each of the exponentiated coefficients in your logistic regression model. Think carefully about what we are modeling when we fit a logistic regression model. How do these exponentiated coefficient estimates compare to your answers in part a?\n\n\nThese exponentiated coefficient estimates compare to your the odds and odds ratios in part a! - exp(Intercept): the estimated odds of survival among passengers in first class is 1.496 (i.e., passengers in first class are 1.496 times more likely to survive than they are to die) - exp(PClass2nd): we estimate that the odds of survival for passengers in 2nd class are only 0.497 times as high as the odds of survival among passengers in 1st class (i.e., the odds of survival are 2 times higher among passengers in 1st class than they are among passengers in 2nd class) - exp(PClass3rd): we estimate that the odds of survival for passengers in 3rd class are only 0.16 times as high as the odds of survival among passengers in 1st class (i.e., the odds of survival are 1/0.16 = 6.21 times higher among passengers in 1st class than they are among passengers in 3rd class)",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-5-logistic-regression-model-quantitative-predictor",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-5-logistic-regression-model-quantitative-predictor",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 5: Logistic regression model (quantitative predictor)",
    "text": "Exercise 5: Logistic regression model (quantitative predictor)\nNow we will explore how to interpret a quantitative predictor in a logistic regression model.\n\nAfter fitting the logistic regression model below, write out the model formula using correct notation.\n\n\nlog_mod &lt;- glm(Survived ~ Age, data = titanic, family = \"binomial\")\ncoef(summary(log_mod))\n##                Estimate Std. Error    z value   Pr(&gt;|z|)\n## (Intercept) -0.08142783 0.17386170 -0.4683483 0.63953556\n## Age         -0.00879462 0.00523158 -1.6810637 0.09275054\n\n\\(\\log(Odds[Survived = 1 | Age]) = \\beta_0 + \\beta_1 Age\\)\n\nWrite an interpretation of each of the exponentiated coefficients in this logistic regression model.",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-6-linear-vs.-logistic-modeling",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-6-linear-vs.-logistic-modeling",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 6: Linear vs. logistic modeling",
    "text": "Exercise 6: Linear vs. logistic modeling\nTo highlight a key difference between linear vs. logistic modeling, consider the following linear and logistic regression models of survival with sex and age as predictors in addition to ticket class.\n\nlin_mod2 &lt;- lm(Survived ~ PClass + Sex + Age, data = titanic)\ncoef(summary(lin_mod2))\n##                 Estimate  Std. Error    t value     Pr(&gt;|t|)\n## (Intercept)  1.130522829 0.051940872  21.765573 8.158449e-82\n## PClass2nd   -0.207433817 0.039239825  -5.286308 1.637737e-07\n## PClass3rd   -0.393344488 0.037709874 -10.430809 7.001373e-24\n## Sexmale     -0.501325667 0.029419802 -17.040416 2.697807e-55\n## Age         -0.006004789 0.001105949  -5.429536 7.633977e-08\n\nlog_mod2 &lt;- glm(Survived ~ PClass + Sex + Age, data = titanic, family = \"binomial\")\ncoef(summary(log_mod2))\n##                Estimate  Std. Error    z value     Pr(&gt;|z|)\n## (Intercept)  3.75966210 0.397567324   9.456668 3.179129e-21\n## PClass2nd   -1.29196240 0.260075781  -4.967638 6.777324e-07\n## PClass3rd   -2.52141915 0.276656805  -9.113888 7.948131e-20\n## Sexmale     -2.63135683 0.201505379 -13.058494 5.684093e-39\n## Age         -0.03917681 0.007616218  -5.143868 2.691392e-07\n\n\nUse the linear regression model to predict the probability of survival for Rose (a 17 year old female in 1st class) and Jack (a 20 year old male in 3rd class). Show your work.\n\n\n## predict for Rose\n## (by hand)\n1.130523 + (-0.207434)*0 + (-0.393344)*0 + (-0.501326)*0 + (-0.006005)*17 \n## [1] 1.028438\n\n## (using predict)\npredict(lin_mod2, newdata = data.frame(PClass = \"1st\", Sex = \"female\", Age = 17))\n##        1 \n## 1.028441\n\n## predict for Jack\n## (by hand)\n1.130523 + (-0.207434)*0 + (-0.393344)*1 + (-0.501326)*1 + (-0.006005)*20\n## [1] 0.115753\n\n## (using predict)\npredict(lin_mod2, newdata = data.frame(PClass = \"3rd\", Sex = \"male\", Age = 20))\n##         1 \n## 0.1157569\n\n\nNow use the logistic regression model to predict the survival probability for Rose and Jack. Show your work. (Hint: use the logistic regression model to obtain the predicted log odds, exponentiate to get the odds, and then convert to probability.)\n\n\n## predict for Rose\n## (by hand)\nlog_odds_rose &lt;- 3.75966210 + (-1.29196240)*0 + (-2.52141915)*0 + (-2.63135683)*0 + (-0.03917681)*17\nodds_rose &lt;- exp(log_odds_rose)\nodds_rose/(1+odds_rose)\n## [1] 0.9566303\n\n## (using predict)\npredict(log_mod2, newdata = data.frame(PClass = \"1st\", Sex = \"female\", Age = 17), type = \"response\")\n##         1 \n## 0.9566303\n\n## predict for Jack\n## (by hand)\nlog_odds_jack &lt;- 3.75966210 + (-1.29196240)*0 + (-2.52141915)*1 + (-2.63135683)*1 + (-0.03917681)*20\nodds_jack &lt;- exp(log_odds_jack)\nodds_jack/(1+odds_jack)\n## [1] 0.101867\n\n## (using predict)\npredict(log_mod2, newdata = data.frame(PClass = \"3rd\", Sex = \"male\", Age = 20), type = \"response\")\n##        1 \n## 0.101867\n\n\nComment on differences that you notice in the predictions from parts a and b.\n\n\nOur linear model predicted that Rose’s probability of survival was over 100% (which doesn’t make sense). The predictions for Jack are fairly similar: 10.2% based on our logistic model and 11.6% based on our linear model.",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-1-exploring-first-steps-enrollment-and-gestational-age-1",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-1-exploring-first-steps-enrollment-and-gestational-age-1",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 1: Exploring First Steps enrollment and Gestational Age",
    "text": "Exercise 1: Exploring First Steps enrollment and Gestational Age\n\n# 2x2 Table: preterm vs. First Steps\nfirststeps %&gt;% \n    count(preterm, firstep)\n## # A tibble: 4 × 3\n##   preterm firstep     n\n##   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n## 1 No            0  1879\n## 2 No            1   343\n## 3 Yes           0   218\n## 4 Yes           1    60\n\n\n343 + 60 = 403 parents were enrolled in the First Steps program. I used both rows of the table where firstep = 1.\n16.12% of people in the study were enrolled in First Steps!\n\n\n403 / 2500\n## [1] 0.1612\n\n\n60 birth parents\n14.89% of birth parents in First Steps had a premature baby.\n\n\n60 / 403\n## [1] 0.1488834\n\n\nThe total number of birth parents who had a premature baby was 218 + 60 = 278. Of those. 60 were enrolled in First Steps. Therefore, 21.58% of birth parents who had a premature baby were enrolled in First Steps.\n\n\n60/278\n## [1] 0.2158273\n\nUsing formal probability notation, we can write\n\n\n\\(P(\\text{First Steps})\\) = .1612\n\n\n\n\n\\(P(\\text{Preterm} | \\text{First Steps})\\) = .1488\n\n\n\n\n\\(P(\\text{First Steps} | \\text{Preterm})\\) = .2158\n\n\n\n\n\n\n\\(P(\\text{Preterm} | \\text{Not in First Steps})\\) = 218 / (218 + 1879) = 0.103958\n\n\n\n\n\\[\n\\frac{(\\text{Preterm} | \\text{First Steps})}{P(\\text{Preterm} | \\text{Not in First Steps})} = .1488 / 0.103958 = 1.43\n\\]\n\nParents in this study in the First Steps program are 1.43 times more likely to have a premature birth than those not enrolled in the First Steps program, indicating that gestational age does differ by First Steps enrollment. This implies that enrollment in the First Steps program may not be associated with better birth outcomes, as measured by gestational age.\n\nNote: However, you may argue that this is not a fair comparison, or that this summary is not what researchers were actually interested in! Ideally, we would compare birth outcomes from mothers in the First Steps program to the birth outcomes from those same mothers not in the First Steps program, to determine if the program made a positive impact. This idea hints at a sub-field of statistics called causal inference and the idea of a counterfactual (“what would have happened if…”). Take more statistics classes to learn about other methods for approaching this question!\n\n\n\n\n# Side-by-side bar chart\nfirststeps %&gt;%\n  ggplot(aes(firstep, fill = preterm)) +\n  geom_bar(position = \"dodge\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n# Stacked bar chart\nfirststeps %&gt;%\n  ggplot(aes(firstep, fill = preterm)) +\n  geom_bar() +\n  theme_classic()\n\n\n\n\n\n\n\n\n# Stacked relative frequency bar chart\nfirststeps %&gt;%\n  ggplot(aes(firstep, fill = preterm)) +\n  geom_bar(position = \"fill\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nBonus Question: Which of the above three plots allows you to directly see the conditional probabilities we calculated previously?\n\nThe stacked relative frequency bar chart!",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-2-exploring-first-steps-enrollment-and-low-birthweights-1",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-2-exploring-first-steps-enrollment-and-low-birthweights-1",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 2: Exploring First Steps enrollment and Low birthweights",
    "text": "Exercise 2: Exploring First Steps enrollment and Low birthweights\n\n\n\n\n# 2x2 Table: low_bwt vs. First Steps\nfirststeps %&gt;%\n  count(low_bwt, firstep)\n## # A tibble: 4 × 3\n##   low_bwt firstep     n\n##   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n## 1 low           0   102\n## 2 low           1    25\n## 3 not low       0  1995\n## 4 not low       1   378\n\n\n\n\n\n\\(P(\\text{Low birth weight} | \\text{First Steps})\\) = 25 / (25 + 378) = 0.062\n\n\n\\(P(\\text{Normal birth weight} | \\text{First Steps})\\) = 378 / (25 + 378) = 0.938\n\n\n\\(P(\\text{Low birth weight} | \\text{Not in First Steps})\\) = 102 / (102 + 1995) = 0.049\n\n\n\\(P(\\text{Normal birth weight} | \\text{Not in First Steps})\\) = 1995 / (102 + 1995) = 0.951\n\n\n\n\n\n\\(Odds(\\text{Low birth weight} | \\text{First Steps})\\) = 0.062 / (1 - 0.062) = 0.06609808\n\n\n\\(Odds(\\text{Normal birth weight} | \\text{First Steps})\\) = 0.938 / (1 - 0.938) = 15.12903\n\n\n\\(Odds(\\text{Low birth weight} | \\text{Not in First Steps})\\) = 0.049 / (1 - 0.049) = 0.05152471\n\n\n\\(Odds(\\text{Normal birth weight} | \\text{Not in First Steps})\\) = 0.951 / (1 - 0.951) = 19.40816\n\n\n\n\n\n0.06609808 / 0.05152471\n## [1] 1.282842\n\n\nThe odds of having a low birth weight baby are 1.28 times higher for those enrollment in First Steps compared to those not in First Steps. Just as in Exercise 1, this implies that the First Steps program may not be associated with improved birth outcomes (with the same caveats as given in the answer to 1 (h)).\nTo go along with your summary, add code below to make one of the three visualization options we tried out in Exercise 1.\n\n\n# Stacked relative frequency bar chart (with some fancy aesthetics)\nfirststeps %&gt;%\n  mutate(Birthweight = low_bwt %&gt;% str_to_title()) %&gt;%\n  ggplot(aes(firstep, fill = Birthweight)) +\n  geom_bar(position = \"fill\") +\n  theme_classic() +\n  scale_fill_viridis_d(option = \"H\") +\n  labs(x = \"First Steps\", title = \"Birthweight by First Steps Enrollment\") +\n  scale_x_continuous(breaks = c(0,1), labels = c(\"Not Enrolled\", \"Enrolled\"))",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-3-conditional-vs.-marginal-probabilities-1",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-3-conditional-vs.-marginal-probabilities-1",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 3: Conditional vs. Marginal probabilities",
    "text": "Exercise 3: Conditional vs. Marginal probabilities\nSuppose we select a person at random from the entire global population. For each of the following probabilities, which do you think is bigger? Explain your reasoning.\n\nP(lung cancer | smoker) is likely bigger, since lung cancer is more rare in the general population than it is among smokers.\nP(likes McDonald’s) is likely bigger, since vegetarians don’t likely like McDonald’s very much (few options that they can eat).\nP(smart | Mac grad) is likely bigger, because there are very few Mac grads relative to the global population. Lots of people are smart, few are Mac grads.",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-4-probability-practice-1",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-4-probability-practice-1",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 4: Probability practice",
    "text": "Exercise 4: Probability practice\nLet’s explore whether birth weight of a baby varies by whether or not it was the first child that a mother had, and whether this relationship differs by First Steps enrollment. We make a table below:\n\nfirststeps %&gt;%\n  count(firstchild, low_bwt, firstep)\n## # A tibble: 8 × 4\n##   firstchild low_bwt firstep     n\n##   &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n## 1 No         low           0    43\n## 2 No         low           1    13\n## 3 No         not low       0  1080\n## 4 No         not low       1   198\n## 5 Yes        low           0    59\n## 6 Yes        low           1    12\n## 7 Yes        not low       0   915\n## 8 Yes        not low       1   180\n\n\nWhat is the probability that a mother enrolled in First steps who is having their first child, has a baby who is born at a low birthweight? Calculate your answer, and write it using formal probability notation.\n\n\nP( | , ) = 12 / (12 + 180) = 0.0625\n\n\nWhat is the probability that a mother not enrolled in First steps who is having their first child, has a baby who is born at a low birthweight? Calculate your answer, and write it using formal probability notation.\n\n\nP( | , ) = 59 / (59 + 915) = 0.06057495\n\n\nWhat is the probability that a mother’s first child has a low birthweight? Calculate your answer, and write it using formal probability notation.\n\n\nP( | ) = (59 + 12) / (59 + 12 + 915 + 180) = 0.06089194\n\n\nHow many times more likely is a child to be born at a low birthweight, comparing children who are the first born to those not first born?\n\n\nP( | ) / P( | ) = ((59 + 12) / (59 + 12 + 915 + 180)) / ((43 + 13) / (43 + 13 + 1080 + 198)) = 1.450533",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-1-exploring-age-1",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-1-exploring-age-1",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 1: Exploring age",
    "text": "Exercise 1: Exploring age\nThe boxplot doesn’t clearly indicate a difference in the age distributions across survivors and non-survivors, but we do notice from the density plot that there is a greater density of younger passengers among the survivors. We also see from the last plot that younger passengers tend to have a higher survival chance.\n\n# Create a boxplot\nggplot(titanic, aes(x = factor(Survived), y = Age)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n# Can flip the boxplot on its side too\nggplot(titanic, aes(y = factor(Survived), x = Age)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n# Create a density plot\nggplot(titanic, aes(x = Age, color = factor(Survived))) +\n    geom_density()\n\n\n\n\n\n\n\n\n# Use the code below to create a plot of the fraction who survived at each age\ntitanic_summ &lt;- titanic %&gt;% \n    group_by(Age) %&gt;%\n    summarize(frac_survived = mean(Survived))\n\nggplot(titanic_summ, aes(x = Age, y = frac_survived)) +\n    geom_point() +\n    geom_smooth(se = FALSE)",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-2-exploring-sex-and-ticket-class-1",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-2-exploring-sex-and-ticket-class-1",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 2: Exploring sex and ticket class",
    "text": "Exercise 2: Exploring sex and ticket class\n\nFemales were more likely to survive than males.\n1st class was most likely to survive, followed by 2nd then 3rd class.\n\n\n# Standard bar plots\nggplot(titanic, aes(x = Sex, fill = factor(Survived))) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nggplot(titanic, aes(x = PClass, fill = factor(Survived))) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n# Mosaic plots\nggplot(data = titanic %&gt;% mutate(Survived = as.factor(Survived))) +\n    geom_mosaic(aes(x = product(Sex), fill = Survived))\n\n\n\n\n\n\n\n\nggplot(data = titanic %&gt;% mutate(Survived = as.factor(Survived))) +\n    geom_mosaic(aes(x = product(PClass), fill = Survived))",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-3-linear-regression-model-1",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-3-linear-regression-model-1",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 3: Linear regression model",
    "text": "Exercise 3: Linear regression model\n\ntitanic %&gt;% \n    count(PClass, Survived)\n## # A tibble: 7 × 3\n##   PClass Survived     n\n##   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n## 1 1st           0   129\n## 2 1st           1   193\n## 3 2nd           0   160\n## 4 2nd           1   119\n## 5 3rd           0   573\n## 6 3rd           1   138\n## 7 &lt;NA&gt;          0     1\n\n\n\n\n\n\n\nClass\nDied\nSurvived\nTotal\n\n\n\n\n1st Class\n129\n193\n322\n\n\n2nd Class\n160\n119\n279\n\n\n3rd Class\n573\n138\n711\n\n\nTotal\n862\n450\n1312\n\n\n\n\n\nthe probability of surviving among 1st class passengers: 193/322 = 0.599\nthe probability of surviving among 2nd class passengers: 119/279 = 0.427\nthe probability of surviving among 3rd class passengers: 138/711 = 0.194\nthe difference in the probability of surviving, comparing 2nd class passengers to 1st class passengers (i.e., how much lower is the probability of 2nd class passengers as compared to 1st class passengers?): 119/279 - 193/322 = -0.173\nthe difference in the probability of surviving, comparing 3rd class passengers to 1st class passengers (i.e., how much lower is the probability of 3rd class passengers as compared to 1st class passengers?): 138/711 - 193/322 = -0.405\n\nThis model can be written as: \\(E[Survived | PClass] = \\beta_0 + \\beta_1 PClass2nd + \\beta_2 PClass3rd\\).\n\nIn the context of a binary variable, the expected value/average is the same as the probability that the variable equals one. To see an example of this, calculate the average of this list of 0’s and 1’s: (0,0,1,1,0,1,0,1). Now calculate the proportion of 1’s. What do you notice?\nThis means that we can also write this model as follows: \\(P[Survived = 1 | PClass] = \\beta_0 + \\beta_1 PClass2nd + \\beta_2 PClass3rd\\)\n\n\n\nlin_mod &lt;- lm(Survived ~ PClass, data = titanic)\nsummary(lin_mod)\n## \n## Call:\n## lm(formula = Survived ~ PClass, data = titanic)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.5994 -0.1941 -0.1941  0.4006  0.8059 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.59938    0.02468  24.284  &lt; 2e-16 ***\n## PClass2nd   -0.17286    0.03623  -4.772 2.03e-06 ***\n## PClass3rd   -0.40529    0.02975 -13.623  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4429 on 1309 degrees of freedom\n##   (1 observation deleted due to missingness)\n## Multiple R-squared:  0.1315, Adjusted R-squared:  0.1302 \n## F-statistic: 99.09 on 2 and 1309 DF,  p-value: &lt; 2.2e-16\n\n\nThe coefficient estimates are the differences in probability from part b!\n\n(Intercept): the estimated probability of survival for passengers in 1st class is 0.599 (59.9%)\nPClass2nd: the difference in the estimated probability of survival comparing passengers in 1st class to passengers in 2nd class is 0.173 (17.3%), where passengers in 1st class have the higher estimated survival probability\n\nOR… comparing passengers in 1st class to passengers in 2nd class, the difference in the proportion of passengers that survived is 0.173 (17.3%), with 1st class having a higher proportion of passengers that survived\nOR… the probability of survival is 17.3% lower among passengers in 2nd class than it is among passengers in 1st class\n\nPClass3rd: the difference in the estimated probability of survival comparing passengers in 1st class to passengers in 3rd class is 0.405 (40.5%), where passengers in 1st class have the higher estimated survival probability",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-4-logistic-regression-model-categorical-predictor-1",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-4-logistic-regression-model-categorical-predictor-1",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 4: Logistic regression model (categorical predictor)",
    "text": "Exercise 4: Logistic regression model (categorical predictor)\n\n\nthe odds of surviving among 1st class passengers: 193/129 = 1.496\nthe odds of surviving among 2nd class passengers: 119/160 = 0.744\nthe odds of surviving among 3rd class passengers: 138/573 = 0.241\nthe ratio of the odds of surviving, comparing 2nd class passengers to 1st class passengers (i.e., how many times higher/lower is the odds of survival among 2nd class passengers as compared to 1st class passengers?): (119/160)/(193/129) = 0.497\nthe ratio of the odds of surviving, comparing 3rd class passengers to 1st class passengers: (138/573)/(193/129) = 0.161\n\n\\(\\log(Odds[Survived = 1 | PClass]) = \\beta_0 + \\beta_1 PClass2nd + \\beta_2 PClass3rd\\)\n\n\nlog_mod &lt;- glm(Survived ~ PClass, data = titanic, family = \"binomial\")\n\n# These logistic coefficient estimates are NOT exponentiated\ncoef(summary(log_mod))\n##               Estimate Std. Error    z value     Pr(&gt;|z|)\n## (Intercept)  0.4028778  0.1137246   3.542574 3.962427e-04\n## PClass2nd   -0.6989281  0.1660923  -4.208071 2.575600e-05\n## PClass3rd   -1.8265098  0.1480705 -12.335410 5.839072e-35\n\n\n# Calculations for exponentiating coefficients\nexp(0.4028778)\n## [1] 1.496124\nexp(-0.6989281)\n## [1] 0.4971179\nexp(-1.8265098)\n## [1] 0.1609744\n\n\nThese exponentiated coefficient estimates compare to your the odds and odds ratios in part a!\n\nexp(Intercept): the estimated odds of survival among passengers in first class is 1.496 (i.e., passengers in first class are 1.496 times more likely to survive than they are to die)\nexp(PClass2nd): we estimate that the odds of survival for passengers in 2nd class are only 0.50 times as high as the odds of survival among passengers in 1st class (i.e., the odds of survival are 2 times higher among passengers in 1st class than they are among passengers in 2nd class)\nexp(PClass3rd): we estimate that the odds of survival for passengers in 3rd class are only 0.16 times as high as the odds of survival among passengers in 1st class (i.e., the odds of survival are 1/0.16 = 6.21 times higher among passengers in 1st class than they are among passengers in 3rd class)",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-5-logistic-regression-model-quantitative-predictor-1",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-5-logistic-regression-model-quantitative-predictor-1",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 5: Logistic regression model (quantitative predictor)",
    "text": "Exercise 5: Logistic regression model (quantitative predictor)\n\nAfter fitting the logistic regression model below, write out the model formula using correct notation.\n\n\nlog_mod &lt;- glm(Survived ~ Age, data = titanic, family = \"binomial\")\ncoef(summary(log_mod))\n##                Estimate Std. Error    z value   Pr(&gt;|z|)\n## (Intercept) -0.08142783 0.17386170 -0.4683483 0.63953556\n## Age         -0.00879462 0.00523158 -1.6810637 0.09275054\n\n\\(\\log(Odds[Survived = 1 | Age]) = \\beta_0 + \\beta_1 Age\\)\n\nWrite an interpretation of each of the exponentiated coefficients in this logistic regression model.\n\n\nexp(Intercept): \\(exp(-0.0814)=0.92\\) –&gt; the estimated odds of survival among passengers who are 0 years old is 0.92 (i.e., passengers who are 0 years old are 0.92 times more likely to survive than they are to die–so very slightly more likely to die)\nexp(Age): \\(exp(-0.0088)=0.99\\) –&gt; For every 1-year increase in a passenger’s age, the estimated odds of survival decrease by about 1%.",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/15+16-Logistic-Regression-I.html#exercise-6-linear-vs.-logistic-modeling-1",
    "href": "activities/15+16-Logistic-Regression-I.html#exercise-6-linear-vs.-logistic-modeling-1",
    "title": "Probability, Odds, and Simple Logistic Regression",
    "section": "Exercise 6: Linear vs. logistic modeling",
    "text": "Exercise 6: Linear vs. logistic modeling\nTo highlight a key difference between linear vs. logistic modeling, consider the following linear and logistic regression models of survival with sex and age as predictors in addition to ticket class.\n\nlin_mod2 &lt;- lm(Survived ~ PClass + Sex + Age, data = titanic)\ncoef(summary(lin_mod2))\n##                 Estimate  Std. Error    t value     Pr(&gt;|t|)\n## (Intercept)  1.130522829 0.051940872  21.765573 8.158449e-82\n## PClass2nd   -0.207433817 0.039239825  -5.286308 1.637737e-07\n## PClass3rd   -0.393344488 0.037709874 -10.430809 7.001373e-24\n## Sexmale     -0.501325667 0.029419802 -17.040416 2.697807e-55\n## Age         -0.006004789 0.001105949  -5.429536 7.633977e-08\n\nlog_mod2 &lt;- glm(Survived ~ PClass + Sex + Age, data = titanic, family = \"binomial\")\ncoef(summary(log_mod2))\n##                Estimate  Std. Error    z value     Pr(&gt;|z|)\n## (Intercept)  3.75966210 0.397567324   9.456668 3.179129e-21\n## PClass2nd   -1.29196240 0.260075781  -4.967638 6.777324e-07\n## PClass3rd   -2.52141915 0.276656805  -9.113888 7.948131e-20\n## Sexmale     -2.63135683 0.201505379 -13.058494 5.684093e-39\n## Age         -0.03917681 0.007616218  -5.143868 2.691392e-07\n\n\n\n\n\n## predict for Rose\n## (by hand)\n1.130523 + (-0.207434)*0 + (-0.393344)*0 + (-0.501326)*0 + (-0.006005)*17\n## [1] 1.028438\n\n## (using predict)\npredict(lin_mod2, newdata = data.frame(PClass = \"1st\", Sex = \"female\", Age = 17))\n##        1 \n## 1.028441\n\n## predict for Jack\n## (by hand)\n1.130523 + (-0.207434)*0 + (-0.393344)*1 + (-0.501326)*1 + (-0.006005)*20\n## [1] 0.115753\n\n## (using predict)\npredict(lin_mod2, newdata = data.frame(PClass = \"3rd\", Sex = \"male\", Age = 20))\n##         1 \n## 0.1157569\n\n\n\n\n\n## predict for Rose\n## (by hand)\nlog_odds_rose &lt;- 3.75966210 + (-1.29196240)*0 + (-2.52141915)*0 + (-2.63135683)*0 + (-0.03917681)*17\nodds_rose &lt;- exp(log_odds_rose)\nodds_rose/(1+odds_rose)\n## [1] 0.9566303\n\n## (using predict)\npredict(log_mod2, newdata = data.frame(PClass = \"1st\", Sex = \"female\", Age = 17), type = \"response\")\n##         1 \n## 0.9566303\n\n## predict for Jack\n## (by hand)\nlog_odds_jack &lt;- 3.75966210 + (-1.29196240)*0 + (-2.52141915)*1 + (-2.63135683)*1 + (-0.03917681)*20\nodds_jack &lt;- exp(log_odds_jack)\nodds_jack/(1+odds_jack)\n## [1] 0.101867\n\n## (using predict)\npredict(log_mod2, newdata = data.frame(PClass = \"3rd\", Sex = \"male\", Age = 20), type = \"response\")\n##        1 \n## 0.101867\n\n\nOur linear model predicted that Rose’s probability of survival was over 100% (which doesn’t make sense). The predictions for Jack are fairly similar: 10.2% based on our logistic model and 11.6% based on our linear model.",
    "crumbs": [
      "Probability, Odds, and Simple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html",
    "href": "activities/11+12-mlr-interaction.html",
    "title": "Multiple Linear Regression - Interaction",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#learning-goals",
    "href": "activities/11+12-mlr-interaction.html#learning-goals",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nDescribe when it would be useful to include an interaction term to a model\nWrite a model formula for an interaction model\nInterpret the coefficients in an interaction model in the data context or boxplots\nVisualize interactions between categorical and quantitative predictors using scatterplots and side-by-side\nCritically think through whether an interaction term makes sense, or should be included in a multiple linear regression model\nWrite a model formula for a multiple linear regression model with an interaction term between two quantitative predictors, two categorical predictors, or one quantitative and one categorical predictor\nInterpret the intercept and slope coefficients in a multiple linear regression model with an interaction term",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#readings-and-videos",
    "href": "activities/11+12-mlr-interaction.html#readings-and-videos",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Readings and videos",
    "text": "Readings and videos\nChoose either the reading or the videos to go through before class.\n\nReading: Section 3.9.3 in the STAT 155 Notes\nVideo:\n\nInteraction variables\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder.",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#class-notes-1",
    "href": "activities/11+12-mlr-interaction.html#class-notes-1",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Class Notes 1",
    "text": "Class Notes 1\nHold on! We sped ahead too quickly. It’s important to visualize our data thoroughly first. Let’s add industry to our original scatterplot. What do you notice about the lines of best fit for these two industries?\n\nggplot(cps_sub, aes(x = education, y = wage, color = industry)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nWhat do you notice about what our model produces? How do you think our multiple linear regression model is limited? How might we try to fix this?\nIn our causal diagram, both years of education and industry affect wages, and one way to capture this is with our model in wage_mod_2:\n\\(E[\\text{wage} \\mid \\text{education}, \\text{industry}) = \\beta_0 + \\beta_1 \\text{education} + \\beta_2 \\text{industrytransportation}\\)\nSome other ways to capture how wages are affected by years of education and industry could look like this:\n\n\\(\\beta_0 + \\beta_1 \\text{education} + \\beta_2 \\text{industrytransportation} + \\beta_3 \\text{education}^2\\)\n\\(\\beta_0 + \\beta_1 \\text{education} + \\beta_2 \\text{industrytransportation} + \\beta_3 \\log(\\text{education})\\)\n\\(\\beta_0 + \\beta_1 \\text{education} + \\beta_2 \\text{industrytransportation} + \\beta_3 \\text{education}*\\text{industrytransportation}\\)\n\nThat last type of model is called an interaction model. A general interaction model formula looks like this:\n\\(E[Y \\mid X_1, X_2) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1*X_2\\)\nThe outcome \\(Y\\) depends on \\(X_1\\) and \\(X_2\\) with the usual multiple linear regression part: \\(\\beta_1 X_1 + \\beta_2 X_2\\). But it also includes an interaction term \\(\\beta_3 X_1*X_2\\).\nLet’s fit an interaction model for our cps_sub data and explore what relationships our model estimates.\n\n# Fit an interaction model\n# NEW SYNTAX: Note the * instead of +\nwage_mod_3 &lt;- lm(wage ~ education * industry, cps_sub)\n\n# Visualize the relationships from the interaction model\nggplot(cps_sub, aes(y = wage, x = education, color = industry)) + \n    geom_line(aes(y = wage_mod_3$fitted.values))\n\n\n\n\n\n\n\n\nHow does our new interaction model compare to our previous one?\nThis is a more complex new model! Let’s explore what is going on mathematically by examining the overall model formula and how we can use it to get model formulas for each industry.\n\n# View coefficient estimates\ncoef(summary(wage_mod_3))\n##                                    Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)                      -65590.606   7611.498 -8.617306 9.218848e-18\n## education                          8678.274    478.344 18.142326 3.646579e-71\n## industrytransportation            90232.230  20117.859  4.485181 7.457878e-06\n## education:industrytransportation  -7580.228   1568.831 -4.831768 1.396075e-06",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#class-notes-2",
    "href": "activities/11+12-mlr-interaction.html#class-notes-2",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Class Notes 2",
    "text": "Class Notes 2\nModel formulas:\nE[wage | education, industry] = -65590.606 + 8678.274 education + 90232.230 transportation - 7580.228 education * transportation\nBroken down by industry:\nManagement:\nE[wage | education, industry = management] = -65590.606 + 8678.274 education\nTransportation:\nE[wage | education, industry = transportation] = -65590.606 + 8678.274 education + 90232.230 - 7580.228 education = (-65590.606 + 90232.230) + (8678.274 - 7580.228)education\n= 24641.62 + 1098.046education\nQuestion 1: The intercept coefficient, -65590.606, corresponds to what property of the lines?\n\nmanagement intercept\ntransportation intercept\nhow the transportation intercept compares to the management intercept\n\nQuestion 2: The transportation coefficient, 90232.230, corresponds to what property of the lines?\n\nmanagement intercept\ntransportation intercept\nhow the transportation intercept compares to the management intercept\n\nQuestion 3: The education coefficient, 8678.274, corresponds to what property of the lines?\n\nmanagement slope\ntransportation slope\nhow the transportation slope compares to the management slope\n\nQuestion 4: The interaction coefficient, -7580.228, corresponds to what property of the lines?\n\nmanagement slope\ntransportation slope\nhow the transportation slope compares to the management slope",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#exercise-1-wages-across-all-industries",
    "href": "activities/11+12-mlr-interaction.html#exercise-1-wages-across-all-industries",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Exercise 1: Wages across all industries",
    "text": "Exercise 1: Wages across all industries\nThe plot below illustrates the relationship between wage and education for all of the industries in our cps dataset.\n\n# Plot\nggplot(cps, aes(y = wage, x = education, color = industry)) + \n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\nWhat about this plot indicates that it would be a good idea to fit an interaction model?\nWhat industry will R use as the reference category?\nFit a model that includes an interaction term between education and industry.\n\n\n# Fit an interaction model called wage_model\nwage_model &lt;- lm(wage ~ education*industry, data = cps)\n\n\n# Display summarized model output\ncoef(summary(wage_model))\n##                                               Estimate Std. Error     t value\n## (Intercept)                                31475.87521  22370.504  1.40702574\n## education                                     61.95396   2039.257  0.03038065\n## industryconstruction                      -14427.01189  25740.953 -0.56046923\n## industryinstallation_production           -33208.72359  25346.017 -1.31021469\n## industrymanagement                        -97066.48097  23305.235 -4.16500759\n## industryservice                           -55462.76415  23229.134 -2.38763810\n## industrytransportation                     -6834.25066  27495.549 -0.24855844\n## education:industryconstruction              2295.51232   2297.659  0.99906577\n## education:industryinstallation_production   3759.05906   2244.792  1.67456904\n## education:industrymanagement                8616.31984   2080.190  4.14208220\n## education:industryservice                   4384.72036   2092.523  2.09542317\n## education:industrytransportation            1036.09210   2409.093  0.43007562\n##                                               Pr(&gt;|t|)\n## (Intercept)                               1.594509e-01\n## education                                 9.757641e-01\n## industryconstruction                      5.751720e-01\n## industryinstallation_production           1.901533e-01\n## industrymanagement                        3.139604e-05\n## industryservice                           1.697551e-02\n## industrytransportation                    8.037075e-01\n## education:industryconstruction            3.177870e-01\n## education:industryinstallation_production 9.405013e-02\n## education:industrymanagement              3.470009e-05\n## education:industryservice                 3.615851e-02\n## education:industrytransportation          6.671499e-01\n\n[Note: The main “education” coefficient (61.95) gives the slope for the reference industry (agriculture). Each “education:industry” coefficient shows how much that slope changes for that industry. To find the total slope for an industry, add the base slope (61.95) to its interaction coefficient. The industry with the highest total slope benefits the most from education. The industry with the lowest total slope benefits the least.]\n\nIn what industry do wages increase the most per additional year of education? What is this increase?\n\nA. Management; 8616.31984 + 61.95396\n\nSimilarly, in what industry do wages increase the least per additional year of education? What is this increase? A. Agreeculture; 61.95396",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#example-thinking-beyond",
    "href": "activities/11+12-mlr-interaction.html#example-thinking-beyond",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Example: Thinking beyond",
    "text": "Example: Thinking beyond\nDo you think there are other variables (which may or may not be in our cps data) that have an interaction with industry in affecting wages? If you were to fit an interaction model, what results might you expect to find?\nIf a variable x has an interaction with the industry variable in affecting wages, then the relationship between x and wages must be different by industry. We might suspect that this could be the case for hours worked per week.\n\nggplot(cps, aes(y = wage, x = hours, color = industry)) + \n    geom_smooth(method = \"lm\", se = FALSE)",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#exercise-1-translating-scientific-questions-into-statistical-questions",
    "href": "activities/11+12-mlr-interaction.html#exercise-1-translating-scientific-questions-into-statistical-questions",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Exercise 1: Translating scientific questions into statistical questions",
    "text": "Exercise 1: Translating scientific questions into statistical questions\n\nLook at the variables we have access to in the cleaned version of the data we read into R, and consider our first research question. How might we translate this question into a statistical one, that we could answer using the data we have available?\n\nThere is no one right answer to this! Brainstorm with your group.\n\nhead(campaigns)\n## # A tibble: 6 × 5\n##   wholename         district             votes incumbent spending\n##   &lt;chr&gt;             &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n## 1 Aengus O Snodaigh Dublin South Central  5591 No          28.9  \n## 2 Aidan McMahon     Louth                  294 No           0.557\n## 3 Aidan Ryan        Limerick East           19 No           2.24 \n## 4 Aine Ni Chonaill  Dublin South Central   926 No           4.08 \n## 5 Alan Dukes        Kildare South         4967 Yes         12.1  \n## 6 Alan Shatter      Dublin South          5363 Yes         11.9\n\n\nQuestion 2 (a) is a bit more specific than Question 1. Translate this question into a statistical one that can be answered using a simple linear regression model. Write out the model statement in \\(E[Y | X] = ...\\) notation that would answer this question, and note which regression coefficient you would interpret to provide you with an answer.\n\n\\[\nE[___ | ___] = ...\n\\]\n\nQuestion 2 (b) is also specific, and builds on Question 2 (a). Translate this question into a statistical one that can be answered using a multiple linear regression model. Write out the model statement in \\(E[Y | X] = ...\\) notation that would answer this question, and note which regression coefficient you would interpret to provide you with an answer.\n\n\\[\nE[___ | ___] = ...\n\\]",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#exercise-2-visualizing-interaction",
    "href": "activities/11+12-mlr-interaction.html#exercise-2-visualizing-interaction",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Exercise 2: Visualizing Interaction",
    "text": "Exercise 2: Visualizing Interaction\n\nWrite R code to visualize the relationship between campaign spending and number of votes a candidate received. Include an aesthetic to distinguish this relationship between incumbents and challengers. Do not include lines of best fit from any statistical model on your plot at this point!\n\n\n# Visualization\ncampaigns %&gt;%\n  ggplot(aes(spending, votes, col = incumbent)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nBased on your visualization from part (a), what are your answers to research questions 2 (a) and 2 (b)? Write your answer in 2-3 sentences, describing general trends you notice, suitable for a general audience.\nAdd lines of best fit from a statistical model that includes an interaction term between incumbent status and spending to your plot from part (a), using geom_smooth. Based on your updated plot, do you think including an interaction between incumbent status and spending in a multiple linear regression model would be meaningful in this context? Why or why not?\n\n\n# Visualization with lines of best fit\ncampaigns %&gt;%\n  ggplot(aes(spending, votes, col = incumbent)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#exercise-3-fitting-and-interpreting-models-with-interaction-terms",
    "href": "activities/11+12-mlr-interaction.html#exercise-3-fitting-and-interpreting-models-with-interaction-terms",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Exercise 3: Fitting and interpreting models with interaction terms",
    "text": "Exercise 3: Fitting and interpreting models with interaction terms\n\nFit the regression model you wrote out in Exercise 1 (c). Report (do not interpret yet!) the regression coefficients below.\n\n\n# Model with interaction term\nlm(data = campaigns, votes ~ spending*incumbent)\n## \n## Call:\n## lm(formula = votes ~ spending * incumbent, data = campaigns)\n## \n## Coefficients:\n##           (Intercept)               spending           incumbentYes  \n##                 690.5                  209.7                 4813.9  \n## spending:incumbentYes  \n##                -125.9\n\n\n(Intercept):\n\n\nincumbentYes:\n\n\nspending:\n\n\nincumbentYes:spending:\n\n\nUsing the coefficient estimates from part (a), write out two separate model statements, one for incumbents and one for challengers. Combine terms (using algebra) when you can!\n\n\nFor incumbents:\n\n\\[\nE[votes | spending] = 690.5 + 4813.9 + 209.7 *spending -125.9 *spending\n\\]\n\nFor challengers:\n\n\\[\nE[votes | spending] = 690.5  + 209.7 * spending\n\\]\n\nInterpret the coefficient for incumbent in your interaction model, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases. Is this coefficient scientifically meaningful?\nWhen interpreting an interaction coefficient where one of the variables interacting is quantitative and one is categorical, it is often convenient to do so in separate sentences: interpret the slope for each category separately!\n\nInterpret the coefficient for the interaction term in your model, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\n\nBased on your interpretation in part (d), and the visualization you made including lines of best fit, do you think that including an interaction term for incumbent status and spending is meaningful, when predicting number of votes? Explain why or why not.",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#exercise-4-interactions-between-two-categorical-variables",
    "href": "activities/11+12-mlr-interaction.html#exercise-4-interactions-between-two-categorical-variables",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Exercise 4: Interactions between two categorical variables",
    "text": "Exercise 4: Interactions between two categorical variables\nLet’s return to our data on bike ridership. Suppose we are interested in the relationship between daily ridership (our response variable) and whether a user is a casual or registered rider and whether the day falls on a weekend. First, we need to create a binary variable indicating whether a user is a casual or registered rider.\n\n# Creating user variable, don't worry about syntax!\nnew_bikes &lt;- bikes %&gt;%\n  dplyr::select(riders_casual, riders_registered, weekend, temp_actual) %&gt;%\n  pivot_longer(cols = riders_casual:riders_registered, names_to = \"user\",\n               names_prefix = \"riders_\", values_to = \"rides\") %&gt;%\n  mutate(weekend = factor(weekend))\nhead(new_bikes)\n## # A tibble: 6 × 4\n##   weekend temp_actual user       rides\n##   &lt;fct&gt;         &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n## 1 TRUE           57.4 casual       331\n## 2 TRUE           57.4 registered   654\n## 3 TRUE           58.8 casual       131\n## 4 TRUE           58.8 registered   670\n## 5 FALSE          46.5 casual       120\n## 6 FALSE          46.5 registered  1229\n\n\nFor each of our three relevant variables, weekend, user, and rides, classify them as quantitative or categorical.\n\n\nweekend:\n\n\nuser:\n\n\nrides:\n\n\nMake an appropriate visualization to explore the relationship between these three variables.\n\n\n# Visualization\nnew_bikes %&gt;%\n  ggplot(aes(y = rides, user, fill = weekend)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nIs the relationship between ridership and weekend status the same for both registered and casual users? Explain why or why not, referencing the visualization you made in part (b).\nTo reflect what you observed in your visualization, fit a multiple linear regression model with an interaction term between weekend and user in our model of ridership.\n\n\n# Multiple linear regression model\nlm(data = new_bikes, rides ~ user * weekend)\n## \n## Call:\n## lm(formula = rides ~ user * weekend, data = new_bikes)\n## \n## Coefficients:\n##                (Intercept)              userregistered  \n##                      625.0                      3300.5  \n##                weekendTRUE  userregistered:weekendTRUE  \n##                      776.7                     -1714.4\n\n\nInterpret the interaction term from your model, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases. Just as in Exercise 3, you may find it useful to first write out multiple model statements for different categories defined by one of your categorical variables, and proceed from there!",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#class-notes-3",
    "href": "activities/11+12-mlr-interaction.html#class-notes-3",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Class Notes 3",
    "text": "Class Notes 3",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#exercise-5-interactions-between-two-quantitative-variables-challenge-question",
    "href": "activities/11+12-mlr-interaction.html#exercise-5-interactions-between-two-quantitative-variables-challenge-question",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Exercise 5: Interactions between two quantitative variables [Challenge Question]",
    "text": "Exercise 5: Interactions between two quantitative variables [Challenge Question]\nHere we’ll explore the relationship between price, milage, and age of a used car. Below is a scatterplot of mileage vs. price, colored by age:\n\ncars %&gt;% \n  ggplot(aes(x = milage, y = price, col = age)) +\n  geom_point(alpha = 0.5) + # make the points less opaque\n  scale_color_viridis_c(option = \"H\") + # a fun, colorblind-friendly palette!\n  theme_classic() # removes the gray background and grid\n\n\n\n\n\n\n\n\nIt’s a little difficult to tell what exactly is going on here. In particular, does the relationship between mileage and price vary with age of a used car? Let’s try adding some fitted lines for cars of different ages.\n\n# Ignore where the numbers in geom_abline() came from for now... we'll get there\ncars %&gt;% \n  ggplot(aes(x = milage, y = price, col = age)) +\n  geom_point(alpha = 0.5) + \n  scale_color_viridis_c(option = \"H\") + \n  theme_classic() +\n  geom_abline(slope = -6.558e-01 + 2.431e-02, intercept = 9.096e+04 -2.665e+03, col = \"black\") +\n  geom_abline(slope = -6.558e-01 + 10 * 2.431e-02, intercept = 9.096e+04 - 10 * 2.665e+03, col = \"blue\") +\n  geom_abline(slope = -6.558e-01 + 30 * 2.431e-02, intercept = 9.096e+04 - 30 * 2.665e+03, col = \"green\") +\n  ggtitle(\"Black: Age = 1yr, Blue: Age = 10yr, Green: Age = 30yr\")\n\n\n\n\n\n\n\n\n\nChallenge question: Based on the fitted lines in the plot above, anticipate what the signs (positive or negative) of the coefficients in the following interaction model will be:\n\n\\[\nE[price | age, milage] = \\beta_0 + \\beta_1 milage + \\beta_2 age + \\beta_3 milage:age\n\\] * \\(\\beta_0\\): Put your response here…\n\n\\(\\beta_1\\): Put your response here…\n\\(\\beta_2\\): Put your response here…\n\\(\\beta_3\\): Put your response here…\n\n\nFit a multiple linear regression model with an interaction term between milage and age in our model of used car price.\n\n\n# Multiple linear regression model\n\n\n\n# ... now do you see where the numbers in geom_abline() came from?\n\nAs before, we could choose distinct ages, and interpret the relationship between mileage and price for each of those groups separately. However, since age is quantitative and not categorical, this doesn’t quite give us the whole picture. Instead, we want to know how the relationship between mileage and price changes for each additional year old a car is. This is what the interaction coefficient estimates, when the interaction term is between two quantitative variables!\n\nInterpret the interaction term, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#reflection",
    "href": "activities/11+12-mlr-interaction.html#reflection",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Reflection",
    "text": "Reflection\nThrough the exercises above, you practiced visualizing, fitting, and interpreting multiple linear regression models with interaction terms between combinations of categorical and quantitative variables. Think about how the fitted lines looked in situations where you think there was a meaningful interaction taking place. How do you think the fitted lines would look if there was no meaningful interaction present? Explain your reasoning.\n\nResponse: Put your response here.",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#exercise-1-wages-across-all-industries-1",
    "href": "activities/11+12-mlr-interaction.html#exercise-1-wages-across-all-industries-1",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Exercise 1: Wages across all industries",
    "text": "Exercise 1: Wages across all industries\nThe plot below illustrates the relationship between wage and education for all of the industries in our cps dataset.\n\n# Plot\nggplot(cps, aes(y = wage, x = education, color = industry)) + \n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\nThe industry-specific lines all have different slopes.\nag (first in alphabetical order)\n\n\n\n# Fit an interaction model called wage_model\nwage_model &lt;- lm(wage ~ education*industry, data = cps)\n\n# Display summarized model output\ncoef(summary(wage_model))\n##                                               Estimate Std. Error     t value\n## (Intercept)                                31475.87521  22370.504  1.40702574\n## education                                     61.95396   2039.257  0.03038065\n## industryconstruction                      -14427.01189  25740.953 -0.56046923\n## industryinstallation_production           -33208.72359  25346.017 -1.31021469\n## industrymanagement                        -97066.48097  23305.235 -4.16500759\n## industryservice                           -55462.76415  23229.134 -2.38763810\n## industrytransportation                     -6834.25066  27495.549 -0.24855844\n## education:industryconstruction              2295.51232   2297.659  0.99906577\n## education:industryinstallation_production   3759.05906   2244.792  1.67456904\n## education:industrymanagement                8616.31984   2080.190  4.14208220\n## education:industryservice                   4384.72036   2092.523  2.09542317\n## education:industrytransportation            1036.09210   2409.093  0.43007562\n##                                               Pr(&gt;|t|)\n## (Intercept)                               1.594509e-01\n## education                                 9.757641e-01\n## industryconstruction                      5.751720e-01\n## industryinstallation_production           1.901533e-01\n## industrymanagement                        3.139604e-05\n## industryservice                           1.697551e-02\n## industrytransportation                    8.037075e-01\n## education:industryconstruction            3.177870e-01\n## education:industryinstallation_production 9.405013e-02\n## education:industrymanagement              3.470009e-05\n## education:industryservice                 3.615851e-02\n## education:industrytransportation          6.671499e-01\n\n\nIn the management industry, wages increase the most per year of education. The increase is 61.95396 + 8616.31984 = $8678.274 per year. That is, every additional year of education is associated with an average increase of $8678.27 in yearly wages in the management industry.\nIn the agriculture industry, wages increase the least per year of education. The increase is $61.95 per year. That is, every additional year of education is associated with an average increase of $61.95 in yearly wages in the ag industry.",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#exercise-1-translating-scientific-questions-into-statistical-questions-1",
    "href": "activities/11+12-mlr-interaction.html#exercise-1-translating-scientific-questions-into-statistical-questions-1",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Exercise 1: Translating scientific questions into statistical questions",
    "text": "Exercise 1: Translating scientific questions into statistical questions\n\nFrom this question, the only clear variable that should be involved in our analysis/exploration is spending. We could first begin by providing numerical and visual summaries of campaign spending. We could also look at whether spending varies by district, number of votes, or incumbency status. This would give us a broad idea of how campagin spending may vary across the variables we access to in our data.\nWe can estimate the average associated increase in number of votes per additional 1,000 Euros spent, via a simple linear regression model. The model statement that allows us to answer this question is given by\n\n\\[\nE[votes | spending] = \\beta_0 + \\beta_1 spending\n\\] The regression coefficient we would interpret to answer this question is the coefficient fpr spending, which in this case is \\(\\beta_1\\).\n\nWe are interested in the how the association between average number of votes and campaign spending varies by incumbency status. The model statement that allows us to answer this question is given by\n\n\\[\nE[votes | spending, incumbent] = \\beta_0 + \\beta_1 spending + \\beta_2 incumbent + \\beta_3 spending:incumbent\n\\]\n(note that the order in which you put spending and incumbent status does not matter!)\nThe regression coefficient we would interpret to answer this question is the interaction coefficient, which in this case is \\(\\beta_3\\).",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#exercise-2-visualizing-interaction-1",
    "href": "activities/11+12-mlr-interaction.html#exercise-2-visualizing-interaction-1",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Exercise 2: Visualizing Interaction",
    "text": "Exercise 2: Visualizing Interaction\n\n\n\n\n# Visualization\ncampaigns %&gt;%\n  ggplot(aes(spending, votes, col = incumbent)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nIn general, the more a candidate spends on their campaign, the more votes they receive. Incumbents appear to spend more than challengers on their campaigns, typically. The impact of spending on votes appears to be greater for challengers than for incumbents, in that more spending may lead to even more votes for challengers, than it would for incumbents.\nI think including an interaction term between incumbent status and spending would be meaningful, since the relationship between spending and votes does seem to vary by incumbent status. In particular, note that the lines on the visualization are not parallel. Parallel lines imply that there is no interaction present, so the further the lines are from parallel, the more intense (in some sense) the interaction term.\n\n\n# Visualization with lines of best fit\ncampaigns %&gt;%\n  ggplot(aes(spending, votes, col = incumbent)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#exercise-3-fitting-and-interpreting-models-with-interaction-terms-1",
    "href": "activities/11+12-mlr-interaction.html#exercise-3-fitting-and-interpreting-models-with-interaction-terms-1",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Exercise 3: Fitting and interpreting models with interaction terms",
    "text": "Exercise 3: Fitting and interpreting models with interaction terms\n\n\n\n\n# Model with interaction term\nlm(data = campaigns, votes ~ spending*incumbent)\n## \n## Call:\n## lm(formula = votes ~ spending * incumbent, data = campaigns)\n## \n## Coefficients:\n##           (Intercept)               spending           incumbentYes  \n##                 690.5                  209.7                 4813.9  \n## spending:incumbentYes  \n##                -125.9\n\n\n(Intercept): 690.5\n\n\nincumbentYes: 4813.9\n\n\nspending: 209.7\n\n\nincumbentYes:spending: -125.9\n\n\n\n\n\nFor incumbents:\n\n\\[\nE[votes | spending] = 690 + 4813.9 + 209.7 * spending - 125.9 * spending = 5503.9 + 83.8 * spending\n\\]\n\nFor challengers:\n\n\\[\nE[votes | spending] = 690.5 + 209.7 * spending\n\\]\n\nOn average, we expect the difference in number of votes between incumbents and challengers to be 4813.9, for campaigns where no money is spent. This is likely not a scientifically meaningful estimate since there are very few campaigns where no money is spent. However, such campaigns do exist, so I would say this one could be meaningful in certain contexts, if not broadly!\nOn average, we expect an increase in spending by 1,000 euros to be associated with an increase in number of votes by 210, for challengers. On average, we expect an increase in spending by 1,000 euros to be associated with an increase in number of votes by 84, for incumbents.\nI think the interaction term is meaningful when predicting number of votes, since 84 and 210 are relatively different numbers! The interaction term gives us the additional information that spending has less of an effect on number of votes for incumbents than it does for challengers, which is particularly meaningful if you are a campaign manager!",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#exercise-4-interactions-between-two-categorical-variables-1",
    "href": "activities/11+12-mlr-interaction.html#exercise-4-interactions-between-two-categorical-variables-1",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Exercise 4: Interactions between two categorical variables",
    "text": "Exercise 4: Interactions between two categorical variables\n\n# Creating user variable, don't worry about syntax!\nnew_bikes &lt;- bikes %&gt;%\n  dplyr::select(riders_casual, riders_registered, weekend, temp_actual) %&gt;%\n  pivot_longer(cols = riders_casual:riders_registered, names_to = \"user\",\n               names_prefix = \"riders_\", values_to = \"rides\") %&gt;%\n  mutate(weekend = factor(weekend))\n\n\n\n\n\nweekend: categorical (binary)\n\n\nuser: categorical (binary)\n\n\nrides: quantitative\n\n\n\n\n\n# Visualization\nnew_bikes %&gt;%\n  ggplot(aes(y = rides, user, fill = weekend)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nThe relationship between ridership and weekend status does not appear to be the same for registered and casual users. Specifically, casual users have higher median riders on weekends, whereas the opposite is true for registered users.\n\n\n\n# Multiple linear regression model\nlm(data = new_bikes, rides ~ user * weekend)\n## \n## Call:\n## lm(formula = rides ~ user * weekend, data = new_bikes)\n## \n## Coefficients:\n##                (Intercept)              userregistered  \n##                      625.0                      3300.5  \n##                weekendTRUE  userregistered:weekendTRUE  \n##                      776.7                     -1714.4\n\n\nOn average, we expect there to be 777 more rides on weekends compared to non-weekends, for casual riders. On average, we expect there to be 938 (776.7 - 1714.4, rounded) less rides on weekends compared to non-weekends, for registered riders.\n\nNote: There are lots of ways you could correctly interpret the interaction term here! You could do it one sentence, you could do it in four (one for each unique group defined by the two categorical variables), or you could compare users and registered riders for weekends, and then separately for non-weekends! All are valid options.",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/11+12-mlr-interaction.html#exercise-5-interactions-between-two-quantitative-variables",
    "href": "activities/11+12-mlr-interaction.html#exercise-5-interactions-between-two-quantitative-variables",
    "title": "Multiple Linear Regression - Interaction",
    "section": "Exercise 5: Interactions between two quantitative variables",
    "text": "Exercise 5: Interactions between two quantitative variables\nHere we’ll explore the relationship between price, milage, and age of a used car. Below is a scatterplot of mileage vs. price, colored by age:\n\ncars %&gt;% \n  ggplot(aes(x = milage, y = price, col = age)) +\n  geom_point(alpha = 0.5) + # make the points less opaque\n  scale_color_viridis_c(option = \"H\") + # a fun, colorblind-friendly palette!\n  theme_classic() # removes the gray background and grid\n\n\n\n\n\n\n\n\nIt’s a little difficult to tell what exactly is going on here. In particular, does the relationship between mileage and price vary with age of a used car? Let’s try adding some fitted lines for cars of different ages.\n\n# Ignore where the numbers in geom_abline() came from for now... we'll get there\ncars %&gt;% \n  ggplot(aes(x = milage, y = price, col = age)) +\n  geom_point(alpha = 0.5) + \n  scale_color_viridis_c(option = \"H\") + \n  theme_classic() +\n  geom_abline(slope = -6.558e-01 + 2.431e-02, intercept = 9.096e+04 -2.665e+03, col = \"black\") +\n  geom_abline(slope = -6.558e-01 + 10 * 2.431e-02, intercept = 9.096e+04 - 10 * 2.665e+03, col = \"blue\") +\n  geom_abline(slope = -6.558e-01 + 30 * 2.431e-02, intercept = 9.096e+04 - 30 * 2.665e+03, col = \"green\") +\n  ggtitle(\"Black: Age = 1yr, Blue: Age = 10yr, Green: Age = 30yr\")\n\n\n\n\n\n\n\n\n\n\n\n\\[\nE[price | age, milage] = \\beta_0 + \\beta_1 milage + \\beta_2 age + \\beta_3 milage:age\n\\] * \\(\\beta_0\\): positive, since the intercept is the average price for a car with zero miles that is brand new.\n\n\\(\\beta_1\\): negative, since the more miles a new car has, the cheaper it should be\n\\(\\beta_2\\): negative, since the intercept of the lines seems to decrease with age (black -&gt; blue -&gt; green)\n\\(\\beta_3\\): positive, since the slope of the lines seems to increase with age (black -&gt; blue -&gt; green)\n\n\n\n\n\n# Multiple linear regression model\n\nlm(data = cars, price ~ milage * age)\n## \n## Call:\n## lm(formula = price ~ milage * age, data = cars)\n## \n## Coefficients:\n## (Intercept)       milage          age   milage:age  \n##   9.096e+04   -6.558e-01   -2.665e+03    2.431e-02\n\n# ... now do you see where the numbers in geom_abline() came from?\n\nAs before, we could choose distinct ages, and interpret the relationship between mileage and price for each of those groups separately. However, since age is quantitative and not categorical, this doesn’t quite give us the whole picture. Instead, we want to know how the relationship between mileage and price changes for each additional year old a car is. This is what the interaction coefficient estimates, when the interaction term is between two quantitative variables!\n\nOn average, we expect that an increase in mileage by 1 mile is associated with an additional increase in price by $0.0243 for each additional year old the car is.",
    "crumbs": [
      "Multiple Linear Regression - Interaction"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html",
    "href": "activities/09-mlr-principles.html",
    "title": "Multiple Linear Regression - Principles",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#learning-goals",
    "href": "activities/09-mlr-principles.html#learning-goals",
    "title": "Multiple Linear Regression - Principles",
    "section": "Learning goals",
    "text": "Learning goals\nWorking with multiple predictors in our plots and models can get complicated!\nThere are no recipes for this process.\nBUT there are some guiding principles that assist in long-term retention, deeper understanding, and the ability to generalize our tools in new settings.\nBy the end of this lesson, you should be familiar with some general principles for…\n\nincorporating additional quantitative or categorical predictors in a visualization\nhow additional quantitative or categorical predictors impact the physical representation of a model\ninterpreting quantitative or categorical coefficients in a multiple regression model",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#readings-and-videos",
    "href": "activities/09-mlr-principles.html#readings-and-videos",
    "title": "Multiple Linear Regression - Principles",
    "section": "Readings and videos",
    "text": "Readings and videos\nPlease watch the following video before class.\n\nMultivariate modeling principles (slides)\nInterpreting multivariate models (Part I) (slides)\n\nPlease watch the following video that include extra examples after class.\n\nInterpreting multivariate models (Part II (slides)",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-1-review-visualization",
    "href": "activities/09-mlr-principles.html#exercise-1-review-visualization",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 1: Review visualization",
    "text": "Exercise 1: Review visualization\nLet’s build a model of rides by windspeed (quantitative) and weekend status (categorical).\nPlot & describe, in words, the relationship between these 3 variables.\n\nbikes %&gt;% \n  ggplot(aes(y = rides, x = windspeed, color = weekend)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#live-note-taking",
    "href": "activities/09-mlr-principles.html#live-note-taking",
    "title": "Multiple Linear Regression - Principles",
    "section": "Live-Note Taking!",
    "text": "Live-Note Taking!",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-2-review-model",
    "href": "activities/09-mlr-principles.html#exercise-2-review-model",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 2: Review model",
    "text": "Exercise 2: Review model\nLet’s build the model. Run the following code:\n\nbike_model_1 &lt;- lm(rides ~ windspeed + weekend, data = bikes)\ncoef(summary(bike_model_1))\n##               Estimate Std. Error   t value      Pr(&gt;|t|)\n## (Intercept) 4738.38053  147.53653 32.116659 1.208405e-141\n## windspeed    -63.97072   10.45274 -6.119997  1.528443e-09\n## weekendTRUE -925.15701  119.86330 -7.718434  3.891082e-14\n\nThe estimated model formula is therefore:\nE[rides | windspeed, weekendTRUE] = 4738.38 - 63.97 * windspeed - 925.16 * weekendTRUE\nThis model formula is represented by 2 lines, one corresponding to weekends and the other to weekdays. Simplify the model formula above for weekdays and weekends:\nweekdays: rides = 4738.38 - 63.97 *windspeed\nweekends: rides = (4738.38- 925.16) - 63.97 *windspeed",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-3-review-coefficient-interpretation",
    "href": "activities/09-mlr-principles.html#exercise-3-review-coefficient-interpretation",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 3: Review coefficient interpretation",
    "text": "Exercise 3: Review coefficient interpretation\n\nThe intercept coefficient, 4738.38, represents the intercept of the sub-model for weekdays, the reference category. What’s its contextual interpretation?\nThe windspeed coefficient, -63.97, represents the shared slope of the weekend and weekday sub-models. What’s its contextual interpretation?\nThe weekendTRUE coefficient, -925.16, represents the change in intercept for the weekend vs weekday sub-model. What’s its contextual interpretation?",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-4-2-categorical-predictors-visualization",
    "href": "activities/09-mlr-principles.html#exercise-4-2-categorical-predictors-visualization",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 4: 2 categorical predictors – visualization",
    "text": "Exercise 4: 2 categorical predictors – visualization\nThus far, we’ve explored a couple examples of multiple regression models that have 2 predictors, 1 quantitative and 1 categorical.\nSo what happens when both predictors are categorical?!\nTo this end, let’s model rides by weekend status and season.\nThe below code plots rides vs season.\nModify this code to also include information about weekend.\nHINT: Remember the visualization principle that additional categorical predictors require some sort of grouping mechanism / mechanism that distinguishes between the 2 groups.\n\n# rides vs season\nbikes %&gt;% \n  ggplot(aes(y = rides, x = season)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n# rides vs season AND weekend\nbikes %&gt;%\n  ggplot(aes(y = rides, x = season, fill = weekend)) +\n  geom_boxplot()",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-5-follow-up",
    "href": "activities/09-mlr-principles.html#exercise-5-follow-up",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 5: follow-up",
    "text": "Exercise 5: follow-up\n\nDescribe (in words) the relationship of ridership with season & weekend status.\nA model of rides by season alone would be represented by only 4 expected outcomes, 1 for each season. Considering this and the plot above, how do you anticipate a model of rides by season and weekend status will be represented?\n\n2 lines, 1 for each weekend status\n8 lines, 1 for each possible combination of season & weekend\n2 expected outcomes, 1 for each weekend status\n8 expected outcomes, 1 for each possible combination of season & weekend",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-6-2-categorical-predictors-build-the-model",
    "href": "activities/09-mlr-principles.html#exercise-6-2-categorical-predictors-build-the-model",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 6: 2 categorical predictors – build the model",
    "text": "Exercise 6: 2 categorical predictors – build the model\nLet’s build the multiple regression model of rides vs season and weekend:\n\nbike_model_2 &lt;- lm(rides ~ weekend + season, bikes)\ncoef(summary(bike_model_2))\n##                Estimate Std. Error     t value      Pr(&gt;|t|)\n## (Intercept)   4260.4492   99.16363  42.9638294 1.384994e-201\n## weekendTRUE   -912.3324  103.23016  -8.8378473  7.298199e-18\n## seasonspring  -116.3824  132.76018  -0.8766364  3.809741e-01\n## seasonsummer   438.4424  132.06413   3.3199205  9.454177e-04\n## seasonwinter -1719.0572  133.30505 -12.8956646  2.081758e-34\n\nThus estimated model formula is given by:\nE[rides | weekend, season] = 4260.45 - 912.33 weekendTRUE - 116.38 seasonspring + 438.44 seasonsummer - 1719.06 seasonwinter\n\nUse this model to predict the ridership on the following days:\n\n\n# a fall weekday\n4260.45 - 912.33*0 - 116.38*0  + 438.44*0 - 1719.06*0\n## [1] 4260.45\n\n# a winter weekday    \n4260.45 - 912.33*0 - 116.38*0  + 438.44*0 - 1719.06*1\n## [1] 2541.39\n\n# a fall weekend day        \n4260.45 - 912.33*1 - 116.38*0  + 438.44*0 - 1719.06*0\n## [1] 3348.12\n\n# a winter weekend day\n4260.45 - 912.33*1 - 116.38*0  + 438.44*0 - 1719.06*1\n## [1] 1629.06\n\n\nWe only made 4 predictions here. How many possible predictions does this model produce? Is this consistent with your intuition in the previous exercise?",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-7-2-categorical-predictors-interpret-the-model",
    "href": "activities/09-mlr-principles.html#exercise-7-2-categorical-predictors-interpret-the-model",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 7: 2 categorical predictors – interpret the model",
    "text": "Exercise 7: 2 categorical predictors – interpret the model\nUse your above predictions and visualization to fill in the below interpretations of the model coefficients.\nHint: What is the consequence of plugging in 0 or 1 for the different weekend and season categories?\n\nInterpreting 4260: On average, we expect there to be 4260 riders on (weekdays/weekends) during the (fall/spring/summer/winter).\nInterpreting -912: On average, in any season, we expect there to be 912 (more/fewer) riders on weekends than on ___.\nInterpreting -1719: On average, on both weekdays and weekends, we expect there to be 1719 (more/fewer) riders in winter than in ___.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-8-2-quantitative-predictors-visualization",
    "href": "activities/09-mlr-principles.html#exercise-8-2-quantitative-predictors-visualization",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 8: 2 quantitative predictors – visualization",
    "text": "Exercise 8: 2 quantitative predictors – visualization\nNext, consider the relationship between rides and 2 quantitative predictors: windspeed and temp_feel. Check out the plot of this relationship below which reflects the visualization principle that quantitative variables require some sort of numerical scaling mechanism – rides and windspeed get numerical axes, and temp_feel gets a color scale:\n\nModify the code below to recreate this plot.\n\nbikes %&gt;%\n  ggplot(aes(y = rides, x = windspeed, color = temp_feel)) +\n  geom_point() #color = temp_feel\n\n\n\n\n\n\n\n\nOPTIONAL: Check out this interactive plot which allows us to explore this point cloud in 3D.\n\n# Install the \"plotly\" package in R Console First!\nlibrary(plotly)\nbikes %&gt;% \n  plot_ly(x = ~windspeed, y = ~temp_feel, z = ~rides, \n          type = \"scatter3d\", \n          mode = \"markers\",\n          marker = list(size = 5, color = ~rides, colorscale = \"Viridis\"))",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-9-follow-up",
    "href": "activities/09-mlr-principles.html#exercise-9-follow-up",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 9: follow-up",
    "text": "Exercise 9: follow-up\nDescribe (in words) the relationship of ridership with windspeed & temperature.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-10-2-quantitative-predictors-modeling",
    "href": "activities/09-mlr-principles.html#exercise-10-2-quantitative-predictors-modeling",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 10: 2 quantitative predictors – modeling",
    "text": "Exercise 10: 2 quantitative predictors – modeling\nLet’s build the multiple regression model of rides vs windspeed and temp_feel:\n\nbike_model_3 &lt;- lm(rides ~ windspeed + temp_feel, data = bikes)\ncoef(summary(bike_model_3))\n##              Estimate Std. Error     t value     Pr(&gt;|t|)\n## (Intercept) -24.06464 299.303032 -0.08040225 9.359394e-01\n## windspeed   -36.54372   9.408116 -3.88427585 1.119805e-04\n## temp_feel    55.51648   3.330739 16.66791759 4.436963e-53\n\nThus estimated model formula is\nE[rides | windspeed, temp_feel] = -24.06 - 36.54 windspeed + 55.52 temp_feel\n\nInterpret the intercept coefficient, -24.06, in context. Is this a correct interpretation?\nInterpret the windspeed coefficient, -36.54, in context.\nInterpret the temp_feel coefficient, 55.52, in context.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-11-which-is-best",
    "href": "activities/09-mlr-principles.html#exercise-11-which-is-best",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 11: Which is “best”?",
    "text": "Exercise 11: Which is “best”?\nWe’ve now observed 3 different models of ridership, each having 2 predictors. The R-squared values of these models, along with those of the simple linear regression models with each predictor alone, are summarized below.\n\n\n\nmodel\npredictors\nR-squared\n\n\n\n\nbike_model_1\nwindspeed & weekend\n0.119\n\n\nbike_model_2\nweekend & season\n0.349\n\n\nbike_model_3\nwindspeed & temp_feel\n0.310\n\n\nbike_model_4\nwindspeed\n0.047\n\n\nbike_model_5\ntemp_feel\n0.296\n\n\nbike_model_6\nweekend\n0.074\n\n\nbike_model_7\nseason\n0.279\n\n\n\n\nWhich model does the best job of explaining the variability in ridership from day to day?\nIf you could only pick one predictor, which would it be?\nWhat happens to R-squared when we add a second predictor to our model, and why does this make sense? For example, how does the R-squared for model 1 (with both windspeed and weekend) compare to those of model 4 (only windspeed) and model 6 (only weekend)?\nAre 2 predictors always better than 1? Provide evidence and explain why this makes sense.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-13-practice-1",
    "href": "activities/09-mlr-principles.html#exercise-13-practice-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 13: Practice 1",
    "text": "Exercise 13: Practice 1\nConsider the relationship of rides vs weekend and weather_cat.\n\nConstruct a visualization of this relationship.\n\nConstruct a model of this relationship.\n\nInterpret the first 3 model coefficients.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-14-practice-2",
    "href": "activities/09-mlr-principles.html#exercise-14-practice-2",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 14: Practice 2",
    "text": "Exercise 14: Practice 2\nConsider the relationship of rides vs temp_feel and humidity.\n\nConstruct a visualization of this relationship.\n\nConstruct a model of this relationship.\n\nInterpret the first 3 model coefficients.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-15-practice-3",
    "href": "activities/09-mlr-principles.html#exercise-15-practice-3",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 15: Practice 3",
    "text": "Exercise 15: Practice 3\nConsider the relationship of rides vs temp_feel and weather_cat.\n\nConstruct a visualization of this relationship.\n\nConstruct a model of this relationship.\n\nInterpret the first 3 model coefficients.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-16-challenge",
    "href": "activities/09-mlr-principles.html#exercise-16-challenge",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 16: CHALLENGE",
    "text": "Exercise 16: CHALLENGE\nWe’ve explored models with 2 predictors. What about 3 predictors?! Consider the relationship of rides vs temp_feel, humidity, AND weekend.\n\nConstruct a visualization of this relationship.\n\nConstruct a model of this relationship.\n\nInterpret each model coefficient.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-1-review-visualization-1",
    "href": "activities/09-mlr-principles.html#exercise-1-review-visualization-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 1: Review visualization",
    "text": "Exercise 1: Review visualization\n\nbikes %&gt;% \n  ggplot(aes(y = rides, x = windspeed, color = weekend)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-2-review-model-1",
    "href": "activities/09-mlr-principles.html#exercise-2-review-model-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 2: Review model",
    "text": "Exercise 2: Review model\nweekdays: rides = 4738.38 - 63.97 windspeed\nweekends: rides = 4738.38 - 63.97 windspeed - 925.16 = 3813.22 - 63.97 windspeed",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-3-review-coefficient-interpretation-1",
    "href": "activities/09-mlr-principles.html#exercise-3-review-coefficient-interpretation-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 3: Review coefficient interpretation",
    "text": "Exercise 3: Review coefficient interpretation\n\nOn average, there are 4738 riders on weekdays with 0 windspeed.\nOn both weekends and weekdays, a 1mph increase in windspeed is associated with 64 fewer riders on average.\nAt any windspeed, the average ridership is 925 lower on weekends than week days.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-4-2-categorical-predictors-visualization-1",
    "href": "activities/09-mlr-principles.html#exercise-4-2-categorical-predictors-visualization-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 4: 2 categorical predictors – visualization",
    "text": "Exercise 4: 2 categorical predictors – visualization\n\nbikes %&gt;% \n  ggplot(aes(y = rides, x = season, fill = weekend)) + \n  geom_boxplot()",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-5-follow-up-1",
    "href": "activities/09-mlr-principles.html#exercise-5-follow-up-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 5: follow-up",
    "text": "Exercise 5: follow-up\n\nIn every season, ridership tends to be lower on weekends. Across weekend status, ridership tends to be highest in summer and lowest in winter.\n8 expected outcomes",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-6-2-categorical-predictors-build-the-model-1",
    "href": "activities/09-mlr-principles.html#exercise-6-2-categorical-predictors-build-the-model-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 6: 2 categorical predictors – build the model",
    "text": "Exercise 6: 2 categorical predictors – build the model\n\n\n\n\n#fall weekday:    \n4260.45 - 912.33*0 - 116.38*0 + 438.44*0 - 1719.06*0\n## [1] 4260.45\n\n#winter weekday:\n4260.45 - 912.33*0 - 116.38*0 + 438.44*0 - 1719.06*1\n## [1] 2541.39\n\n#fall weekend:    \n4260.45 - 912.33*1 - 116.38*0 + 438.44*0 - 1719.06*0\n## [1] 3348.12\n\n#winter weekend:\n4260.45 - 912.33*1 - 116.38*0 + 438.44*0 - 1719.06*1\n## [1] 1629.06\n\n\n8: 2 weekend categories * 4 season categories",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-7-2-categorical-predictors-interpret-the-model-1",
    "href": "activities/09-mlr-principles.html#exercise-7-2-categorical-predictors-interpret-the-model-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 7: 2 categorical predictors – interpret the model",
    "text": "Exercise 7: 2 categorical predictors – interpret the model\n\nWe expect there to be, on average, 4260 riders on weekdays during the fall.\nOn average, in any season, there are 912 fewer riders on weekends than on weekdays.\nOn average, on both weekdays and weekends, we expect there to be 1719 fewer riders in winter than in fall.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-8-2-quantitative-predictors-visualization-1",
    "href": "activities/09-mlr-principles.html#exercise-8-2-quantitative-predictors-visualization-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 8: 2 quantitative predictors – visualization",
    "text": "Exercise 8: 2 quantitative predictors – visualization\n\nbikes %&gt;% \n  ggplot(aes(y = rides, x = windspeed, color = temp_feel)) + \n  geom_point()",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-9-follow-up-1",
    "href": "activities/09-mlr-principles.html#exercise-9-follow-up-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 9: follow-up",
    "text": "Exercise 9: follow-up\nRidership tends to increase with temperature (no matter the windspeed) and decrease with windspeed (no matter the temperature).",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-10-2-quantitative-predictors-modeling-1",
    "href": "activities/09-mlr-principles.html#exercise-10-2-quantitative-predictors-modeling-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 10: 2 quantitative predictors – modeling",
    "text": "Exercise 10: 2 quantitative predictors – modeling\n\n-24.06 = average ridership on days with 0 windspeed and a 0 degree temperature. (Note: this is a correct interpretation, even though it doesn’t make conceptual sense! The model doesn’t know that ridership can’t be negative!)\nAt any temperature, a 1mph increase in windspeed is associated with a 37 rider decrease in average ridership.\nAt any windspeed, a 1 degree increase in temperature is associated with a 56 rider increase in average ridership.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-11-which-is-best-1",
    "href": "activities/09-mlr-principles.html#exercise-11-which-is-best-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 11: Which is best?",
    "text": "Exercise 11: Which is best?\n\nmodel 2\ntemperature\nR-squared increases (our model is stronger when we include another predictor)\nnope. model 1 (with windspeed and weekend) has a lower R-squared than model 5 (with only temperature)",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-13-practice-1-1",
    "href": "activities/09-mlr-principles.html#exercise-13-practice-1-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 13: Practice 1",
    "text": "Exercise 13: Practice 1\n\nbikes %&gt;% \n  ggplot(aes(y = rides, x = weekend, fill = weather_cat)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\nnew_model_1 &lt;- lm(rides ~ weekend + weather_cat, bikes)\ncoef(summary(new_model_1))\n##                     Estimate Std. Error   t value      Pr(&gt;|t|)\n## (Intercept)        4211.8741   75.54724 55.751529 9.461947e-265\n## weekendTRUE        -982.2106  117.24719 -8.377264  2.786301e-16\n## weather_catcateg2  -608.8640  113.00211 -5.388077  9.628947e-08\n## weather_catcateg3 -2360.2049  319.71640 -7.382183  4.270163e-13\n\n\nThe average ridership on a weekday with nice weather (categ1) is 4212 rides.\nOn days with the same weather, the average ridership is 982 less on a weekend than on a weekday.\nOn any day of the week, the average ridership is 609 less on dreary days than on nice days.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-14-practice-2-1",
    "href": "activities/09-mlr-principles.html#exercise-14-practice-2-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 14: Practice 2",
    "text": "Exercise 14: Practice 2\n\nbikes %&gt;% \n  ggplot(aes(y = rides, x = temp_feel, color = humidity)) + \n    geom_point()\n\n\n\n\n\n\n\n\nnew_model_2 &lt;- lm(rides ~ temp_feel + humidity, bikes)\ncoef(summary(new_model_2))\n##                Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)   315.83704 303.777334  1.039699 2.988249e-01\n## temp_feel      60.43316   3.272315 18.468015 9.451345e-63\n## humidity    -1868.99356 336.963661 -5.546573 4.078901e-08\n\n\nOn average, there are 316 riders on days with 0 humidity that feel like 0 degrees.\nAt any humidity, a 1 degree increase in ridership is associated with a 60 ride increase in average ridership.\nAt any temperature, a 1 percentage point increase in humidity (i.e. a 0.1 increase in humidity) is associated with a 187 decrease in average ridership.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-15-practice-3-1",
    "href": "activities/09-mlr-principles.html#exercise-15-practice-3-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 15: Practice 3",
    "text": "Exercise 15: Practice 3\n\nnew_model_3 &lt;- lm(rides ~ temp_feel + weather_cat, bikes)\ncoef(summary(new_model_3))\n##                      Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)        -288.68840 251.264383 -1.148943 2.509574e-01\n## temp_feel            55.30133   3.215495 17.198387 7.082670e-56\n## weather_catcateg2  -386.42241 100.187725 -3.856984 1.249775e-04\n## weather_catcateg3 -1919.01375 283.022420 -6.780430 2.481218e-11\n\nbikes %&gt;% \n  ggplot(aes(y = rides, x = temp_feel, color = weather_cat)) + \n  geom_point() + \n  geom_line(aes(y = new_model_3$fitted.values), size = 1.5)\n\n\n\n\n\n\n\n\n\nOn average, there are -289 riders on nice weather days that feel like 0 degrees. (Note: this is a correct interpretation, even though it doesn’t make conceptual sense!)\nOn days with the same weather, a 1 degree increase in temperature is associated with a 55 ride increase in average ridership.\nOn days with the same temperature, average ridership is 386 rides lower on a dreary weather day than on a nice weather day.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/09-mlr-principles.html#exercise-16-challenge-1",
    "href": "activities/09-mlr-principles.html#exercise-16-challenge-1",
    "title": "Multiple Linear Regression - Principles",
    "section": "Exercise 16: CHALLENGE",
    "text": "Exercise 16: CHALLENGE\n\nbikes %&gt;% \n  ggplot(aes(y = rides, x = temp_feel, color = humidity, size = weekend)) + \n  geom_point()\n\n\n\n\n\n\n\n\nnew_model_4 &lt;- lm(rides ~ temp_feel + humidity + weekend, bikes)\ncoef(summary(new_model_4))\n##                Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)   668.60236 292.181063  2.288315 2.240530e-02\n## temp_feel      59.36751   3.119256 19.032585 7.626695e-66\n## humidity    -1906.43437 320.982938 -5.939364 4.433789e-09\n## weekendTRUE  -869.05771 100.057822 -8.685555 2.471050e-17\n\n\nOn average, there are 669 riders on weekdays that feel like 0 degrees and have no humidity.\nOn days with the same humidity levels and time of the week, a 1 degree increase in temperature is associated with a 59 ride increase in average ridership.\nOn days with the same temperature and time of the week, a 0.1 point increase in humidity levels (i.e. a 1 percentage point increase in humidity) is associated with a 190.6 ride decrease in average ridership.\nOn days with the same temperature and humidity, the average ridership is 869 rides lower on weekends compared to weekdays.",
    "crumbs": [
      "Multiple Linear Regression - Principles"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html",
    "href": "activities/07-slr-cat-predictor.html",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#learning-goals",
    "href": "activities/07-slr-cat-predictor.html#learning-goals",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nWrite a model formula for a simple linear regression model with a categorical predictor using indicator variables\nInterpret the coefficients in a simple linear regression model with a categorical predictor",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#readings-and-videos",
    "href": "activities/07-slr-cat-predictor.html#readings-and-videos",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Readings and videos",
    "text": "Readings and videos\nComplete both the reading and the videos to go through before class.\n\nReading: Section 3.9 in the STAT 155 Notes only up through section 3.9.1 Indicator Variables\nVideos:\n\nSimple linear regression: categorical predictor (slides)\nR Code for Categorical Predictors\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-1-get-to-know-the-data",
    "href": "activities/07-slr-cat-predictor.html#exercise-1-get-to-know-the-data",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\nWrite R code to answer the following:\n\n\nHow many cases and variables do we have? What does a case represent?\n\n\nWhat do the first few rows of the data look like?\n\n\nConstruct and interpret two different visualizations of the price variable.\n\n\nConstruct and interpret a visualization of the cut variable.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-2-visualizations",
    "href": "activities/07-slr-cat-predictor.html#exercise-2-visualizations",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 2: Visualizations",
    "text": "Exercise 2: Visualizations\nStart by visualizing this relationship of interest, that between price and cut.\n\nThe appropriate plot depends upon the type of variables we’re plotting. When exploring the relationship between a quantitative response and a quantitative predictor, a scatterplot was an effective choice. After running the code below, explain why a scatterplot is not effective for exploring the relationship between the outcome price and categorical cut predictor.\n\n\n# Try a scatterplot\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_point()\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\n\nSeparately run each chunk below, with two plots. Comment (#) on what changes in the code / output.\n\nb.1.\n\n# Univariate boxplot\nggplot(diamonds, aes(y = price)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\nb.2.\n\n# ???\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\nb.3.\n\n# Univariate density plot\nggplot(diamonds, aes(x = price)) + \n    geom_density()\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\nb.4.\n\n# Comparisons in density plot\nggplot(diamonds, aes(x = price, color = cut)) + \n    geom_density()\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\nb.5.\n\n# Univariate histogram\nggplot(diamonds, aes(x = price)) + \n    geom_histogram()\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\nb.6. What’s the difference between this and b4 density plot comparison? Can we now interpret the relationship between price and cut as we could in density plot? Why not?\n\n# ???\nggplot(diamonds, aes(x = price)) + \n    geom_histogram() + \n    facet_wrap(~ cut)\n\n\n\n\n\n\n\n\n\nResponse: Put your response here.\n\n\nDo you notice anything interesting about the relationship between price and cut? What do you think might be happening here?\n\n\nResponse: Put your response here.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-3-numerical-summaries",
    "href": "activities/07-slr-cat-predictor.html#exercise-3-numerical-summaries",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 3: Numerical summaries",
    "text": "Exercise 3: Numerical summaries\nLet’s follow up our plots with some numerical summaries.\n\nTo warm up, first calculate the mean price across all diamonds.\n\n\ndiamonds %&gt;% \n     summarize(mean(price))\n## # A tibble: 1 × 1\n##   `mean(price)`\n##           &lt;dbl&gt;\n## 1         3933.\n\n\nTo summarize the trends we observed in the grouped plots above, we can calculate the mean price for each type of cut. This requires the inclusion of the group_by() function:\n\n\n# Calculate mean price by cut\ndiamonds %&gt;% \n    group_by(cut) %&gt;% \n    summarize(mean(price))\n## # A tibble: 5 × 2\n##   cut       `mean(price)`\n##   &lt;fct&gt;             &lt;dbl&gt;\n## 1 Fair              4359.\n## 2 Good              3929.\n## 3 Very Good         3982.\n## 4 Premium           4584.\n## 5 Ideal             3458.\n\n\nExamine the group mean measurements. can you match these numbers up with what you see in the plots?\nBased on the results above, we can see that, on average, diamonds with a “Fair” cut tend to cost more than higher-quality cuts. Let’s construct a new variable named cutFair, using on the following criteria:\n\n\ncutFair = 1 if the diamond is of Fair cut\ncutFair = 0 otherwise (any other value of cut (Good, Very Good, Premium, Ideal))\n\nThe ifelse function allows to create a new variable from an existing one, based on whether or not the values in that variable meet a certain “condition” (remember, you can always look up function documentation in R by typing ?ifelse in the Console, and hitting enter!).\nFill in the following code to create cutFair. The condition was given to you already. Try to use this to complete the code.\n\n# In the first blank, put what value cutFair should have if the condition is \"met\", or TRUE\n# In the second blank, put what value cutFair should have if the condition is \"not met\", or FALSE\ndiamonds &lt;- diamonds %&gt;%\n  mutate(cutFair=ifelse(cut == \"Fair\", 1, 0))\n\nVariables like cutFair that are coded as 0/1 to numerically indicate if a categorical variable is at a particular state are known as an indicator variable. You will sometimes see these referred to as a “binary variable” or “dichotomous variable”; you may also encounter the term “dummy variable” in older statistical literature.\n\nNow, let’s calculate the group means based on the new cutFair indicator variable:\n\n\ndiamonds %&gt;% \n    group_by(cutFair) %&gt;% \n    summarize(mean(price))\n## # A tibble: 2 × 2\n##   cutFair `mean(price)`\n##     &lt;dbl&gt;         &lt;dbl&gt;\n## 1       0         3920.\n## 2       1         4359.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories",
    "href": "activities/07-slr-cat-predictor.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories",
    "text": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories\nNext, let’s model the trend in the relationship between the cutFair and price variables using a simple linear regression model:\n\n# Construct the model\ndiamond_mod0 &lt;- lm(price ~ cutFair, data = diamonds)\n\n# Summarize the model\nsummary(diamond_mod0)\n## \n## Call:\n## lm(formula = price ~ cutFair, data = diamonds)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n##  -4022  -2977  -1529   1391  14903 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  3919.69      17.44  224.80  &lt; 2e-16 ***\n## cutFair       439.06     100.93    4.35 1.36e-05 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3989 on 53938 degrees of freedom\n## Multiple R-squared:  0.0003507,  Adjusted R-squared:  0.0003322 \n## F-statistic: 18.93 on 1 and 53938 DF,  p-value: 1.362e-05\n\nCompare these results to the output of exercise 3e. What do you notice? How do you interpret the intercept and cutFair coefficient terms from this model?",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories",
    "href": "activities/07-slr-cat-predictor.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 5: Modeling trend using a categorical predictor with >2 categories",
    "text": "Exercise 5: Modeling trend using a categorical predictor with &gt;2 categories\nUsing a single binary predictor like the cutFair indicator variable is useful when there are two clearly delineated categories. However, the cut variable actually contains 5 categories! Because we’ve collapsed all non-Fair classifications into a single category (i.e. cutFair = 0), the model above can’t tell us anything about the difference in expected price between, say, Premium and Ideal cuts. The good news is that it is very straightforward to model categorical predictors with &gt;2 categories. We can do this by using the cut variable as our predictor:\n\n# Construct the model\ndiamond_mod &lt;- lm(price ~ cut, data = diamonds)\n\n# Summarize the model\ncoef(summary(diamond_mod))\n##               Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)  4358.7578   98.78795 44.122361 0.000000e+00\n## cutGood      -429.8933  113.84940 -3.775982 1.595493e-04\n## cutVery Good -376.9979  105.16422 -3.584849 3.375707e-04\n## cutPremium    225.4999  104.39521  2.160060 3.077240e-02\n## cutIdeal     -901.2158  102.41155 -8.799943 1.408406e-18\n\n\nEven though we specified a single predictor variable in the model, we are seeing 4 coefficient estimates–why do you think this is the case?\n\n\nNOTE: We see 4 indicator variables (for Good, Very Good, Premium, and Ideal), but we do not see cutFair in the model output. This is because Fair is the reference level of the cut variable (it’s first alphabetically). You’ll see below that it is, indeed, still in the model. You’ll also see why the term “reference level” makes sense!\n\n\nAfter examining the summary table output from the code chunk above, complete the model formula:\n\n\n\nE[price | cut] = ___ +/- ___ cutGood +/- ___ cutVery Good +/- ___ cutPremium +/- ___ cutIdeal",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-6-making-sense-of-the-model",
    "href": "activities/07-slr-cat-predictor.html#exercise-6-making-sense-of-the-model",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 6: Making sense of the model",
    "text": "Exercise 6: Making sense of the model\nRecall our model: E[price | cut] = 4358.7578 - 429.8933 cutGood - 376.9979 cutVery Good + 225.4999 cutPremium - 901.2158 cutIdeal\n\nUse the model formula to calculate the expected/typical price for diamonds of Good cut.\nSimilarly, calculate the expected/typical price for diamonds of Fair cut.\nRe-examine these 2 calculations. Where have you seen these numbers before?!",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-7-interpreting-coefficients",
    "href": "activities/07-slr-cat-predictor.html#exercise-7-interpreting-coefficients",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 7: Interpreting coefficients",
    "text": "Exercise 7: Interpreting coefficients\nRecall that our model formula is not a formula for a line. Thus we can’t interpret the coefficients as “slopes” as we have before. Taking this into account and reflecting upon your calculations above…\n\nInterpret the intercept coefficient (4358.7578) in terms of the data context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\nInterpret the cutGood and cutVery Good coefficients (-429.8933 and -376.9979) in terms of the data context. Hint: where did you use these value in the prediction calculations above?",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-8-modeling-choices-challenge",
    "href": "activities/07-slr-cat-predictor.html#exercise-8-modeling-choices-challenge",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 8: Modeling choices (CHALLENGE)",
    "text": "Exercise 8: Modeling choices (CHALLENGE)\nWhy do we fit this model in this way (using 4 indicator variables cutGood, cutVery Good, cutPremium, cutIdeal)? Instead, suppose that we created a single variable cutCat that gave each category a numerical value: 0 for Fair, 1 for Good, 2 for Very Good, 3 for Premium, and 4 for Ideal.\nHow would this change things? What are the pros and cons of each approach?",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#render-your-work",
    "href": "activities/07-slr-cat-predictor.html#render-your-work",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Render your work",
    "text": "Render your work\n\nClick the “Render” button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\nScroll through and inspect the document to check that your work translated to the HTML format correctly.\nClose the browser tab.\nGo to the “Background Jobs” pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your “Activities” subfolder within your “STAT155” folder and locate the HTML file. You can open it again in your browser to double check.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-9-diamond-color",
    "href": "activities/07-slr-cat-predictor.html#exercise-9-diamond-color",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 9: Diamond color",
    "text": "Exercise 9: Diamond color\nConsider modeling price by color.\n\nBefore creating a visualization that shows the relationship between price and color, write down what you expect the plot to look like. Then construct and interpret an appropriate plot.\nCompute the average price for each color.\nFit an appropriate linear model with lm() and display a short summary of the model.\nWrite out the model formula from the above summary.\nWhich color is the reference level? How can you tell from the model summary?\nInterpret the intercept and two other coefficients from the model in terms of the data context.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-10-diamond-clarity",
    "href": "activities/07-slr-cat-predictor.html#exercise-10-diamond-clarity",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 10: Diamond clarity",
    "text": "Exercise 10: Diamond clarity\nIf you want more practice, repeat the steps from Exercise 8 for the clarity variable.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-1-get-to-know-the-data-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-1-get-to-know-the-data-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\nA case represents a single diamond.\nThe distribution of price is right skewed with considerable high outliers. The right skew is evidenced by the mean price ($3932) being much higher than the median price ($2401).\nMost diamonds in this data are of Good cut or better. Ideal cut diamonds are the most common with each succesive grade being the next most common.\n\n\ndim(diamonds)\n## [1] 53940    11\n\nhead(diamonds)\n## # A tibble: 6 × 11\n##   carat cut       color clarity depth table price     x     y     z cutFair\n##   &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n## 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43       0\n## 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31       0\n## 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31       0\n## 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63       0\n## 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75       0\n## 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48       0\n\n# Visualize price (outcome variable)\nggplot(diamonds, aes(x = price)) +\n    geom_histogram()\n\n\n\n\n\n\n\nggplot(diamonds, aes(y = price)) +\n    geom_boxplot()\n\n\n\n\n\n\n\ndiamonds %&gt;%\n    summarize(mean(price), median(price), sd(price))\n## # A tibble: 1 × 3\n##   `mean(price)` `median(price)` `sd(price)`\n##           &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;\n## 1         3933.            2401       3989.\n\n# Visualize cut (predictor variable)\nggplot(diamonds, aes(x = cut)) +\n    geom_bar()\n\n\n\n\n\n\n\ndiamonds %&gt;% \n    count(cut)\n## # A tibble: 5 × 2\n##   cut           n\n##   &lt;fct&gt;     &lt;int&gt;\n## 1 Fair       1610\n## 2 Good       4906\n## 3 Very Good 12082\n## 4 Premium   13791\n## 5 Ideal     21551",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-2-visualizations-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-2-visualizations-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 2: Visualizations",
    "text": "Exercise 2: Visualizations\nStart by visualizing this relationship of interest, that between price and cut.\n\nWe just don’t see anything clearly on a scatterplot. With the small number of unique values of the predictor variable, all of the points are bunched up on each other.\n\n\n# Try a scatterplot\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_point()\n\n\n\n\n\n\n\n\n\nSeparately run each chunk below, with two plots. Comment (#) on what changes in the code / output.\n\n\n# Univariate boxplot\nggplot(diamonds, aes(y = price)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n# Separate boxes by category\nggplot(diamonds, aes(y = price, x = cut)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\n\n# Univariate density plot\nggplot(diamonds, aes(x = price)) + \n    geom_density()\n\n\n\n\n\n\n\n\n\n# Separate density plots by category\nggplot(diamonds, aes(x = price, color = cut)) + \n    geom_density()\n\n\n\n\n\n\n\n\n\n# Univariate histogram\nggplot(diamonds, aes(x = price)) + \n    geom_histogram()\n\n\n\n\n\n\n\n\n\n# Separate histograms by category\nggplot(diamonds, aes(x = price)) + \n    geom_histogram() + \n    facet_wrap(~ cut)\n\n\n\n\n\n\n\n\n\nThe relationship between price and cut seems to be opposite what we would expect. The diamonds with the best cut (Ideal) have the lowest average price, and the ones with the worst cut (Fair) are woth the most. Maybe something else is different between the diamonds with the best and worst cuts…size maybe?",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-3-numerical-summaries-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-3-numerical-summaries-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 3: Numerical summaries",
    "text": "Exercise 3: Numerical summaries\nLet’s follow up our plots with some numerical summaries.\n\nMean price across all diamonds:\n\n\ndiamonds %&gt;% \n    summarize(mean(price))\n## # A tibble: 1 × 1\n##   `mean(price)`\n##           &lt;dbl&gt;\n## 1         3933.\n\n\nMean price for each type of cut:\n\n\ndiamonds %&gt;% \n    group_by(cut) %&gt;% \n    summarize(mean(price))\n## # A tibble: 5 × 2\n##   cut       `mean(price)`\n##   &lt;fct&gt;             &lt;dbl&gt;\n## 1 Fair              4359.\n## 2 Good              3929.\n## 3 Very Good         3982.\n## 4 Premium           4584.\n## 5 Ideal             3458.\n\n\nGroup means should reflect what you see in the plots (easiest to see in the boxplots)\nCreate our new cutFair variable:\n\n\ndiamonds &lt;- diamonds %&gt;%\n  mutate(cutFair=ifelse(cut == \"Fair\", 1, 0))\n\n\nCalculate the group means based on this new variable\n\n\ndiamonds %&gt;% \n    group_by(cutFair) %&gt;% \n    summarize(mean(price))\n## # A tibble: 2 × 2\n##   cutFair `mean(price)`\n##     &lt;dbl&gt;         &lt;dbl&gt;\n## 1       0         3920.\n## 2       1         4359.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-4-modeling-trend-using-a-categorical-predictor-with-exactly-2-categories-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories",
    "text": "Exercise 4: Modeling trend using a categorical predictor with exactly 2 categories\n\n# Construct the model\ndiamond_mod0 &lt;- lm(price ~ cutFair, data = diamonds)\n\n# Summarize the model\ncoef(summary(diamond_mod0))\n##              Estimate Std. Error    t value     Pr(&gt;|t|)\n## (Intercept) 3919.6946    17.4367 224.795616 0.000000e+00\n## cutFair      439.0632   100.9269   4.350309 1.361951e-05\n\nThe intercept is the expected value (mean) of the price for all diamonds with a cut quality that isn’t Fair (Good, Very Good, Premium, or Ideal, i.e. when cutFair = 0)–the same as we saw in exercise 3e.\n\nWhen we add the intercept and coefficient for cutFair, we get 3919.69 + 439.06 = 4358.75–this is the mean price for all diamonds with a Fair cut quality that we saw in exercise 3e! Therefore, the coefficient of cutFair (439.06) is interpreted as the difference between the mean value of diamonds with a Fair cut quality and the mean value of diamonds with a higher cut quality.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-5-modeling-trend-using-a-categorical-predictor-with-2-categories-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 5: Modeling trend using a categorical predictor with >2 categories",
    "text": "Exercise 5: Modeling trend using a categorical predictor with &gt;2 categories\n\n# Construct the model\ndiamond_mod &lt;- lm(price ~ cut, data = diamonds)\n\n# Summarize the model\ncoef(summary(diamond_mod))\n##               Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)  4358.7578   98.78795 44.122361 0.000000e+00\n## cutGood      -429.8933  113.84940 -3.775982 1.595493e-04\n## cutVery Good -376.9979  105.16422 -3.584849 3.375707e-04\n## cutPremium    225.4999  104.39521  2.160060 3.077240e-02\n## cutIdeal     -901.2158  102.41155 -8.799943 1.408406e-18\n\n\nWe are seeing 4 coefficient estimates because each category is being assigned to a separate indicator variable–cutGood = 1 when cut == \"Good\" and 0 otherwise, cutVery Good = 1 when `cut == “Very Good” and 0 otherwise, and so on.\nE[price | cut] = 4358.7578 - 429.8933 cutGood - 376.9979 cutVery Good + 225.4999 cutPremium - 901.2158 cutIdeal",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-6-making-sense-of-the-model-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-6-making-sense-of-the-model-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 6: Making sense of the model",
    "text": "Exercise 6: Making sense of the model\n\nExpected/typical price for diamonds of Good cut:\n\nE[price | cut] = 4358.7578 - 429.8933 * 1 - 376.9979 * 0 + 225.4999 * 0 - 901.2158 * 0 = 4358.7578 - 429.8933 = $3928.865\n\npredict(diamond_mod, newdata = data.frame(cut = \"Good\"))\n##        1 \n## 3928.864\n\n\nExpected/typical price for diamonds of Fair cut:\n\nE[price | cut] = 4358.7578 - 429.8933 * 0 - 376.9979 * 0 + 225.4999 * 0 - 901.2158 * 0 = $4358.7578\n\npredict(diamond_mod, newdata = data.frame(cut = \"Fair\"))\n##        1 \n## 4358.758\n\n\nThese come from our group mean calculations in Exercise 3b! The predicted value for diamonds of Fair cut is also the same as what we obtained using the SLR model in exercise 4 with only a single cutFair indicator variable.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-7-interpreting-coefficients-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-7-interpreting-coefficients-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 7: Interpreting coefficients",
    "text": "Exercise 7: Interpreting coefficients\nRecall that our model formula is not a formula for a line. Thus we can’t interpret the coefficients as “slopes” as we have before. Taking this into account and reflecting upon your calculations above…\n\nThe average price of a Fair cut diamonds is $4358.7578.\n\nInterpretation of cutGood coefficient: On average, Good cut diamonds are worth $429.89 less than Fair cut diamonds.\nInterpretation of cutVery Good coefficient: On average, Very Good cut diamonds are worth $377.00 less than Fair cut diamonds.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-8-modeling-choices-challenge-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-8-modeling-choices-challenge-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 8: Modeling choices (CHALLENGE)",
    "text": "Exercise 8: Modeling choices (CHALLENGE)\nWhy do we fit this model in this way (using 4 indicator variables cutGood, cutVery Good, cutPremium, cutIdeal)? Instead, suppose that we created a single variable cutCat that gave each category a numerical value: 0 for Fair, 1 for Good, 2 for Very Good, 3 for Premium, and 4 for Ideal.\n\nIf we used 0-4 instead of creating indicator variables, we would be constraining the change from 0 to 1, from 1 to 2, etc. to always be of the same magnitude. That is, a 1 unit change in the cut variable would always have the same change in price in our model.\nUsing separate indicator variables allows the difference between subsequent categories to be different, which allows our model to be a bit more nuanced. It is possible to take nuance too far though. For example, in our previous investigations of bikeshare data, we modeled ridership versus temperature. We treated temperature as a quantitative predictor. Imagine if we had created an indicator variable for each unique temperature in the data—that would be so many variables! Having so many variables creates a very complex model which can be hard to make sense of. (These ideas are addressed further in STAT 253: Statistical Machine Learning!)",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-9-diamond-color-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-9-diamond-color-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 9: Diamond color",
    "text": "Exercise 9: Diamond color\nConsider modeling price by color.\n\nThe best color diamonds are J, and worst are D. We would expect D diamonds to have the lowest price and increase steadily as we get to J. This is in fact what we see in the boxplots.\n\n\nggplot(diamonds, aes(x = color, y = price)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\ndiamonds %&gt;% \n    group_by(color) %&gt;% \n    summarize(mean(price))\n## # A tibble: 7 × 2\n##   color `mean(price)`\n##   &lt;fct&gt;         &lt;dbl&gt;\n## 1 D             3170.\n## 2 E             3077.\n## 3 F             3725.\n## 4 G             3999.\n## 5 H             4487.\n## 6 I             5092.\n## 7 J             5324.\n\n\nWe fit a linear model and obtain the model formula: E[price | color] = 3169.95 - 93.20 colorE + 554.93 colorF + 829.18 colorG + 1316.72 colorH + 1921.92 colorI + 2153.86 colorJ\n\n\ndiamond_mod2 &lt;- lm(price ~ color, data = diamonds)\n\ncoef(summary(diamond_mod2))\n##               Estimate Std. Error   t value      Pr(&gt;|t|)\n## (Intercept) 3169.95410   47.70694 66.446391  0.000000e+00\n## colorE       -93.20162   62.04724 -1.502107  1.330752e-01\n## colorF       554.93230   62.38527  8.895246  6.004834e-19\n## colorG       829.18158   60.34470 13.740751  6.836340e-43\n## colorH      1316.71510   64.28715 20.481777  7.074714e-93\n## colorI      1921.92086   71.55308 26.860072 7.078041e-158\n## colorJ      2153.86392   88.13203 24.439060 3.414906e-131\n\n\nColor D is the reference level because we don’t see its indicator variable in the model output.\nInterpretation of the intercept: Diamonds with D color cost $3169.95 on average.\nInterpretation of the colorE coefficient: Diamonds with E color cost $93.20 less than D color diamonds on average.\nInterpretation of the colorF coefficient: Diamonds with F color cost $554.93 more than D color diamonds on average.",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/07-slr-cat-predictor.html#exercise-10-diamond-clarity-1",
    "href": "activities/07-slr-cat-predictor.html#exercise-10-diamond-clarity-1",
    "title": "Simple Linear Regression - Categorical Predictor",
    "section": "Exercise 10: Diamond clarity",
    "text": "Exercise 10: Diamond clarity\nWe see the unexpected result that diamonds of better clarity (VS1 and higher) have lower average prices. In fact the best clarity diamonds (VVS1 and IF) have the lowest average prices. What might be going on? What if the most clear diamonds were also quite small…\n\nggplot(diamonds, aes(x = clarity, y = price)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\ndiamonds %&gt;% \n    group_by(clarity) %&gt;% \n    summarize(mean(price))\n## # A tibble: 8 × 2\n##   clarity `mean(price)`\n##   &lt;fct&gt;           &lt;dbl&gt;\n## 1 I1              3924.\n## 2 SI2             5063.\n## 3 SI1             3996.\n## 4 VS2             3925.\n## 5 VS1             3839.\n## 6 VVS2            3284.\n## 7 VVS1            2523.\n## 8 IF              2865.\n\ndiamond_mod3 &lt;- lm(price ~ clarity, data = diamonds)\n\ncoef(summary(diamond_mod3))\n##                  Estimate Std. Error      t value      Pr(&gt;|t|)\n## (Intercept)  3924.1686910   144.5619 27.145247517 3.513547e-161\n## claritySI2   1138.8599147   150.2746  7.578526239  3.550711e-14\n## claritySI1     71.8324571   148.6049  0.483378837  6.288287e-01\n## clarityVS2      0.8207037   148.8672  0.005512992  9.956013e-01\n## clarityVS1    -84.7132999   150.9746 -0.561109670  5.747251e-01\n## clarityVVS2  -640.4316203   154.7737 -4.137858008  3.510944e-05\n## clarityVVS1 -1401.0540535   158.5401 -8.837224284  1.010097e-18\n## clarityIF   -1059.3295848   171.8990 -6.162510636  7.210567e-10",
    "crumbs": [
      "Simple Linear Regression - Categorical Predictor"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html",
    "href": "activities/05-slr-model-eval.html",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#learning-goals",
    "href": "activities/05-slr-model-eval.html#learning-goals",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nUse residual plots to evaluate the correctness of a model\nExplain the rationale for the R-squared metric of model strength\nInterpret the R-squared metric\nThink about ethical implications of modeling by examining the impacts of biased data, power dynamics, the role of categorization, and the role of emotion and lived experience",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#readings-and-videos",
    "href": "activities/05-slr-model-eval.html#readings-and-videos",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Readings and videos",
    "text": "Readings and videos\nChoose either the reading or the videos to go through before class.\n\nReading: Sections 1.7, 3.7, and 3.8 in the STAT 155 Notes\n\nNote: You do not need to focus on the “Ladder of Power” in Section 3.8. Transformations in general will be the focus of the next activity we do.\n\nVideos:\n\nModel evaluation: is the model wrong? (slides)\nModel evaluation: is the model strong? (slides)\nModel evaluation: is the model fair? (slides)\nR Code for Evaluating and Using a Linear Model",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#model-assumptions",
    "href": "activities/05-slr-model-eval.html#model-assumptions",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Model Assumptions",
    "text": "Model Assumptions\nOne way to think about model evaluation is to consider whether or not underlying assumptions of our regression models are being met (or not). Asking ourselves if our models are “wrong”, “strong”, and “fair” approaches this from one perspective. To the first question (whether our model is wrong), recall the following four assumptions of linear regression:\n\nLinearity\nIndependence\nNormality\nEqual Variance\n\nNote that they spell “LINE” (how convenient!).\nBy assumptions, we mean that the above four “things” are needed mathematically in order for linear regression to “work”.\nWhereas we can check some of these assumptions using a residual plot, we need to examine the context of our data collection when checking the Independence assumption. What we mean by independence, is that the residuals in our model do not depend on one another. This may seem like an unsatisfying definition, so here are some examples:\n\nSuppose I want to understand the association between a person’s high school GPA and their college GPA. I collect data from every graduating senior, at three different high schools. If I have college GPA as my outcome, and high school GPA as my predictor, are my residuals independent? Probably not! It is reasonable to believe that students from the same high school may have similar GPAs, due to resources their high school may have had available, or specific teachers grading differently at one school or another. This is an example of clustering, where we have clusters of students within schools. The independence assumption of our linear regression model would be violated. One way to address this would be to include which high school they went to as an additional covariate in our regression model (we’ll get to this with multiple linear regression), and more advanced methods are covered in a course on Correlated Data.\nSuppose I want to understand the association between a mouse’s weight and their water consumption across time. I collect data for 365 days for ten different mice, recording their weight and water consumption each day of the year. If I have weight as my predictor and water consumption as my outcome, are my residuals independent? Nope! This is an example of correlated data that is longitudinal in nature: I have multiple observations per individual (mouse) across time. A mouse’s weight one day is certainly not independent of it’s weight the following day. The independence assumption of our linear regression model would again be violated. One way to address this would be to include “Mouse ID” as a predictor in our regression model (again, we’ll get to this with multiple linear regression).\n\nAll types of data that will violate the independence assumption of linear regression will have some sort of correlation structure (within individual, across time, across space, etc.). Think about clusters. If your observations fall neatly into specific clusters, your data may violate the independence assumption of linear regression.\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-1-is-the-model-correct",
    "href": "activities/05-slr-model-eval.html#exercise-1-is-the-model-correct",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 1: Is the model correct?",
    "text": "Exercise 1: Is the model correct?\nLet’s revisit the Capital Bikeshare data:\n\n# Load packages and import data\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nbikes &lt;- read_csv(\"https://mac-stat.github.io/data/bikeshare.csv\")\n\nWe previously explored a model of daily ridership among registered users as a function of temperature:\n\n# Fit a linear model\nbike_model &lt;- lm(riders_registered ~ temp_feel, data = bikes)\n\n# Check it out\nsummary(bike_model)\n## \n## Call:\n## lm(formula = riders_registered ~ temp_feel, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3607.1  -959.2  -153.8   998.2  3304.8 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -667.916    251.608  -2.655  0.00811 ** \n## temp_feel     57.892      3.306  17.514  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1310 on 729 degrees of freedom\n## Multiple R-squared:  0.2961, Adjusted R-squared:  0.2952 \n## F-statistic: 306.7 on 1 and 729 DF,  p-value: &lt; 2.2e-16\n\nPlot this relationship with both a curved and linear trend line. Based on this plot, do you think the model is correct? If not, which of the LINE assumptions does it violate?\n\n# Plot temp_feel vs riders_registered with a model trend\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE) +\n    geom_smooth(se = FALSE, color = \"red\")",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-2-residual-plots",
    "href": "activities/05-slr-model-eval.html#exercise-2-residual-plots",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 2: Residual plots",
    "text": "Exercise 2: Residual plots\nPlotting the residuals vs the predictions (also called “fitted values”) for each case can help us assess how wrong our model is. This will be a particularly important tool when evaluating models with multiple predictors. Construct the residual plot for bike_model. As with the scatterplot, this plot indicates that bike_model violates one of the LINE assumptions. Explain which assumption that is and how you can tell that from just the residual plot.\nNotes:\n\nInformation about the residuals (.resid) and predictions (.fitted) are stored within our model, thus we start our ggplot() with the model name as opposed to the raw dataset. We will rarely start ggplot() with a model instead of the data.\nWe can fix this model by adding a quadratic “transformation term” (Next CP quiz topic!).\n\n\n# Check out the residual plot for bike_model\nggplot(bike_model, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) + # Check this first!\n    geom_smooth(se = FALSE)",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-3-whats-incorrect-about-this-model",
    "href": "activities/05-slr-model-eval.html#exercise-3-whats-incorrect-about-this-model",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 3: What’s incorrect about this model?",
    "text": "Exercise 3: What’s incorrect about this model?\nConsider another example. The mammals data includes data on the average brain weight (g) and body weight (kg) for a variety of mammals:\n\n# Import the data\nmammals &lt;- read_csv(\"https://mac-stat.github.io/data/mammals.csv\")\n\n# Check it out\nhead(mammals)\n## # A tibble: 6 × 4\n##    ...1 animal            body brain\n##   &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n## 1     1 Arctic fox        3.38  44.5\n## 2     2 Owl monkey        0.48  15.5\n## 3     3 Mountain beaver   1.35   8.1\n## 4     4 Cow             465    423  \n## 5     5 Grey wolf        36.3  120. \n## 6     6 Goat             27.7  115\n\nFit a model of brain vs body weight:\n\n# Construct the model\nmammal_model &lt;- lm(brain ~ body, mammals)\n\n# Check it out\nsummary(mammal_model)\n## \n## Call:\n## lm(formula = brain ~ body, data = mammals)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -810.07  -88.52  -79.64  -13.02 2050.33 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 91.00440   43.55258    2.09   0.0409 *  \n## body         0.96650    0.04766   20.28   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 334.7 on 60 degrees of freedom\n## Multiple R-squared:  0.8727, Adjusted R-squared:  0.8705 \n## F-statistic: 411.2 on 1 and 60 DF,  p-value: &lt; 2.2e-16\n\n\nConstruct two plots that will help us evaluate mammal_model:\n\n\n# Scatterplot of brain weight (y) vs body weight (x)\n# Include a model trend line (i.e. a representation of mammal_model)\n\n\n# Residual plot for mammal_model\n\n\nThese two plots confirm that our model is wrong. What is wrong? That is, which of the LINE assumptions are violated? (NOTE: We again can fix this model by “transforming” one or both of the brain and body variables (next CP quiz topic!).",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-4-exploring-mammals",
    "href": "activities/05-slr-model-eval.html#exercise-4-exploring-mammals",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 4: Exploring mammals",
    "text": "Exercise 4: Exploring mammals\nJust for fun, let’s dig into the mammals data. Discuss what you observe:\n\n# Label the points by the animal name!\n# Discuss: What 2 things are new in this code?\nggplot(mammals, aes(x = body, y = brain, label = animal)) + \n    geom_text() + \n    geom_smooth(method = \"lm\", se = FALSE) \n\n\n\n\n\n\n\n\n\n# Zoom in\nggplot(mammals, aes(x = body, y = brain, label = animal)) + \n    geom_text() + \n    lims(y = c(0, 1500), x = c(0, 600))\n\n\n\n\n\n\n\n\n\n# Zoom in more\nggplot(mammals, aes(x = body, y = brain, label = animal)) + \n    geom_text() + \n    lims(y = c(0, 500), x = c(0, 200))",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-5-is-the-model-strong-developing-r-squared-intuition",
    "href": "activities/05-slr-model-eval.html#exercise-5-is-the-model-strong-developing-r-squared-intuition",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 5: Is the model strong? Developing R-squared intuition",
    "text": "Exercise 5: Is the model strong? Developing R-squared intuition\nThe R-squared metric is a way to quantify the strength of a model. It measures how much variation in the outcome/response variable can be explained by the variation in the predictors.\nWhere does R-squared come from? Well, it turns out that we can partition the variance of the observed response values into the variability that’s explained by the model (the variance of the predictions) and the variability that’s left unexplained by the model (the variance of the residuals):\n\\[\\text{Var(observed) = Var(predicted) + Var(residuals)}\\]\nStrong models have residuals that don’t deviate far from 0. So the smaller the variance in the residuals (thus larger the variance in the predictions), the stronger the model. Take a look at the picture below and write a few sentences addressing the following:\n\nThe two rows of plots show a stronger and a weaker model. Just by looking at the blue trend line and the dispersion of the points about the line, which row corresponds to the stronger model? How can you tell? Which row would you expect to have a higher correlation?\nWhat is different about the variance of the residuals from the first to the second row?\n\n\nPutting this together, the R-squared compares Var(predicted) to Var(response):\n\\[R^2 = \\frac{\\text{variance of predicted values}}{\\text{variance of observed response values}} = 1 - \\frac{\\text{variance of residuals}}{\\text{variance of observed response values}}\\]\n\n\n\n\n\n\nR-squared\n\n\n\n\n\n\\[\nR^2 = 1 - \\frac{SSE}{SSTO} = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2}\n\\] where \\(y_i\\) are our observed outcomes, \\(i = 1, \\dots, n\\), \\(\\hat{y}_i\\) are our fitted values/predictions, and \\(\\bar{y}\\) is our observed average outcome.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-6-r-squared-interpretations",
    "href": "activities/05-slr-model-eval.html#exercise-6-r-squared-interpretations",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 6: R-squared Interpretations",
    "text": "Exercise 6: R-squared Interpretations\nRecall bikemod1 from Exercise 1, where we predicted registered riders by what the temperature felt like on a given day. Use the summary function to look out the model output for bikemod1, and interpret the \\(R^2\\) value for this model, in the context of the problem. (NOTE: \\(R^2\\) is reported in output here as “Multiple R-squared”).\n\n# Get R-squared\nsummary(bike_model)\n## \n## Call:\n## lm(formula = riders_registered ~ temp_feel, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3607.1  -959.2  -153.8   998.2  3304.8 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -667.916    251.608  -2.655  0.00811 ** \n## temp_feel     57.892      3.306  17.514  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1310 on 729 degrees of freedom\n## Multiple R-squared:  0.2961, Adjusted R-squared:  0.2952 \n## F-statistic: 306.7 on 1 and 729 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-7-further-exploring-r-squared",
    "href": "activities/05-slr-model-eval.html#exercise-7-further-exploring-r-squared",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 7: Further exploring R-squared",
    "text": "Exercise 7: Further exploring R-squared\nIn this exercise, we’ll look at data from a synthetic dataset called Anscombe’s quartet. Load the data in as follows, and look at the first few rows:\n\ndata(anscombe)\n\n# Look at the first few rows\n\nThe anscombe data is actually 4 datasets in one: x1 and y1 go together, and so forth. Examine the coefficient estimates (in the “Estimate” column of the “Coefficients:” part) and the “Multiple R-squared” value on the second to last line. What do you notice? How do these models compare?\n\nanscombe_mod1 &lt;- lm(y1 ~ x1, data = anscombe)\nanscombe_mod2 &lt;- lm(y2 ~ x2, data = anscombe)\nanscombe_mod3 &lt;- lm(y3 ~ x3, data = anscombe)\nanscombe_mod4 &lt;- lm(y4 ~ x4, data = anscombe)\n\nsummary(anscombe_mod1)\n## \n## Call:\n## lm(formula = y1 ~ x1, data = anscombe)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.92127 -0.45577 -0.04136  0.70941  1.83882 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0001     1.1247   2.667  0.02573 * \n## x1            0.5001     0.1179   4.241  0.00217 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.237 on 9 degrees of freedom\n## Multiple R-squared:  0.6665, Adjusted R-squared:  0.6295 \n## F-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\nsummary(anscombe_mod2)\n## \n## Call:\n## lm(formula = y2 ~ x2, data = anscombe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.9009 -0.7609  0.1291  0.9491  1.2691 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)    3.001      1.125   2.667  0.02576 * \n## x2             0.500      0.118   4.239  0.00218 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.237 on 9 degrees of freedom\n## Multiple R-squared:  0.6662, Adjusted R-squared:  0.6292 \n## F-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\nsummary(anscombe_mod3)\n## \n## Call:\n## lm(formula = y3 ~ x3, data = anscombe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1586 -0.6146 -0.2303  0.1540  3.2411 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0025     1.1245   2.670  0.02562 * \n## x3            0.4997     0.1179   4.239  0.00218 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.236 on 9 degrees of freedom\n## Multiple R-squared:  0.6663, Adjusted R-squared:  0.6292 \n## F-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\nsummary(anscombe_mod4)\n## \n## Call:\n## lm(formula = y4 ~ x4, data = anscombe)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -1.751 -0.831  0.000  0.809  1.839 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0017     1.1239   2.671  0.02559 * \n## x4            0.4999     0.1178   4.243  0.00216 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.236 on 9 degrees of freedom\n## Multiple R-squared:  0.6667, Adjusted R-squared:  0.6297 \n## F-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nNow take a look at the following scatterplots of the 4 pairs of variables. What do you notice? What takeaway can we draw from this exercise?\n\nggplot(anscombe, aes(x = x1, y = y1)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x2, y = y2)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x3, y = y3)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x4, y = y4)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\n\nComplete Exercise 8-11 after the class.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-8-biased-data-biased-results-example-1",
    "href": "activities/05-slr-model-eval.html#exercise-8-biased-data-biased-results-example-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 8: Biased data, biased results: example 1",
    "text": "Exercise 8: Biased data, biased results: example 1\nDATA ARE NOT NEUTRAL. Data can reflect personal biases, institutional biases, power dynamics, societal biases, the limits of our knowledge, and so on. In turn, biased data can lead to biased analyses. Consider an example.\n\nDo a Google image search for “statistics professor.” What do you observe?\nThese search results are produced by a search algorithm / model. Explain why the data used by this model are not neutral.\nWhat are the potential implications, personal or societal, of the search results produced from this biased data?",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-9-biased-data-biased-results-example-2",
    "href": "activities/05-slr-model-eval.html#exercise-9-biased-data-biased-results-example-2",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 9: Biased data, biased results: example 2",
    "text": "Exercise 9: Biased data, biased results: example 2\nConsider the example of a large company that developed a model / algorithm to review the résumés of applicants for software developer & other tech positions. The model then gave each applicant a score indicating their hireability or potential for success at the company. You can think of this model as something like:\n\\[\\text{potential for success } = \\beta_0 + \\beta_1 (\\text{features from the résumé})\\]\nSkim this Reuter’s article about the company’s résumé model.\n\nExplain why the data used by this model are not neutral.\nWhat are the potential implications, personal or societal, of the results produced from this biased data?",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-10-rigid-data-collection-systems",
    "href": "activities/05-slr-model-eval.html#exercise-10-rigid-data-collection-systems",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 10: Rigid data collection systems",
    "text": "Exercise 10: Rigid data collection systems\nWhen working with categorical variables, we’ve seen that our units of observation fall into neat groups. Reality isn’t so discrete. For example, check out questions 6 and 9 on page 2 of the 2020 US Census. With your group, discuss the following:\n\nWhat are a couple of issues you see with these questions?\nWhat impact might this type of data collection have on a subsequent analysis of the census responses and the policies it might inform?\nCan you think of a better way to write these questions while still preserving the privacy of respondents?\n\nFOR A DEEPER DISCUSSION: Read Chapter 4 of Data Feminism on “What gets counted counts”.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-11-presenting-data-elevating-emotion-and-embodiment",
    "href": "activities/05-slr-model-eval.html#exercise-11-presenting-data-elevating-emotion-and-embodiment",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 11: Presenting data: “Elevating emotion and embodiment”",
    "text": "Exercise 11: Presenting data: “Elevating emotion and embodiment”\nNote: The following example highlights work done by W.E.B. Du Bois in the late 1800s / early 1900s. His work uses language common to that time period and addresses the topic of slavery.\nThe types of visualizations we’ve been learning in this course are standard practice, hence widely understood. Yet these standard visualizations can also suppress the lived experiences of people represented in the data, hence can miss the larger point. W.E.B. Du Bois (1868–1963), a “sociologist, socialist, historian, civil rights activist, Pan-Africanist, author, writer, and editor”1, was a pioneer in elevating emotion and embodiment in data visualization. For the Paris World Fair of 1900, Du Bois and his team of students from Atlanta University presented 60 data visualizations of the Black experience in America, less than 50 years after the abolishment of slavery. To this end, Du Bois noted that “I wanted to set down its aim and method in some outstanding way which would bring my work to notice by the thinking world.” That is, he wanted to increase the impact of his work by partnering technical visualizations with design that better connects to lived experiences. Check out:\n\nAn article by Allen Hillery (@AlDatavizguy).\nA complete set of the data visualizations provided by Anthony Starks (@ajstarks).\n\nDiscuss your observations. In what ways do you think the W.E.B. Du Bois visualizations might have been more effective at sharing his work than, say, plainer bar charts?\nFOR A DEEPER DISCUSSION AND MORE MODERN EXAMPLES: Read Chapter 3 of Data Feminism on the principle of elevating emotion and embodiment, i.e. the value of “multiple forms of knowledge, including the knowledge that comes from people as living, feeling bodies in the world.”",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-1-is-the-model-correct-1",
    "href": "activities/05-slr-model-eval.html#exercise-1-is-the-model-correct-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 1: Is the model correct?",
    "text": "Exercise 1: Is the model correct?\nThe red curved trend line shows a clear downward trend around 85 degrees, which contextually makes plenty of sense—extremely hot days would naturally see less riders. Overall the combination of the upward trend and downward trend makes for a curved relationship that is not captured well by a straight line of best fit. Specifically, a simple linear regression model would violate the Linearity assumption.\n\n# Load packages and import data\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\nbikes &lt;- read_csv(\"https://mac-stat.github.io/data/bikeshare.csv\")\n\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) + \n    geom_point() + \n    geom_smooth(se = FALSE, color = \"red\") +\n    geom_smooth(method = \"lm\", se = FALSE)",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-2-residual-plots-1",
    "href": "activities/05-slr-model-eval.html#exercise-2-residual-plots-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 2: Residual plots",
    "text": "Exercise 2: Residual plots\nThe residual plot shows a lingering trend in the residuals—the blue curve traces the trend in the residuals, and it does not lie flat on the y = 0 line. This again suggests that the Linearity assumption is violated.\n\nbike_model &lt;- lm(riders_registered ~ temp_feel, data = bikes)\n\n# Check out the residual plot for bike_model\nggplot(bike_model, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-3-whats-incorrect-about-this-model-1",
    "href": "activities/05-slr-model-eval.html#exercise-3-whats-incorrect-about-this-model-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 3: What’s incorrect about this model?",
    "text": "Exercise 3: What’s incorrect about this model?\n\n# Import the data\nmammals &lt;- read_csv(\"https://mac-stat.github.io/data/mammals.csv\")\n\n# Check it out\nhead(mammals)\n## # A tibble: 6 × 4\n##    ...1 animal            body brain\n##   &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;dbl&gt;\n## 1     1 Arctic fox        3.38  44.5\n## 2     2 Owl monkey        0.48  15.5\n## 3     3 Mountain beaver   1.35   8.1\n## 4     4 Cow             465    423  \n## 5     5 Grey wolf        36.3  120. \n## 6     6 Goat             27.7  115\n\n# Construct the model\nmammal_model &lt;- lm(brain ~ body, mammals)\n\n# Check it out\nsummary(mammal_model)\n## \n## Call:\n## lm(formula = brain ~ body, data = mammals)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -810.07  -88.52  -79.64  -13.02 2050.33 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 91.00440   43.55258    2.09   0.0409 *  \n## body         0.96650    0.04766   20.28   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 334.7 on 60 degrees of freedom\n## Multiple R-squared:  0.8727, Adjusted R-squared:  0.8705 \n## F-statistic: 411.2 on 1 and 60 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n# Scatterplot of brain weight (y) vs body weight (x)\n# Include a model trend line (i.e. a representation of mammal_model)\nggplot(mammals, aes(y = brain, x = body)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n# Residual plot for mammal_model\nggplot(mammal_model, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\n\nThe biggest issue here is that the assumption of equal variance is violated. There’s much greater variability in the residuals as the predictions increase. This is because there’s much greater variability in the brain weights (y) as body weights (x) increase.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-4-exploring-mammals-1",
    "href": "activities/05-slr-model-eval.html#exercise-4-exploring-mammals-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 4: Exploring mammals",
    "text": "Exercise 4: Exploring mammals\nAnswers will vary.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-5-is-the-model-strong-developing-r-squared-intuition-1",
    "href": "activities/05-slr-model-eval.html#exercise-5-is-the-model-strong-developing-r-squared-intuition-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 5: Is the model strong? Developing R-squared intuition",
    "text": "Exercise 5: Is the model strong? Developing R-squared intuition\nThe R-squared metric is a way to quantify the strength of a model. It measures how much variation in the outcome/response variable can be explained by the model.\nWhere does R-squared come from? Well, it turns out that we can partition the variance of the observed response values into the variability that’s explained by the model (the variance of the predictions) and the variability that’s left unexplained by the model (the variance of the residuals):\n\\[\\text{Var(observed) = Var(predicted) + Var(residuals)}\\]\n“Good” models have residuals that don’t deviate far from 0. So the smaller the variance in the residuals (thus larger the variance in the predictions), the stronger the model. Take a look at the picture below and write a few sentences addressing the following:\n\nThe first row corresponds to the weaker model. We can tell because the points are much more dispersed from the trend line than in the second row. Recall that the correlation metric measures how closely clustered points are about a straight line of best fit, so we would expect the correlation to be lower for the first row than the second row.\nThe variance of the residuals is much lower for the second row—the residuals are all quite small. This indicates a stronger model.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-6-r-squared-interpretations-1",
    "href": "activities/05-slr-model-eval.html#exercise-6-r-squared-interpretations-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 6: R-squared Interpretations",
    "text": "Exercise 6: R-squared Interpretations\n\nsummary(bike_model)\n## \n## Call:\n## lm(formula = riders_registered ~ temp_feel, data = bikes)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3607.1  -959.2  -153.8   998.2  3304.8 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -667.916    251.608  -2.655  0.00811 ** \n## temp_feel     57.892      3.306  17.514  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1310 on 729 degrees of freedom\n## Multiple R-squared:  0.2961, Adjusted R-squared:  0.2952 \n## F-statistic: 306.7 on 1 and 729 DF,  p-value: &lt; 2.2e-16\n\nMultiple R-squared: 0.2961\nInterpretation: 29.61% of the variation in number of registered riders on any given day can be explained by the variation in temperature (specifically, what temperature it “feels” like it is).",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercise-7-further-exploring-r-squared-1",
    "href": "activities/05-slr-model-eval.html#exercise-7-further-exploring-r-squared-1",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercise 7: Further exploring R-squared",
    "text": "Exercise 7: Further exploring R-squared\nIn this exercise, we’ll look at data from a synthetic dataset called Anscombe’s quartet. Load the data in as follows, and look at the first few rows:\n\ndata(anscombe)\n\n# Look at the first few rows\nhead(anscombe)\n##   x1 x2 x3 x4   y1   y2    y3   y4\n## 1 10 10 10  8 8.04 9.14  7.46 6.58\n## 2  8  8  8  8 6.95 8.14  6.77 5.76\n## 3 13 13 13  8 7.58 8.74 12.74 7.71\n## 4  9  9  9  8 8.81 8.77  7.11 8.84\n## 5 11 11 11  8 8.33 9.26  7.81 8.47\n## 6 14 14 14  8 9.96 8.10  8.84 7.04\n\nAll of these models have close to the same intercept, slope, and R-squared!\n\nanscombe_mod1 &lt;- lm(y1 ~ x1, data = anscombe)\nanscombe_mod2 &lt;- lm(y2 ~ x2, data = anscombe)\nanscombe_mod3 &lt;- lm(y3 ~ x3, data = anscombe)\nanscombe_mod4 &lt;- lm(y4 ~ x4, data = anscombe)\n\nsummary(anscombe_mod1)\n## \n## Call:\n## lm(formula = y1 ~ x1, data = anscombe)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.92127 -0.45577 -0.04136  0.70941  1.83882 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0001     1.1247   2.667  0.02573 * \n## x1            0.5001     0.1179   4.241  0.00217 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.237 on 9 degrees of freedom\n## Multiple R-squared:  0.6665, Adjusted R-squared:  0.6295 \n## F-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\nsummary(anscombe_mod2)\n## \n## Call:\n## lm(formula = y2 ~ x2, data = anscombe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.9009 -0.7609  0.1291  0.9491  1.2691 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)    3.001      1.125   2.667  0.02576 * \n## x2             0.500      0.118   4.239  0.00218 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.237 on 9 degrees of freedom\n## Multiple R-squared:  0.6662, Adjusted R-squared:  0.6292 \n## F-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\nsummary(anscombe_mod3)\n## \n## Call:\n## lm(formula = y3 ~ x3, data = anscombe)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1586 -0.6146 -0.2303  0.1540  3.2411 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0025     1.1245   2.670  0.02562 * \n## x3            0.4997     0.1179   4.239  0.00218 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.236 on 9 degrees of freedom\n## Multiple R-squared:  0.6663, Adjusted R-squared:  0.6292 \n## F-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\nsummary(anscombe_mod4)\n## \n## Call:\n## lm(formula = y4 ~ x4, data = anscombe)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -1.751 -0.831  0.000  0.809  1.839 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   3.0017     1.1239   2.671  0.02559 * \n## x4            0.4999     0.1178   4.243  0.00216 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.236 on 9 degrees of freedom\n## Multiple R-squared:  0.6667, Adjusted R-squared:  0.6297 \n## F-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nBut when we look at the scatterplots, they all look substantially different, and we would want to approach our modeling differently for each one:\n\nx1 and y1: A linear model seems appropriate for this data.\nx2 and y2: The scatterplot is clearly curved—a “linear” regression model with squared terms, for example, would be more appropriate for this data. (We’ll talk more about ways to handle nonlinear relationships soon!)\nx3 and y3: There is a very clear outlier at about x3 = 13 that we would want to dig into to better understand the context. After that investigation, we might consider removing this outlier and refitting the model.\nx4 and y4: There is clearly something strange going on with most of the cases having an x4 value of exactly 8. We would not want to jump straight into modeling. Instead, we should dig deeper to find out more about this data.\n\n\nggplot(anscombe, aes(x = x1, y = y1)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x2, y = y2)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x3, y = y3)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(anscombe, aes(x = x4, y = y4)) +\n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE)",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#exercises-8---11",
    "href": "activities/05-slr-model-eval.html#exercises-8---11",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Exercises 8 - 11",
    "text": "Exercises 8 - 11\nNo solutions for these exercises. These require longer discussions, not discrete answers.",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/05-slr-model-eval.html#footnotes",
    "href": "activities/05-slr-model-eval.html#footnotes",
    "title": "Simple Linear Regression - Model Evaluation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/W._E._B._Du_Bois↩︎",
    "crumbs": [
      "Simple Linear Regression - Model Evaluation"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html",
    "href": "activities/02-foundations-univariate.html",
    "title": "Univariate Visualization & Summaries",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#learning-goals",
    "href": "activities/02-foundations-univariate.html#learning-goals",
    "title": "Univariate Visualization & Summaries",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this activity, you should be able to:\n\nWhat summarizations and visualizations are appropriate for categorical or quantitative variables\nWrite R code to read in data and to summarize and visualize a single variable at a time.\nInterpret key features of barplots, boxplots, histograms, and density plots\nDescribe information about the distribution of a quantitative variable using the concepts of shape, center, spread, and outliers\nRelate summary statistics of data to the concepts of shape, center, spread, and outliers",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#readings-and-videos",
    "href": "activities/02-foundations-univariate.html#readings-and-videos",
    "title": "Univariate Visualization & Summaries",
    "section": "Readings and videos",
    "text": "Readings and videos\nYou should have gone through the followings before the class and finished checkpoint quizzes!\n\nReading: Sections 2.1-2.4, 2.6 in the STAT 155 Notes\nVideos:\n\nUnivariate summaries (slides)\n\nPart 1\nPart 2\n\nR Code for Categorical Visualization and Summarization\nR Code for Quantitative Visualization and Summarization\nQuarto docs\n\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-1-importing-and-getting-to-know-the-data",
    "href": "activities/02-foundations-univariate.html#exercise-1-importing-and-getting-to-know-the-data",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 1: Importing and getting to know the data",
    "text": "Exercise 1: Importing and getting to know the data\nFirst, in the Console pane of RStudio, run the following command to install some necessary packages (you will need to do this any time you are installing a new package):\ninstall.packages(\"tidyverse\") # We have already installed it!\nNow, in the Quarto pane, run the following code chunk to load the package and load a dataset (you can either click the green arrow in the top right of the code chunk, put your cursor in the code chunk and hit Ctrl+Alt+C [on Windows/Linux] or Command+Option+C [on Mac]).\n\n# Load package\nlibrary(tidyverse)\n\n# Read in the Dear Abby data\nabby &lt;- read_csv(\"https://mac-stat.github.io/data/dear_abby.csv\")\n\nThroughout this activity, we’ll work only with the most recent year of data, from 2017. Run the following chunk:\n\n# Wrangle the Dear Abby data\n# Ignore this code for now!\nabby &lt;- abby %&gt;% \n  filter(year == 2017) %&gt;% \n  mutate(month = month(month, label = TRUE)) %&gt;%\n  mutate(\n    parents = str_detect(question_only, \"mother|mama|mom|father|papa|dad\"),\n    marriage = str_detect(question_only, \"marriage|marry|married\"),\n    money = str_detect(question_only, \"money|finance\")\n  ) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    themes = c(\n      if (parents) \"parents\",\n      if (marriage) \"marriage\",\n      if (money) \"money\"\n    ) %&gt;% paste(collapse = \", \"),\n    themes = ifelse(themes == \"\", \"other\", themes)\n  ) %&gt;%\n  ungroup() %&gt;% \n  select(year, month, day, question_only, bing_pos, afinn_overall, afinn_pos, afinn_neg, themes)\n\n\nClick on the Environment tab (generally in the upper right hand pane in RStudio). Then click the abby line. The abby data will pop up as a separate pane (like viewing a spreadsheet) – check it out.\nIn this tidy dataset, what is the unit of observation? That is, what is represented in each row of the dataset?\nWhat term do we use for the columns of the dataset?\nTry out each function below. Identify what each function tells you about the abby data and note this in the ???:\n\n\n# ??? [what do both numbers mean?]\ndim(abby)\n## [1] 514   9\n\n\n# ???\nnrow(abby)\n## [1] 514\n\n\n# ???\nncol(abby)\n## [1] 9\n\n\n# ???\nhead(abby)\n## # A tibble: 6 × 9\n##    year month day   question_only     bing_pos afinn_overall afinn_pos afinn_neg\n##   &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1  2017 Aug   30    \"i moved to the …    0.75             14        16         2\n## 2  2017 Aug   30    \"under what circ…   NA                NA        NA        NA\n## 3  2017 Aug   28    \"i'm not a dog p…    0.333             5         5         0\n## 4  2017 Aug   28    \"my 62-year-old …    0.143           -11         8        19\n## 5  2017 Aug   27    \"i have a friend…    0.222             0         7         7\n## 6  2017 Aug   27    \"i have been sel…    0.333            -5         2         7\n## # ℹ 1 more variable: themes &lt;chr&gt;\n\n\n# ???\nnames(abby)\n## [1] \"year\"          \"month\"         \"day\"           \"question_only\"\n## [5] \"bing_pos\"      \"afinn_overall\" \"afinn_pos\"     \"afinn_neg\"    \n## [9] \"themes\"\n\n\n[OPTIONAL] If you’re not sure how exactly to use a function, you can pull up a built-in help page with information about the arguments a function takes (i.e., what goes inside the parentheses), and the output it produces. To do this, click inside the Console pane, and enter ?function_name. For example, to pull up a help page for the dim() function, we can type ?dim and hit Enter. Try pulling up the help page for the read_csv() function we used to load the dataset.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-2-preparing-to-summarize-and-visualize-the-data",
    "href": "activities/02-foundations-univariate.html#exercise-2-preparing-to-summarize-and-visualize-the-data",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 2: Preparing to summarize and visualize the data",
    "text": "Exercise 2: Preparing to summarize and visualize the data\nIn the next exercises, we will be exploring themes in the Dear Abby questions and the overall “mood” or sentiment of the questions. Before continuing, read the codebook for this dataset for some context about sentiment analysis, which gives us a measure of the mood/sentiment of a text.\n\nWhat sentiment variables do we have in the dataset? Are they quantitative or categorical?\nCheck out the theme variable. Is this quantitative or categorical?\nWhat visualizations are appropriate for looking at the distribution of a single quantitative variable? What about a single categorical variable?",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-3-exploring-themes-in-the-letters",
    "href": "activities/02-foundations-univariate.html#exercise-3-exploring-themes-in-the-letters",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 3: Exploring themes in the letters",
    "text": "Exercise 3: Exploring themes in the letters\n\nThe code below makes a barplot of the themes variable using the ggplot2 visualization package. Before making the plot, make note of what you expect the plot might look like. (This might be hard–just do your best!) Then compare to what you observe when you run the code chunk to make the plot. (Clearly defining your expectations first is good scientific practice to avoid confirmation bias.)\n\n\n# Load package\n# install.packages(\"ggplot2\") # Run in R Console first time to install (copy without '#')\nlibrary(ggplot2)\n\n# barplot\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\nWe can follow up on the barplot with a simple numerical summary. Whereas the ggplot2 package is great for visualizations, dplyr is great for numerical summaries. The code below constructs a table of the number of questions with each theme. Make sure that these numerical summaries match up with what you saw in the barplot.\n\n\n# install.packages(\"dplyr\")\n# Construct a table of counts\nabby %&gt;% \n    count(themes)\n## # A tibble: 8 × 2\n##   themes                       n\n##   &lt;chr&gt;                    &lt;int&gt;\n## 1 marriage                    75\n## 2 marriage, money              5\n## 3 money                       21\n## 4 other                      234\n## 5 parents                    127\n## 6 parents, marriage           33\n## 7 parents, marriage, money     4\n## 8 parents, money              15\n\n\nBefore proceeding, let’s break down the plotting code above. Run each chunk to see how the two lines of code above build up the plot in “layers”. Add comments (on the lines starting with #) to document what you notice.\n\n\n# ???\nggplot(abby, aes(x = themes)) #sets up the \"canvas\" of the plot with axis labels\n\n\n\n\n\n\n\n\n\n# ???\nggplot(abby, aes(x = themes)) +\n    geom_bar() # Adds the bars- Any Problem you notice?\n\n\n\n\n\n\n\n\n\n# ???\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) #Rotates the x axis labels\n\n\n\n\n\n\n\n\n\n# ???\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme_classic() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) # Changes the visual theme of the plot with a white background and removes gridlines",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-4-exploring-sentiment",
    "href": "activities/02-foundations-univariate.html#exercise-4-exploring-sentiment",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 4: Exploring sentiment",
    "text": "Exercise 4: Exploring sentiment\nWe’ll look at the distribution of the bing_pos sentiment variable and associated summary statistics.\n\n\n\n\nThe code below creates a boxplot of this variable. In the comment, make note of how this code is similar to the code for the barplot above. As in the previous exercise, before running the code chunk to create the plot, make note of what you expect the boxplot to look like.\n\n\n# ???\nggplot(abby, aes(x = bing_pos)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nChallenge: Using the code for the barplot and boxplot as a guide, try to make a histogram and a density plot of the overall average ratings.\n\nWhat information is given by the tallest bar of the histogram?\nHow would you describe the shape of the distribution?\n\n\n\n# Histogram\n\n# Density plot\n\n\nWe can compute summary statistics (numerical summaries) for a quantitative variable using the summary() function or with the summarize() function from the dplyr package. (1st Qu. and 3rd Qu. stand for first and third quartile.) After inspecting these summaries, look back to your boxplot, histogram, and density plot. Which plots show which summaries most clearly?\n\n\n# Summary statistics\n# Using summary() - convenient for computing many summaries in one command\n# Does not show the standard deviation\nabby %&gt;% \n    select(bing_pos) %&gt;% \n    summary()\n##     bing_pos     \n##  Min.   :0.0000  \n##  1st Qu.:0.1667  \n##  Median :0.3333  \n##  Mean   :0.3650  \n##  3rd Qu.:0.5000  \n##  Max.   :1.0000  \n##  NA's   :19\n\n# Using summarize() from dplyr\n# Note that we use %&gt;% to pipe the data into the summarize() function\n# We need to use na.rm = TRUE because there are missing values (NAs)\nabby %&gt;% \n    summarize(mean(bing_pos, na.rm = TRUE), median(bing_pos, na.rm = TRUE), sd(bing_pos, na.rm = TRUE))\n## # A tibble: 1 × 3\n##   `mean(bing_pos, na.rm = TRUE)` median(bing_pos, na.rm…¹ sd(bing_pos, na.rm =…²\n##                            &lt;dbl&gt;                    &lt;dbl&gt;                  &lt;dbl&gt;\n## 1                          0.365                    0.333                  0.279\n## # ℹ abbreviated names: ¹​`median(bing_pos, na.rm = TRUE)`,\n## #   ²​`sd(bing_pos, na.rm = TRUE)`\n\n\nWrite a good paragraph describing the information in the histogram (or density plot) by discussing shape, center, spread, and outliers. Incorporate the numerical summaries from part c.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#pause-math-box",
    "href": "activities/02-foundations-univariate.html#pause-math-box",
    "title": "Univariate Visualization & Summaries",
    "section": "Pause: Math box",
    "text": "Pause: Math box\nBelow is an example of a “math box” which summarizes the formulas for some of the numerical summaries above. You are not required to memorize, nor will you be assessed on, any formulas presented in this or any future math box. They serve 3 purposes:\n\nTo emphasize that there’s “math” / a formal structure behind what we’re doing.\nTo provide students that plan to continue studying Statistics a glimpse into the formal statistical theory they’ll explore in later courses.\nTo make happy the students that are simply interested in math!\n\n\n\n\n\n\n\n\n\n\nMATH BOX: Univariate numerical summaries\n\n\n\nLet \\((y_1, y_2, ..., y_n)\\) be a sample of \\(n\\) data points.\nmean: \\[\\overline{y} = \\frac{y_1 + y_2 + \\cdots + y_n}{n} = \\frac{\\sum_{i=1}^n y_i}{n}\\]\nvariance: \\[\\text{var}(y) = \\frac{(y_1 - \\overline{y})^2 + (y_2 - \\overline{y})^2 + \\cdots + (y_n - \\overline{y})^2}{n - 1} = \\frac{\\sum_{i=1}^n (y_i - \\overline{y})^2}{n - 1}\\]\nstandard deviation: \\[\\text{sd}(y) = \\sqrt{\\text{var}(y)}\\]",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-5-box-plots-vs.-histograms-vs.-density-plots",
    "href": "activities/02-foundations-univariate.html#exercise-5-box-plots-vs.-histograms-vs.-density-plots",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 5: Box plots vs. histograms vs. density plots",
    "text": "Exercise 5: Box plots vs. histograms vs. density plots\nWe took 3 different approaches to plotting the quantitative average course variable above. They all have pros and cons.\n\nWhat is one pro about the boxplot in comparison to the histogram and density plot?\nWhat is one con about the boxplot in comparison to the histogram and density plots?\nIn this example, which plot do you prefer and why?",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-6-returning-to-our-context-looking-ahead",
    "href": "activities/02-foundations-univariate.html#exercise-6-returning-to-our-context-looking-ahead",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 6: Returning to our context, looking ahead",
    "text": "Exercise 6: Returning to our context, looking ahead\nIn this activity, we explored data on Dear Abby question, with a focus on exploring a single variable at a time.\n\nIn big picture terms, what have we learned about Dear Abby questions?\nWhat further curiosities do you have about the data?",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-7-different-ways-to-think-about-data-visualization",
    "href": "activities/02-foundations-univariate.html#exercise-7-different-ways-to-think-about-data-visualization",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 7: Different ways to think about data visualization",
    "text": "Exercise 7: Different ways to think about data visualization\nIn working with and visualizing data, it’s important to keep in mind what a data point represents. It can reflect the experience of a real person. It might reflect the sentiment in a piece of art. It might reflect history. We’ve taken one very narrow and technical approach to data visualization. Check out the following examples, and write some notes about anything you find interesting.\n\nDear Data\nW.E.B. DuBois\nDecolonizing Data Viz",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-8-rendering-your-work",
    "href": "activities/02-foundations-univariate.html#exercise-8-rendering-your-work",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 8: Rendering your work",
    "text": "Exercise 8: Rendering your work\nSave this file, and then click the “Render” button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\n\nScroll through and inspect the document to see how your work was translated into this HTML format. Neat!\nClose the browser tab.\nGo to the “Background Jobs” pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your “Activities” subfolder within your “STAT155” folder and locate the HTML file. You can open it again in your browser to double check.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#reflection",
    "href": "activities/02-foundations-univariate.html#reflection",
    "title": "Univariate Visualization & Summaries",
    "section": "Reflection",
    "text": "Reflection\nGo to the top of this file and review the learning objectives for this lesson. Which objectives do you have a good handle on, are at least familiar with, or are struggling with? What feels challenging right now? What are some wins from the day?\n\nResponse: Put your response here.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#advice-make-an-r-code-cheat-sheet",
    "href": "activities/02-foundations-univariate.html#advice-make-an-r-code-cheat-sheet",
    "title": "Univariate Visualization & Summaries",
    "section": "Advice: make an R code “cheat sheet”!",
    "text": "Advice: make an R code “cheat sheet”!\nYou will continue to pick up new R code and ideas. You’re highly encouraged to start tracking this in a cheat sheet (eg: in a Google doc). The cheat sheet will be a handy reference for you, and the act of making it will help deepen your understanding and retention.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-9-read-in-and-get-to-know-the-weather-data",
    "href": "activities/02-foundations-univariate.html#exercise-9-read-in-and-get-to-know-the-weather-data",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 9: Read in and get to know the weather data",
    "text": "Exercise 9: Read in and get to know the weather data\nDaily weather data are available for 3 locations in Perth, Australia.\n\nView the codebook here.\nComplete the code below to read in the data.\n\n\n# Replace the ??? with your own name for the weather data\n# Replace the ___ with the correct function\n??? &lt;- ___(\"https://mac-stat.github.io/data/weather_3_locations.csv\")\n## Error in parse(text = input): &lt;text&gt;:3:5: unexpected assignment\n## 2: # Replace the ___ with the correct function\n## 3: ??? &lt;-\n##        ^",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-10-exploring-the-data-structure",
    "href": "activities/02-foundations-univariate.html#exercise-10-exploring-the-data-structure",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 10: Exploring the data structure",
    "text": "Exercise 10: Exploring the data structure\nCheck out the basic features of the weather data.\n\n# Examine the first six cases\n\n# Find the dimensions of the data\n\nWhat does a case represent in this data?",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-11-exploring-rainfall",
    "href": "activities/02-foundations-univariate.html#exercise-11-exploring-rainfall",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 11: Exploring rainfall",
    "text": "Exercise 11: Exploring rainfall\nThe raintoday variable contains information about rainfall.\n\nIs this variable quantitative or categorical?\nCreate an appropriate visualization, and compute appropriate numerical summaries.\nWhat do you learn about rainfall in Perth?\n\n\n# Visualization\n\n# Numerical summaries",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-12-exploring-temperature",
    "href": "activities/02-foundations-univariate.html#exercise-12-exploring-temperature",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 12: Exploring temperature",
    "text": "Exercise 12: Exploring temperature\nThe maxtemp variable contains information on the daily high temperature.\n\nIs this variable quantitative or categorical?\nCreate an appropriate visualization, and compute appropriate numerical summaries.\nWhat do you learn about high temperatures in Perth?\n\n\n# Visualization\n\n# Numerical summaries",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-13-customizing-challenge",
    "href": "activities/02-foundations-univariate.html#exercise-13-customizing-challenge",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 13: Customizing! (CHALLENGE)",
    "text": "Exercise 13: Customizing! (CHALLENGE)\nThough you will naturally absorb some RStudio code throughout the semester, being an effective statistical thinker and “programmer” does not require that we memorize all code. That would be impossible! In contrast, using the foundation you built today, do some digging online to learn how to customize your visualizations.\n\nFor the histogram below, add a title and more meaningful axis labels. Specifically, title the plot “Distribution of max temperatures in Perth”, change the x-axis label to “Maximum temperature” and y-axis label to “Number of days”. HINT: Do a Google search for something like “add axis labels ggplot”.\n\n\n# Add a title and axis labels\nggplot(weather, aes(x = maxtemp)) + \n    geom_histogram()\n## Error: object 'weather' not found\n\n\nAdjust the code below in order to color the bars green. NOTE: Color can be an effective tool, but here it is simply gratuitous.\n\n\n# Make the bars green\nggplot(weather, aes(x = raintoday)) + \n    geom_bar()\n## Error: object 'weather' not found\n\n\nCheck out the ggplot2 cheat sheet. Try making some of the other kinds of univariate plots outlined there.\nWhat else would you like to change about your plot? Try it!",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-14-optional-challenge",
    "href": "activities/02-foundations-univariate.html#exercise-14-optional-challenge",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 14: Optional challenge",
    "text": "Exercise 14: Optional challenge\nAt the top of this activity, we searched for words related to some topics of interest (parents, marriage, money) and combined them into a single theme variable. It looked something like this:\n\nabby_new &lt;- abby %&gt;% \n  mutate(\n    parents = str_detect(question_only, \"mother|mama|mom|father|papa|dad\"),\n    marriage = str_detect(question_only, \"marriage|marry|married\"),\n    money = str_detect(question_only, \"money|finance\")\n  ) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    themes = c(\n      if (parents) \"parents\",\n      if (marriage) \"marriage\",\n      if (money) \"money\"\n    ) %&gt;% paste(collapse = \", \"),\n    themes = ifelse(themes == \"\", \"other\", themes)\n  ) %&gt;%\n  ungroup()\n\nCheck it out:\n\nhead(abby_new)\n## # A tibble: 6 × 12\n##    year month day   question_only     bing_pos afinn_overall afinn_pos afinn_neg\n##   &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1  2017 Aug   30    \"i moved to the …    0.75             14        16         2\n## 2  2017 Aug   30    \"under what circ…   NA                NA        NA        NA\n## 3  2017 Aug   28    \"i'm not a dog p…    0.333             5         5         0\n## 4  2017 Aug   28    \"my 62-year-old …    0.143           -11         8        19\n## 5  2017 Aug   27    \"i have a friend…    0.222             0         7         7\n## 6  2017 Aug   27    \"i have been sel…    0.333            -5         2         7\n## # ℹ 4 more variables: themes &lt;chr&gt;, parents &lt;lgl&gt;, marriage &lt;lgl&gt;, money &lt;lgl&gt;\n\n\nUnderstand the code!\n\nInside mutate() the line parents = str_detect(question_only, \"mother|mama|mom|father|papa|dad\") created a new variable called parents. This variable takes on TRUE or FALSE. Explain what TRUE and FALSE mean here.\nThe themes variable combines the information from the parents, marriage, and money variables. Check out the themes for the first 3 rows / data points. Convince yourself that you understand how it corresponds to the parents, marriage, and money variables.\n\nBeyond parents, marriage, and money, what are some other topics that might pop up in the Dear Abby letters (and that you’re interested in exploring)? Modify the code below to explore those topics! Update the themes variable accordingly.\n\n\nabby_new &lt;- abby %&gt;% \n  mutate(\n    parents = str_detect(question_only, \"mother|mama|mom|father|papa|dad\"),\n    marriage = str_detect(question_only, \"marriage|marry|married\"),\n    money = str_detect(question_only, \"money|finance\")\n  ) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    themes = c(\n      if (parents) \"parents\",\n      if (marriage) \"marriage\",\n      if (money) \"money\"\n    ) %&gt;% paste(collapse = \", \"),\n    themes = ifelse(themes == \"\", \"other\", themes)\n  ) %&gt;%\n  ungroup()\n\n# Check out the raw data\nhead(abby_new)\n## # A tibble: 6 × 12\n##    year month day   question_only     bing_pos afinn_overall afinn_pos afinn_neg\n##   &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1  2017 Aug   30    \"i moved to the …    0.75             14        16         2\n## 2  2017 Aug   30    \"under what circ…   NA                NA        NA        NA\n## 3  2017 Aug   28    \"i'm not a dog p…    0.333             5         5         0\n## 4  2017 Aug   28    \"my 62-year-old …    0.143           -11         8        19\n## 5  2017 Aug   27    \"i have a friend…    0.222             0         7         7\n## 6  2017 Aug   27    \"i have been sel…    0.333            -5         2         7\n## # ℹ 4 more variables: themes &lt;chr&gt;, parents &lt;lgl&gt;, marriage &lt;lgl&gt;, money &lt;lgl&gt;\n\n# Check out the number of letters belonging to each theme\nabby_new %&gt;% \n  count(themes)\n## # A tibble: 8 × 2\n##   themes                       n\n##   &lt;chr&gt;                    &lt;int&gt;\n## 1 marriage                    75\n## 2 marriage, money              5\n## 3 money                       21\n## 4 other                      234\n## 5 parents                    127\n## 6 parents, marriage           33\n## 7 parents, marriage, money     4\n## 8 parents, money              15",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-1-importing-and-getting-to-know-the-data-1",
    "href": "activities/02-foundations-univariate.html#exercise-1-importing-and-getting-to-know-the-data-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 1: Importing and getting to know the data",
    "text": "Exercise 1: Importing and getting to know the data\n\nNote how clicking the abby data causes both a popup pane and the command View(abby) to appear in the Console. In fact, the View() function is the underlying command that opens a dataset pane. (View() should always be entered in the Console and NOT your Quarto document.)\nEach row / case corresponds to a single question.\nColumns = variables\nTry out each function below. Identify what each function tells you about the abby data and note this in the ???:\n\n\n# First number = number of rows / cases\n# Second number = number of columns / variables\ndim(abby)\n## [1] 514   6\n\n# Number of rows (cases)\nnrow(abby)\n## [1] 514\n\n# Number of columns (variables)\nncol(abby)\n## [1] 6\n\n# View first few rows of the dataset (6 rows, by default)\nhead(abby)\n## # A tibble: 6 × 6\n##    year month day   question_only                                bing_pos themes\n##   &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt;                                           &lt;dbl&gt; &lt;chr&gt; \n## 1  2017 Aug   30    \"i moved to the philippines five years ago.…    0.75  paren…\n## 2  2017 Aug   30    \"under what circumstances do you ask your a…   NA     money \n## 3  2017 Aug   28    \"i'm not a dog person. i'm not even an anim…    0.333 other \n## 4  2017 Aug   28    \"my 62-year-old father has recently started…    0.143 paren…\n## 5  2017 Aug   27    \"i have a friend, \\\"charlene,\\\" whom i met …    0.222 other \n## 6  2017 Aug   27    \"i have been selected to attend a symposium…    0.333 other\n\n# Get all column (variable) names\nnames(abby)\n## [1] \"year\"          \"month\"         \"day\"           \"question_only\"\n## [5] \"bing_pos\"      \"themes\"\n\n\nWe can display the first 10 rows with head(abby, n = 10).",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-2-preparing-to-summarize-and-visualize-the-data-1",
    "href": "activities/02-foundations-univariate.html#exercise-2-preparing-to-summarize-and-visualize-the-data-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 2: Preparing to summarize and visualize the data",
    "text": "Exercise 2: Preparing to summarize and visualize the data\n\nThe sentiment variables are afinn_overall, afinn_pos, afinn_neg, and bing_pos, and they are quantitative. The afinn variables don’t have units but we can still get a sense of the scale by remembering that each word gets a score between -5 and 5. The bing_pos variable doesn’t have units because it’s a fraction, but we know that it ranges from 0 to 1.\ncategorical\nAppropriate visualizations:\n\nsingle quantitative variable: boxplot, histogram, density plot\nsingle categorical variable: barplot",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-3-exploring-themes-in-the-letters-1",
    "href": "activities/02-foundations-univariate.html#exercise-3-exploring-themes-in-the-letters-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 3: Exploring themes in the letters",
    "text": "Exercise 3: Exploring themes in the letters\n\nExpectations about the plot will vary\n\n\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\nCounts in the table below match the barplot\n\n\n# Construct a table of counts\nabby %&gt;% \n    count(themes)\n## # A tibble: 8 × 2\n##   themes                       n\n##   &lt;chr&gt;                    &lt;int&gt;\n## 1 marriage                    75\n## 2 marriage, money              5\n## 3 money                       21\n## 4 other                      234\n## 5 parents                    127\n## 6 parents, marriage           33\n## 7 parents, marriage, money     4\n## 8 parents, money              15\n\n\nWhat do the plot layers do?\n\n\n# Just sets up the \"canvas\" of the plot with axis labels\nggplot(abby, aes(x = themes))\n\n\n\n\n\n\n\n\n\n# Adds the bars\nggplot(abby, aes(x = themes)) +\n    geom_bar()\n\n\n\n\n\n\n\n\n\n# Rotates the x axis labels\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\n# Changes the visual theme of the plot with a white background and removes gridlines\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme_classic() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-4-exploring-sentiment-1",
    "href": "activities/02-foundations-univariate.html#exercise-4-exploring-sentiment-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 4: Exploring sentiment",
    "text": "Exercise 4: Exploring sentiment\n\n\n\n\nggplot(abby, aes(x = bing_pos)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nWe replace geom_boxplot() with geom_histogram() and geom_density().\n\n\n# Histogram\nggplot(abby, aes(x = bing_pos)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\n# Density plot\nggplot(abby, aes(x = bing_pos)) +\n    geom_density()\n\n\n\n\n\n\n\n\n\n\nBoxplot shows min, max, median, 1st and 3rd quartile easily. (It shows median, 1st and 3rd quartile directly as lines)\nHistogram and density plot show min and max but the mean and median aren’t shown directly–we have to roughly guess based on the peak of the distribution\n\n\n\n# Summary statistics\nabby %&gt;% \n    select(bing_pos) %&gt;% \n    summary()\n##     bing_pos     \n##  Min.   :0.0000  \n##  1st Qu.:0.1667  \n##  Median :0.3333  \n##  Mean   :0.3650  \n##  3rd Qu.:0.5000  \n##  Max.   :1.0000  \n##  NA's   :19\n\nabby %&gt;% \n    summarize(mean(bing_pos, na.rm = TRUE), median(bing_pos, na.rm = TRUE), sd(bing_pos, na.rm = TRUE))\n## # A tibble: 1 × 3\n##   `mean(bing_pos, na.rm = TRUE)` median(bing_pos, na.rm…¹ sd(bing_pos, na.rm =…²\n##                            &lt;dbl&gt;                    &lt;dbl&gt;                  &lt;dbl&gt;\n## 1                          0.365                    0.333                  0.279\n## # ℹ abbreviated names: ¹​`median(bing_pos, na.rm = TRUE)`,\n## #   ²​`sd(bing_pos, na.rm = TRUE)`\n\n\nThe distribution of sentiment scores is roughly tri-modal, ranging from 0 to 1. There’s a group of very negative letters (with roughly 0% of words being positive), a group of very positive letters (with roughly 100% of words being positive), and a group of mostly negative letters. The typical sentiment is around 0.34.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-5-box-plots-vs.-histograms-vs.-density-plots-1",
    "href": "activities/02-foundations-univariate.html#exercise-5-box-plots-vs.-histograms-vs.-density-plots-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 5: Box plots vs. histograms vs. density plots",
    "text": "Exercise 5: Box plots vs. histograms vs. density plots\n\nBoxplots very clearly show key summary statistics like median, 1st and 3rd quartile\nBoxplots can oversimplify by not showing the shape of the distribution.",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-6-returning-to-our-context-looking-ahead-1",
    "href": "activities/02-foundations-univariate.html#exercise-6-returning-to-our-context-looking-ahead-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 6: Returning to our context, looking ahead",
    "text": "Exercise 6: Returning to our context, looking ahead\n\nAnswers will vary",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-9-read-in-and-get-to-know-the-weather-data-1",
    "href": "activities/02-foundations-univariate.html#exercise-9-read-in-and-get-to-know-the-weather-data-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 9: Read in and get to know the weather data",
    "text": "Exercise 9: Read in and get to know the weather data\n\nweather &lt;- read_csv(\"https://raw.githubusercontent.com/Mac-STAT/data/main/weather_3_locations.csv\")",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-10-exploring-the-data-structure-1",
    "href": "activities/02-foundations-univariate.html#exercise-10-exploring-the-data-structure-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 10: Exploring the data structure",
    "text": "Exercise 10: Exploring the data structure\nCheck out the basic features of the weather data.\n\n# Examine the first six cases\nhead(weather)\n## # A tibble: 6 × 24\n##   date       location  mintemp maxtemp rainfall evaporation sunshine windgustdir\n##   &lt;date&gt;     &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      \n## 1 2020-01-01 Wollongo…    17.1    23.1        0          NA       NA SSW        \n## 2 2020-01-02 Wollongo…    17.7    24.2        0          NA       NA SSW        \n## 3 2020-01-03 Wollongo…    19.7    26.8        0          NA       NA NE         \n## 4 2020-01-04 Wollongo…    20.4    35.5        0          NA       NA SSW        \n## 5 2020-01-05 Wollongo…    19.8    21.4        0          NA       NA SSW        \n## 6 2020-01-06 Wollongo…    18.3    22.9        0          NA       NA NE         \n## # ℹ 16 more variables: windgustspeed &lt;dbl&gt;, winddir9am &lt;chr&gt;, winddir3pm &lt;chr&gt;,\n## #   windspeed9am &lt;dbl&gt;, windspeed3pm &lt;dbl&gt;, humidity9am &lt;dbl&gt;,\n## #   humidity3pm &lt;dbl&gt;, pressure9am &lt;dbl&gt;, pressure3pm &lt;dbl&gt;, cloud9am &lt;dbl&gt;,\n## #   cloud3pm &lt;dbl&gt;, temp9am &lt;dbl&gt;, temp3pm &lt;dbl&gt;, raintoday &lt;chr&gt;,\n## #   risk_mm &lt;dbl&gt;, raintomorrow &lt;chr&gt;\n\n# Find the dimensions of the data\ndim(weather)\n## [1] 2367   24\n\nA case represents a day of the year in a particular area (Hobart, Uluru, Wollongong as seen by the location variable).",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-11-exploring-rainfall-1",
    "href": "activities/02-foundations-univariate.html#exercise-11-exploring-rainfall-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 11: Exploring rainfall",
    "text": "Exercise 11: Exploring rainfall\nThe raintoday variable contains information about rainfall.\n\nraintoday is categorical (No, Yes)\nIt is more common to have no rain.\n\n\n# Visualization\nggplot(weather, aes(x = raintoday)) +\n    geom_bar()\n\n\n\n\n\n\n\n\n# Numerical summaries\nweather %&gt;% \n    count(raintoday)\n## # A tibble: 3 × 2\n##   raintoday     n\n##   &lt;chr&gt;     &lt;int&gt;\n## 1 No         1864\n## 2 Yes         446\n## 3 &lt;NA&gt;         57",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-12-exploring-temperature-1",
    "href": "activities/02-foundations-univariate.html#exercise-12-exploring-temperature-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 12: Exploring temperature",
    "text": "Exercise 12: Exploring temperature\nThe maxtemp variable contains information on the daily high temperature.\n\nmaxtemp is quantitative\nThe typical max temperature is around 23 degrees Celsius (with an average of 23.62 and a median of 22 degrees). The max temperatures ranged from 8.6 to 45.4 degrees. Finally, on the typical day, the max temp falls about 7.8 degrees from the mean. There are multiple modes in the distribution of max temperature—this likely reflects the different cities in the dataset.\n\n\n# Visualization\nggplot(weather, aes(x = maxtemp)) + \n    geom_histogram()\n\n\n\n\n\n\n\n\n# Numerical summaries\nsummary(weather$maxtemp)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n##    8.60   18.10   22.00   23.62   27.40   45.40      34\n\n# There are missing values (NAs) in this variable, so we add\n# the na.rm = TRUE argument\nweather %&gt;% \n    summarize(sd(maxtemp, na.rm = TRUE))\n## # A tibble: 1 × 1\n##   `sd(maxtemp, na.rm = TRUE)`\n##                         &lt;dbl&gt;\n## 1                        7.80",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "activities/02-foundations-univariate.html#exercise-13-customizing-challenge-1",
    "href": "activities/02-foundations-univariate.html#exercise-13-customizing-challenge-1",
    "title": "Univariate Visualization & Summaries",
    "section": "Exercise 13: Customizing! (CHALLENGE)",
    "text": "Exercise 13: Customizing! (CHALLENGE)\n\n\n\n\nggplot(weather, aes(x = maxtemp)) + \n    geom_histogram() + \n    labs(x = \"Maximum temperature\", y = \"Number of days\", title = \"Distribution of max temperatures in Perth\")\n\n\n\n\n\n\n\n\n\n\n\n\n# Make the bars green\nggplot(weather, aes(x = raintoday)) + \n    geom_bar(fill = \"green\")",
    "crumbs": [
      "Univariate Visualization & Summaries"
    ]
  },
  {
    "objectID": "R_Resources.html",
    "href": "R_Resources.html",
    "title": "R and RStudio Resources",
    "section": "",
    "text": "Crowd-sourced R functions\nInteractive RStudio tutorials (R basics, data viz, and more)\nMarkdown (Quarto) basics, and Quarto/RStudio tutorial\nRStudio cheatsheets\nGoogle it! If you have a question about R, someone else probably asked it. Add “R tidyverse” to your search term.\nExample: scatterplot R tidyverse\nNote about ChatGPT for code: Please avoid using ChatGPT as a first resort for R code. It often suggests overly wordy or unfamiliar approaches. Our goal is for you to understand the code you write; sticking to tools and syntax we use in class will support your learning best."
  },
  {
    "objectID": "R_Resources.html#getting-help",
    "href": "R_Resources.html#getting-help",
    "title": "R and RStudio Resources",
    "section": "",
    "text": "Crowd-sourced R functions\nInteractive RStudio tutorials (R basics, data viz, and more)\nMarkdown (Quarto) basics, and Quarto/RStudio tutorial\nRStudio cheatsheets\nGoogle it! If you have a question about R, someone else probably asked it. Add “R tidyverse” to your search term.\nExample: scatterplot R tidyverse\nNote about ChatGPT for code: Please avoid using ChatGPT as a first resort for R code. It often suggests overly wordy or unfamiliar approaches. Our goal is for you to understand the code you write; sticking to tools and syntax we use in class will support your learning best."
  },
  {
    "objectID": "R_Resources.html#getting-started",
    "href": "R_Resources.html#getting-started",
    "title": "R and RStudio Resources",
    "section": "Getting Started",
    "text": "Getting Started\nThe videos below introduce R and RStudio and show how to install them on your computer. Start with the overview, then watch the step-by-step video for your operating system.\nDownload links (mentioned in the videos): - R: http://cran.r-project.org/ - RStudio: https://www.rstudio.com/products/rstudio/download/#download\n\n\n\n\n\n\nImportant\n\n\n\nVersion numbers in the videos are out-of-date. Please download the latest versions of both R and RStudio. As of August 25, 2025, the latest versions are: R 4.5.1 and RStudio 2025.05.1+513.\n\n\n\nInstallation / Overview\n\nInstalling R and RStudio (Overview) (Length: 4:30)\n\nPick one of the following: - Mac — Installing R and RStudio Step-by-Step (Length: 3:24) - Windows — Installing R and RStudio Step-by-Step (Length: 5:43)\nIf you run into installation issues, contact me right away. If we can’t get RStudio working locally, you can use Macalester’s online RStudio server:\nQuick intro video"
  },
  {
    "objectID": "R_Resources.html#intro-videos-to-r-watch-in-order",
    "href": "R_Resources.html#intro-videos-to-r-watch-in-order",
    "title": "R and RStudio Resources",
    "section": "Intro Videos to R (watch in order)",
    "text": "Intro Videos to R (watch in order)\n\nIntro to R and RStudio (11:31)\nR Data Types (8:05)\nR Error Messages and Troubleshooting (Optional) (7:52)\nIntro to RMarkdown / Quarto (9:33)\nFile Structure & Organization (Optional)\n\nMac (5:13)\nWindows (7:44)\n\nTips on File Naming & Organization (Optional)\nR Packages (7:30)\n\n\nYou can adjust video speed and pause as needed. If anything doesn’t work, please let me know so we can fix it quickly."
  },
  {
    "objectID": "activities/01_foundations_welcome.html",
    "href": "activities/01_foundations_welcome.html",
    "title": "Collecting and Summarizing Data",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#see-class-notes",
    "href": "activities/01_foundations_welcome.html#see-class-notes",
    "title": "Collecting and Summarizing Data",
    "section": "See class notes",
    "text": "See class notes\nGo to your class-notes to see what we covered in live-note taking!\n\n\n\nEXAMPLE 1: Tidy data (Class Activities!)\nWelcome to the first in-class activities of STAT 155! Fill out the Day 1 Activities form of the following questions (anonymous). We’ll come back to this in a future class. Wait until everyone in your group is done with this.\n\nHow many hours of sleep did you get last night?\nHow many cups of coffee did you drink this morning?\nWhat is your declared or potential major? (If you are a double major, just pick whichever one you think of first.)\nWhat is your anticipated graduation year?\nHow many stats/data science courses have you taken in the past?\nOn a scale of 1 (get me out of here) to 10 (yay!), how excited are you about this course?\nIs it your birthday this semester? (yes/no)\nHow many unread emails do you have in your inbox right now?",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-1-use-r-as-a-calculator",
    "href": "activities/01_foundations_welcome.html#example-1-use-r-as-a-calculator",
    "title": "Collecting and Summarizing Data",
    "section": "Example 1: Use R as a calculator",
    "text": "Example 1: Use R as a calculator\nType the following lines in the console (bottom left), one by one, hitting Return/Enter after each line. In some cases you might even get an error! This error is important to learning how R code does and doesn’t work.\n\n4 + 2\n\n\n4^2\n\n\n4*2\n\n\n4(2)",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-2-functions-and-arguments",
    "href": "activities/01_foundations_welcome.html#example-2-functions-and-arguments",
    "title": "Collecting and Summarizing Data",
    "section": "Example 2: Functions and arguments",
    "text": "Example 2: Functions and arguments\nWe can also use built-in functions to perform common tasks. These functions have names and require information about arguments in order to run:\nfunction(argument) Cheatcode: RiceCooker(Rice)\nTry out the following functions one by one in the RStudio console. For each function, note its…\n\nname\nthe argument or information it needs to run\nwhat output it produces (what the function does)\nhow the name connects to what the function does\n\n\nsqrt(9)\n\n\nnchar(\"macalester\")\n\n\nsqrt(nchar(\"snow\"))\n\nSome functions have more than 1 argument, separated by commas:\nfunction(argument1 = ___, argument2 = ___) Cheatcode: RiceCooker(Rice,Chicken)\nTry out the following, one by one.\n\nrep(x = 2, times = 5)\n\n\nrep(times = 5, x = 2)\n\n\nrep(2, 5)\n\n\nrep(5, 2)\n\nFinally, R is case sensitive. Try using Rep() instead of rep(). Take time to read the error message!\n\nRep(5, 2)",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-3-save-it-for-later",
    "href": "activities/01_foundations_welcome.html#example-3-save-it-for-later",
    "title": "Collecting and Summarizing Data",
    "section": "Example 3: Save it for later",
    "text": "Example 3: Save it for later\nWe’ll often want to store some R output for later use. In R:\nname &lt;- output\nwhere name is the name under which to store a result, output is the result we wish to store, and &lt;- is the assignment operator (I think of this as an arrow pointing the output into the name).\nIMPORTANT: Try out each line one at a time. Why doesn’t the first line produce any output?\n\ndegrees_c &lt;- -13\n\n\ndegrees_c\n\n\ndegrees_c * (9/5) + 32",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-4-import-data",
    "href": "activities/01_foundations_welcome.html#example-4-import-data",
    "title": "Collecting and Summarizing Data",
    "section": "Example 4: Import data",
    "text": "Example 4: Import data\nNext, let’s work with some data!! The first step is importing our data into RStudio. How we do this depends on:\n\nfile format (eg: .xls Excel spreadsheet, .csv, .txt)\nfile location (eg: online, on your desktop, built into RStudio itself).\n\nThe data from the survey you took before class is stored as a .csv file online. Import this data using the read_csv() function, and store it as survey using the code below:\nFirst, in the Console pane of RStudio, run the following command to install some necessary packages (you will need to do this any time you are installing a new package):\ninstall.packages(\"tidyverse\")\n\n# Load the \"tidyverse\" package which contains the read_csv() function\nlibrary(tidyverse)\n\n# Import the data\nsurvey &lt;- read_csv(\"https://mac-stat.github.io/data/112_fall_2024_survey.csv\")\n\n\n\nCheck out the data\nIn the Environment tab in the upper right pane of RStudio, click on survey. What happens?!\n\n\nIn the modern era, datasets often contain hundreds of variables and millions of observations. We need more effective ways to explore such data.",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-5-get-to-know-the-data",
    "href": "activities/01_foundations_welcome.html#example-5-get-to-know-the-data",
    "title": "Collecting and Summarizing Data",
    "section": "Example 5: Get to know the data",
    "text": "Example 5: Get to know the data\nPAUSE: Make sure you’re still in sync with your group.\nBefore we can learn anything from our data, we must understand its structure. For each function below:\n\ntry it out\ndiscuss with your group what the function does\ndiscuss with your group how the function’s name connects to what it does\n\n\ndim(survey) # (Number of row (case/obs.), Number of column (variable))\n\n\nnrow(survey) # Number of case/obs.\n\n\nncol(survey) # Number of variables\n\n\nhead(survey) # View first few rows of the dataset (6 rows, by default)\n\n\nhead(survey, 3) # Controlling the view of first few rows of the dataset\n\n\ntail(survey) # View first few rows of the dataset (6 rows, by default)\n\n\nnames(survey) # Get all column (variable) names\n\n\nstr(survey) # Overall info about data",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-6-code-communication",
    "href": "activities/01_foundations_welcome.html#example-6-code-communication",
    "title": "Collecting and Summarizing Data",
    "section": "Example 6 : Code = communication",
    "text": "Example 6 : Code = communication\nIt’s important to recognize from day 1 that code is a form of communication, both to yourself and others!!!!! Code structure and details are important to readability and clarity, just as grammar, punctuation, spelling, paragraphs, and line spacing are important in written essays. All of the code below works, but has bad structure. With your group, discuss what is unfortunate about each line, then make it better.\n\nseq(from=1, to=9, by=2)\nseq(from = 1, to=9, by=2)\ntemp_cel &lt;- -13\nthisisthetemperaturetodayincelsius &lt;- -13\nthis_is_the_temperature_today_in_celsius &lt;- -13",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-7-you-will-make-so-many-mistakes",
    "href": "activities/01_foundations_welcome.html#example-7-you-will-make-so-many-mistakes",
    "title": "Collecting and Summarizing Data",
    "section": "Example 7: You will make so many mistakes!",
    "text": "Example 7: You will make so many mistakes!\nMistakes are common when, and even important to, learning any new language. You’ll get better and better at interpreting error messages, finding help, and fixing errors. In addition to finding help online, R has built-in help files. For example:\n\nIn the console, type ?rep and press Return/Enter.\nCheck out the documentation file that pops up in the Help tab (lower right).\nQuickly scroll through, noting the type of information provided.\nPause at the “Examples” section at the bottom – perhaps the most useful section! Try out a couple of the provided examples in your console.",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#example-8-make-a-cheat-sheet",
    "href": "activities/01_foundations_welcome.html#example-8-make-a-cheat-sheet",
    "title": "Collecting and Summarizing Data",
    "section": "Example 8: Make a “cheat sheet”",
    "text": "Example 8: Make a “cheat sheet”\nYou will continue to pick up new R code and ideas. You’re highly encouraged to start tracking this in a cheat sheet (eg: in a Google doc). The cheat sheet will be a handy reference for you, and the act of making it will help deepen your understanding and retention.",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#exercise-complete-this-after-the-class",
    "href": "activities/01_foundations_welcome.html#exercise-complete-this-after-the-class",
    "title": "Collecting and Summarizing Data",
    "section": "Exercise: Complete this after the class",
    "text": "Exercise: Complete this after the class\nComplete this exercise after class. First, try it on your own (or with your group), and then check your work against the solution provided at the end of this .qmd file.\nUse R code to do the following:\n\nImport & name data on different Himalayan peaks from the url below:\nhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/peaks.csv NOTE: A codebook, i.e. a description of the data, is here.\nUse a function to show which variables are recorded on each peak.\nHow many peaks are included in the dataset? Answer this using a function, not by counting up the rows yourself.\nShow the first 6 rows of the dataset. NOTE: This gives us a quick glimpse without having to print out the entire dataset!",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#exercise-1-use-r-as-a-calculator",
    "href": "activities/01_foundations_welcome.html#exercise-1-use-r-as-a-calculator",
    "title": "Collecting and Summarizing Data",
    "section": "Exercise 1: Use R as a calculator",
    "text": "Exercise 1: Use R as a calculator\n\n4 + 2\n## [1] 6\n4^2\n## [1] 16\n4*2\n## [1] 8\n#4(2) # We need to use * for multiplication",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#exercise-2-functions-and-arguments",
    "href": "activities/01_foundations_welcome.html#exercise-2-functions-and-arguments",
    "title": "Collecting and Summarizing Data",
    "section": "Exercise 2: Functions and arguments",
    "text": "Exercise 2: Functions and arguments\n\n# Calculate the square root of 9\nsqrt(9)\n## [1] 3\n\n# Calculate the number of characters in the word \"macalester\"\nnchar(\"macalester\")\n## [1] 10\n\n# Calculate the square root of the number of characters in the word \"snow\"\nsqrt(nchar(\"snow\"))\n## [1] 2\n\n\n# Repeat the number 2, 5 times\nrep(x = 2, times = 5)\n## [1] 2 2 2 2 2\n\n# Repeat the number 2, 5 times\nrep(times = 5, x = 2)\n## [1] 2 2 2 2 2\n\n# Repeat the number 2, 5 times\nrep(2, 5)\n## [1] 2 2 2 2 2\n\n# Repeat the number 5, 2 times\nrep(5, 2)\n## [1] 5 5",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#exercise-3-save-it-for-later",
    "href": "activities/01_foundations_welcome.html#exercise-3-save-it-for-later",
    "title": "Collecting and Summarizing Data",
    "section": "Exercise 3: Save it for later",
    "text": "Exercise 3: Save it for later\n\n# Nothing shows up -- all we're doing here is storing -13 as degrees_c\ndegrees_c &lt;- -13\n\n# Print the contents of degrees_c\ndegrees_c\n## [1] -13\n\n# We can \"do math\" with the contents of degrees_c\ndegrees_c * (9/5) + 32\n## [1] 8.6",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#exercise-4-import-data",
    "href": "activities/01_foundations_welcome.html#exercise-4-import-data",
    "title": "Collecting and Summarizing Data",
    "section": "Exercise 4: Import data",
    "text": "Exercise 4: Import data\n\n# Load the tidyverse package\nlibrary(tidyverse)\n\n# Import the data\nsurvey &lt;- read_csv(\"https://mac-stat.github.io/data/112_fall_2024_survey.csv\")",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#exercise-5-get-to-know-the-data",
    "href": "activities/01_foundations_welcome.html#exercise-5-get-to-know-the-data",
    "title": "Collecting and Summarizing Data",
    "section": "Exercise 5: Get to know the data",
    "text": "Exercise 5: Get to know the data\n\n# Dimensions of the survey data set\n# First number = number of rows\n# Second number = number of columns\ndim(survey)\n## [1] 98  4\n\n\n# Number of rows in the survey data set\nnrow(survey)\n## [1] 98\n\n\n# First 6 rows (the head) of the survey data set\nhead(survey)\n## # A tibble: 6 × 4\n##   cafe_mac     minutes_to_campus fave_temp hangout      \n##   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;        \n## 1 Cheesecake                  15        18 the mountains\n## 2 Cheese pizza                10        24 a beach      \n## 3 udon noodles                 4        18 the mountains\n## 4 egg rolls                    7        10 a beach      \n## 5 Tacos                        5        18 the mountains\n## 6 pasta                       35         7 the mountains\n\n\n# First 3 rows of the survey data set\nhead(survey, 3)\n## # A tibble: 3 × 4\n##   cafe_mac     minutes_to_campus fave_temp hangout      \n##   &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;        \n## 1 Cheesecake                  15        18 the mountains\n## 2 Cheese pizza                10        24 a beach      \n## 3 udon noodles                 4        18 the mountains\n\n\n# Last 6 rows (the tail) of the survey data set\ntail(survey)\n## # A tibble: 6 × 4\n##   cafe_mac        minutes_to_campus fave_temp hangout \n##   &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   \n## 1 Burger                         10        21 a forest\n## 2 Pepperoni Pizza                10        24 a beach \n## 3 Hamburger                       6        23 a beach \n## 4 Ginger Cookies                 10        26 a beach \n## 5 bbq chicken                     5        14 a city  \n## 6 Breakfast food                 15        25 a city\n\n\n# Names of the variables in the survey data set\nnames(survey)\n## [1] \"cafe_mac\"          \"minutes_to_campus\" \"fave_temp\"        \n## [4] \"hangout\"\n\n\n# Structure of all variables in the survey data set\nstr(survey)\n## spc_tbl_ [98 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n##  $ cafe_mac         : chr [1:98] \"Cheesecake\" \"Cheese pizza\" \"udon noodles\" \"egg rolls\" ...\n##  $ minutes_to_campus: num [1:98] 15 10 4 7 5 35 5 15 7 20 ...\n##  $ fave_temp        : num [1:98] 18 24 18 10 18 7 75 24 13 16 ...\n##  $ hangout          : chr [1:98] \"the mountains\" \"a beach\" \"the mountains\" \"a beach\" ...\n##  - attr(*, \"spec\")=\n##   .. cols(\n##   ..   cafe_mac = col_character(),\n##   ..   minutes_to_campus = col_double(),\n##   ..   fave_temp = col_double(),\n##   ..   hangout = col_character()\n##   .. )\n##  - attr(*, \"problems\")=&lt;externalptr&gt;",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#exercise-6-code-communication",
    "href": "activities/01_foundations_welcome.html#exercise-6-code-communication",
    "title": "Collecting and Summarizing Data",
    "section": "Exercise 6: Code = communication",
    "text": "Exercise 6: Code = communication\n\n# Make it less smooshy. Add spaces!\nseq(from = 1, to = 9, by = 2)\n## [1] 1 3 5 7 9\n\n# Use consistent spacing\nseq(from = 1, to = 9, by = 2)\n## [1] 1 3 5 7 9\n\n# Use more descriptive names when storing objects\nmy_output &lt;- -13\n\n# Use a shorter and easier to read name\ncelsius_today &lt;- -13\nCelsiusToday  &lt;- -13",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#exercise-7-you-will-make-so-many-mistakes",
    "href": "activities/01_foundations_welcome.html#exercise-7-you-will-make-so-many-mistakes",
    "title": "Collecting and Summarizing Data",
    "section": "Exercise 7: You will make so many mistakes!",
    "text": "Exercise 7: You will make so many mistakes!",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/01_foundations_welcome.html#exercise-8-your-turn",
    "href": "activities/01_foundations_welcome.html#exercise-8-your-turn",
    "title": "Collecting and Summarizing Data",
    "section": "Exercise 8: Your turn",
    "text": "Exercise 8: Your turn\n\n# a\npeaks &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/peaks.csv\")\n\n# b\nnames(peaks)\n## [1] \"peak_id\"                    \"peak_name\"                 \n## [3] \"peak_alternative_name\"      \"height_metres\"             \n## [5] \"climbing_status\"            \"first_ascent_year\"         \n## [7] \"first_ascent_country\"       \"first_ascent_expedition_id\"\n\n# c\ndim(peaks)\n## [1] 468   8\nnrow(peaks)\n## [1] 468\n\n# d\nhead(peaks)\n## # A tibble: 6 × 8\n##   peak_id peak_name     peak_alternative_name height_metres climbing_status\n##   &lt;chr&gt;   &lt;chr&gt;         &lt;chr&gt;                         &lt;dbl&gt; &lt;chr&gt;          \n## 1 AMAD    Ama Dablam    Amai Dablang                   6814 Climbed        \n## 2 AMPG    Amphu Gyabjen &lt;NA&gt;                           5630 Climbed        \n## 3 ANN1    Annapurna I   &lt;NA&gt;                           8091 Climbed        \n## 4 ANN2    Annapurna II  &lt;NA&gt;                           7937 Climbed        \n## 5 ANN3    Annapurna III &lt;NA&gt;                           7555 Climbed        \n## 6 ANN4    Annapurna IV  &lt;NA&gt;                           7525 Climbed        \n## # ℹ 3 more variables: first_ascent_year &lt;dbl&gt;, first_ascent_country &lt;chr&gt;,\n## #   first_ascent_expedition_id &lt;chr&gt;",
    "crumbs": [
      "Collecting and Summarizing Data"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html",
    "href": "activities/03_04-slr-intro-formalization.html",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#learning-goals",
    "href": "activities/03_04-slr-intro-formalization.html#learning-goals",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nVisualize and describe the relationship between two quantitative variables using a scatterplot\nWrite R code to create a scatterplot and compute the linear correlation between two quantitative variables\nDescribe/identify weak / strong, and positive / negative correlation from a point cloud\nBuild intuition for fitting lines to quantify the relationship between two quantitative variables\nDifferentiate between a response / outcome variable and a predictor / explanatory variable\nWrite a model formula for a simple linear regression model with a quantitative predictor\nWrite R code to fit a linear regression model\nInterpret the intercept and slope coefficients in a simple linear regression model with a quantitative predictor\nCompute expected / predicted / fitted values and residuals from a linear regression model formula\nInterpret predicted values and residuals in the context of the data\nExplain the connection between residuals and the least squares criterion",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#readings-and-videos",
    "href": "activities/03_04-slr-intro-formalization.html#readings-and-videos",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Readings and videos",
    "text": "Readings and videos\nChoose either the reading or the videos to go through after class. CP Quiz due on Wednesday at 09:00 am on these topics\n\nReading: Sections 2.8, 3.1-3.3, 3.6 in the STAT 155 Notes\nVideos:\n\nSimple linear regression Part 1: motivation & scatterplots\nSimple linear regression Part 2: correlation\nSimple linear regression Part 3: simple linear regression models\nR Code for Fitting a Linear Model (Time: 11:07)\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\nCreate a new code chunk to look at the first few rows of the data and learn how much data (in terms of cases and variables) we have.\n\nWhat does a case represent?\nHow many and what kinds of variables do we have?\nThinking about the who, what, when, where, why, and how of this data, which of the 5W’s + H seem most relevant to our investigations? Explain your thoughts.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-2-get-to-know-the-outcomeresponse-variable",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-2-get-to-know-the-outcomeresponse-variable",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 2: Get to know the outcome/response variable",
    "text": "Exercise 2: Get to know the outcome/response variable\nLet’s get acquainted with the riders_registered variable.\n\nConstruct an appropriate plot to visualize the distribution of this variable, and compute appropriate numerical summaries.\nWrite a good paragraph interpreting the pclot and numerical summaries.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#a-little-bit-of-live-note-taking",
    "href": "activities/03_04-slr-intro-formalization.html#a-little-bit-of-live-note-taking",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "A little-bit of Live Note Taking!",
    "text": "A little-bit of Live Note Taking!",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#before-starting-the-exercise-lets-do-the-followings-first",
    "href": "activities/03_04-slr-intro-formalization.html#before-starting-the-exercise-lets-do-the-followings-first",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Before starting the Exercise, let’s do the followings first!",
    "text": "Before starting the Exercise, let’s do the followings first!\nMost of you should have figured out the followings by now, still let’s skim through the followings!\n\nFirst, save this activity in your device as STAT 155 -&gt; activities -&gt; copy-paste 03_04.qmd\nClick on Render button! What happens? ::: {.callout-tip title=“Answers”} The html has saved in the same location (folder) where you initially saved the .qmd file! You need to submit .html file like this for the PP! :::\nNow, click on the +C at the top right side of RStudio and then choose R. What happens?\n\n\n\n\n\n\n\nAnswers\n\n\n\nIt creates code chunks",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-3-explore-the-relationship-between-ridership-and-temperature",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-3-explore-the-relationship-between-ridership-and-temperature",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 3: Explore the relationship between ridership and temperature",
    "text": "Exercise 3: Explore the relationship between ridership and temperature\nWe’d like to understand how daily ridership among registered users relates with the temperature that it feels like that day (temp_feel).\n\nWhat type of plot would be appropriate to visualize this relationship? Sketch and describe what you expect this plot to look like.\nCreate an appropriate plot using ggplot(). How does the plot compare to what you predicted?\n\n\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) +\n    geom_point()\n\n\n\n\n\n\n\n\nType-in the followings in the.qmd file you have been working from the last class!\n\n\n\n\n\n\nComments\n\n\n\nTrend: Linear (?); Direction/Association: Positive/Negative, Strength: spread of the points (dispersed? close together? moderately close together?); Outlier?\n\n\n\nAdd the following two lines after your plot to add a linear (blue) and curved (red) smoothing line. What do you notice? Is a simple linear regression model appropriate for this data?\n\n\n# Add a red straight line of best fit and a blue curve of best fit\nYOUR_PLOT +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    geom_smooth(color = \"blue\", se = FALSE)\n\nWhat do you think “eval = TRUE/FALSE” doing here {r eval = TRUE/FALSE}?\n\n# Add a red straight line of best fit and a blue curve of best fit\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    geom_smooth(color = \"blue\", se = FALSE)\n\n\n\n\n\n\n\n\nComments: If we only displayed the red line of best fit on the plot, we might miss the slight downward trend at the highest temperatures that we can see more clearly with the blue curve of best fit. A linear model is not appropriate if fit to the whole range of the data, but there does seem to be a linear relationship between ridership and temperature below 80 degrees Fahrenheit.\n\nCompute Correlation of temp_feel and riders_registered.\n\n\nCorrelation\nWe can quantify the linear relationship between two quantitative variables using a numerical summary known as correlation (sometimes known as a “correlation coefficient” or “Pearson’s correlation”). Correlation can range from -1 to 1, where a correlation of 0 indicates that there is no linear relationship between the two quantitative variables.\nBelow is an example of a “Math Box”. You’ll see these occasionally throughout the activities. You are not required to memorize, nor will you be assessed on, anything in the math boxes. If you plan on continuing with Statistics courses at Macalester (or are interested in the math behind everything!), these math boxes are for you!\n\n\n\n\n\n\nCorrelation\n\n\n\n\n\nThe Pearson correlation coefficient, \\(r_{x, y}\\), of \\(x\\) and \\(y\\) is the (almost) average of products of the z-scores of variables \\(x\\) and \\(y\\):\n\\[\nr_{x, y} = \\frac{\\sum z_x z_y}{n - 1}\n\\]\n\n\n\nIn general, we will want to be able to describe (qualitatively) two aspects of correlation:\n\nStrength\n\n\nIs the correlation between x and y strong, or weak, i.e. how closely do the points fit around a line? This has to do with how dispersed our point clouds are.\n\n\nDirection\n\n\nIs the correlation between x and y positive or negative, i.e. does y go “up” when x goes “up” (positive), or does y go “down” when x goes “up” (negative)?\n\nStronger correlations will be further from 0 (closer to -1 or 1), and positive and negative correlations will have the appropriate respective sign (above or below zero).\nWhiteboard Time (a little-bit!)\n\n# correlation\n\n# Note: the order in which you put your two quantitative variables into the cor\n# function doesn't matter! Try switching them around to confirm this for yourself\nbikes %&gt;%\n    summarize(cor(temp_feel, riders_registered))\n## # A tibble: 1 × 1\n##   `cor(temp_feel, riders_registered)`\n##                                 &lt;dbl&gt;\n## 1                               0.544",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#see-class-notes-slr-formalization",
    "href": "activities/03_04-slr-intro-formalization.html#see-class-notes-slr-formalization",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "See class-notes: SLR Formalization",
    "text": "See class-notes: SLR Formalization",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-4-filtering-our-data",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-4-filtering-our-data",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 4: Filtering our data",
    "text": "Exercise 4: Filtering our data\nThe relationship between registered riders and temperature looks linear below 80 degrees. We can use the filter() function from the dplyr package to subset our cases. (We’ll learn techniques soon for handling this nonlinear relationship.)\nIf we wanted to only keep cases where registered ridership was greater than 2000, we would use the following code:\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (bikes data is \"fed into\" the filter() function)\nNEW_DATASET_NAME &lt;- bikes %&gt;% \n    filter(riders_registered &gt; 2000)\n\nAdapt the example above to create a new dataset called bikes_sub that only keeps cases where the felt temperature is less than 80 degrees.\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (bikes data is \"fed into\" the filter() function)\nbikes_sub &lt;- bikes %&gt;% \n    filter(temp_feel &lt; 80)\n\nDid it work? Check the dimensions of bikes and bikes_sub!",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-5-model-fitting-and-coefficient-interpretation",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-5-model-fitting-and-coefficient-interpretation",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 5: Model fitting and coefficient interpretation",
    "text": "Exercise 5: Model fitting and coefficient interpretation\nLet’s fit a simple linear regression model and examine the results. Step through code chunk slowly, and make note of new code.\n\n# Construct and save the model as bike_mod\n# What's the purpose of \"riders_registered ~ temp_feel\"?\n# What's the purpose of \"data = bikes_sub\"?\nbike_mod &lt;- lm(riders_registered ~ temp_feel, data = bikes_sub)\n\n\n# A long summary of the model stored in bike_mod\nsummary(bike_mod)\n## \n## Call:\n## lm(formula = riders_registered ~ temp_feel, data = bikes_sub)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3681.8  -928.3   -98.6   904.9  3496.7 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -2486.412    421.379  -5.901 7.37e-09 ***\n## temp_feel      86.493      6.464  13.380  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1267 on 428 degrees of freedom\n## Multiple R-squared:  0.2949, Adjusted R-squared:  0.2933 \n## F-statistic:   179 on 1 and 428 DF,  p-value: &lt; 2.2e-16\n\n\n# A simplified model summary\ncoef(summary(bike_mod))\n##                Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept) -2486.41180 421.379174 -5.900652 7.368345e-09\n## temp_feel      86.49251   6.464247 13.380135 2.349753e-34\n\n\nUsing the model summary output, complete the following model formula:\nE[riders_registered | temp_feel] = ___ + ___ * temp_feel\nIntercept interpretation: On days that feel like 0 degrees Fahrenheit, we can expect an average of -2486.41180 riders—a negative number of riders doesn’t make sense! This results because of extrapolation—0 degrees is so far below the minimum temperature in the data. We only have information on the relationship between ridership and temperature in the ~40-100 degree range and have no idea what that relationship looks like outside that range.\nSlope interpretation: Every 1 degree increase in feeling temperature is associated with an average of about 86 more riders.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-6-predictions-and-residuals",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-6-predictions-and-residuals",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 6: Predictions and residuals",
    "text": "Exercise 6: Predictions and residuals\nOn August 17, 2012, the temp_feel was 53.816 degrees and there were 5665 riders. We can get data for this day using the filter() and select() dplyr functions. Note, but don’t worry about the syntax – we haven’t learned this yet:\n\nbikes_sub %&gt;% \n    filter(date == \"2012-08-17\") %&gt;% \n    select(riders_registered, temp_feel) \n## # A tibble: 1 × 2\n##   riders_registered temp_feel\n##               &lt;dbl&gt;     &lt;dbl&gt;\n## 1              5665      53.8\n\n\nPeak back at the scatterplot. More riders than expected – the point is far above the trend line.\n\n\nggplot(bikes_sub, aes(x = temp_feel, y = riders_registered)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) \n\n\n\n\n\n\n\n\n\nUse your model formula from the previous exercise to predict the ridership on August 17, 2012 from the temperature on that day. (That is, where do days with this temperature fall on the model trend line? How many registered riders would we expect on a 53.816 degree day?)\n\n\n\n\n\n\n\nAnswer\n\n\n\n-2486.41180 + 86.49251 * 53.816 = 2168.269\n\n\n\nCheck your part b calculation using the predict() function. Take careful note of the syntax – there’s a lot going on!\n\n\n# What is the purpose of newdata = ___???\npredict(bike_mod, newdata = data.frame(temp_feel = 53.816))\n##        1 \n## 2168.269\n\n\nCalculate the residual or prediction error. How far does the observed ridership fall from the model prediction?\n\n\n\n\n\n\n\nAnswer\n\n\n\nresidual = observed y - predicted y = 5665 - 2168.269 = 3496.731\n\n\n\nAre positive residuals above or below the trend line? When we have positive residuals, does the model over- or under-estimate ridership? Repeat these questions for negative residuals.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\nPositive residuals are above the trend line—we under-estimate ridership.\nNegative residuals are below the trend line—we over-estimate ridership.\n\n\n\n\nFor an 85 degree day, how many registered riders would we expect? Do you think it’s a good idea to make this prediction? (Revisit the visualization and filtering we did in Exercises 3 and 4.) [Complete after the class!]",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-7-changing-temperature-units-challenge-complete-after-the-class",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-7-changing-temperature-units-challenge-complete-after-the-class",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 7: Changing temperature units (CHALLENGE) [Complete after the class!]",
    "text": "Exercise 7: Changing temperature units (CHALLENGE) [Complete after the class!]\nSuppose we had measured temperature in degrees Celsius rather than degrees Fahrenheit. How do you think our intercept and slope estimates, and their coefficient interpretations, would change?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#render-your-work-again-this-is-a-good-practice-to-render-often--to-see-which-part-of-the-paragraph-might-cause-non-rendering-if-applicable-complete-after-the-class",
    "href": "activities/03_04-slr-intro-formalization.html#render-your-work-again-this-is-a-good-practice-to-render-often--to-see-which-part-of-the-paragraph-might-cause-non-rendering-if-applicable-complete-after-the-class",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Render your work Again (this is a good practice to render often- to see which part of the paragraph might cause non-rendering, if applicable!) [Complete after the class!]",
    "text": "Render your work Again (this is a good practice to render often- to see which part of the paragraph might cause non-rendering, if applicable!) [Complete after the class!]\n\nClick the “Render” button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\nScroll through and inspect the document to check that your work translated to the HTML format correctly.\nClose the browser tab.\nGo to the “Background Jobs” pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your “Activities” subfolder within your “STAT155” folder and locate the HTML file. You can open it again in your browser to double check.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-8-ridership-and-windspeed",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-8-ridership-and-windspeed",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 8: Ridership and windspeed",
    "text": "Exercise 8: Ridership and windspeed\nLet’s pull together everything that you’ve practiced in the preceding exercises to investigate the relationship between riders_registered and windspeed. Go back to using the bikes dataset (instead of bikes_sub) because we no longer need to only keep days less than 80 degrees.\n\n# Construct and interpret a visualization of this relationship\n# Include a representation of the relationship trend\n\n\n# Use lm to construct a model of riders_registered vs windspeed\n# Save this as bike_mod2\n\n\n# Get a short summary of this model\n\n\nSummarize your observations from the visualizations.\nWrite out a formula for the model trend.\nInterpret both the intercept and the windspeed coefficient. (Note: What does a negative slope indicate?)\nUse this model to predict the ridership on August 17, 2012 and calculate the corresponding residual. (Note: You’ll first need to find the windspeed on this date!)",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-9-data-drills-filter-select-summarize-complete-after-class",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-9-data-drills-filter-select-summarize-complete-after-class",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 9: Data drills (filter, select, summarize) [Complete after class]",
    "text": "Exercise 9: Data drills (filter, select, summarize) [Complete after class]\nThis exercise is designed to help you keep building your dplyr skills. These skills are important to data cleaning and digging, which in turn is important to really making meaning of our data. We’ll work with a simpler set of 10 data points:\n\nnew_bikes &lt;- bikes %&gt;% \n    select(date, temp_feel, humidity, riders_registered, day_of_week) %&gt;% \n    head(10)\n\n\nVerb 1: summarize\nThus far, in the dplyr grammar you’ve seen 3 verbs or action words: summarize(), select(), filter(). Try out the following code and then summarize the point of the summarize() function:\n\nnew_bikes %&gt;% \n    summarize(mean(temp_feel), mean(humidity))\n## # A tibble: 1 × 2\n##   `mean(temp_feel)` `mean(humidity)`\n##               &lt;dbl&gt;            &lt;dbl&gt;\n## 1              52.0            0.544\n\n\n\nVerb 2: select\nTry out the following code and then summarize the point of the select() function:\n\nnew_bikes %&gt;%\n    select(date, temp_feel)\n## # A tibble: 10 × 2\n##    date       temp_feel\n##    &lt;date&gt;         &lt;dbl&gt;\n##  1 2011-01-01      64.7\n##  2 2011-01-02      63.8\n##  3 2011-01-03      49.0\n##  4 2011-01-04      51.1\n##  5 2011-01-05      52.6\n##  6 2011-01-06      53.0\n##  7 2011-01-07      50.8\n##  8 2011-01-08      46.6\n##  9 2011-01-09      42.5\n## 10 2011-01-10      45.6\n\n\nnew_bikes %&gt;% \n    select(-date, -temp_feel)\n## # A tibble: 10 × 3\n##    humidity riders_registered day_of_week\n##       &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n##  1    0.806               654 Sat        \n##  2    0.696               670 Sun        \n##  3    0.437              1229 Mon        \n##  4    0.590              1454 Tue        \n##  5    0.437              1518 Wed        \n##  6    0.518              1518 Thu        \n##  7    0.499              1362 Fri        \n##  8    0.536               891 Sat        \n##  9    0.434               768 Sun        \n## 10    0.483              1280 Mon\n\n\n\nVerb 3: filter\nTry out the following code and then summarize the point of the filter() function:\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850)\n## # A tibble: 7 × 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-03      49.0    0.437              1229 Mon        \n## 2 2011-01-04      51.1    0.590              1454 Tue        \n## 3 2011-01-05      52.6    0.437              1518 Wed        \n## 4 2011-01-06      53.0    0.518              1518 Thu        \n## 5 2011-01-07      50.8    0.499              1362 Fri        \n## 6 2011-01-08      46.6    0.536               891 Sat        \n## 7 2011-01-10      45.6    0.483              1280 Mon\n\n\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sat\")\n## # A tibble: 2 × 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-01      64.7    0.806               654 Sat        \n## 2 2011-01-08      46.6    0.536               891 Sat\n\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850, day_of_week == \"Sat\")\n## # A tibble: 1 × 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-08      46.6    0.536               891 Sat",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-10-your-turn-complete-after-the-class",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-10-your-turn-complete-after-the-class",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 10: Your turn [Complete after the class!]",
    "text": "Exercise 10: Your turn [Complete after the class!]\nUse dplyr verbs to complete each task below.\n\n# Keep only information about the humidity and day of week\n\n# Keep only information about the humidity and day of week using a different approach\n\n# Keep only information for Sundays\n\n# Keep only information for Sundays with temperatures below 50\n\n# Calculate the maximum and minimum temperatures",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\nCreate a new code chunk by clicking the green “C” button with a green + sign in the top right of the menu bar. In this code chunk, use an appropriate function to look at the first few rows of the data.\nCreate a new code chunk, and use an appropriate function to learn how much data we have (in terms of cases and variables).\nWhat does a case represent?\nNavigate to the FAQ page and read the response to the “How does this site work? Do you just download results from the federations?” question. What do you learn about data quality and completeness from this response?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-2-mutating-our-data",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-2-mutating-our-data",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 2: Mutating our data",
    "text": "Exercise 2: Mutating our data\nStrength-to-weight ratio (SWR) is defined as TotalKg/BodyweightKg. We can use the mutate() function from the dplyr package to create a new variable in our dataframe for SWR using the following code:\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (lifts data is \"fed into\" the mutate() function).\n# When creating a new variable, we often reassign the data frame to itself,\n# which updates the existing columns in lifts with the additional \"new\" column(s)\n# in lifts!\nlifts &lt;- lifts %&gt;% \n    mutate(NEW_VARIABLE_NAME = Age/BestSquatKg)\n## Error in `mutate()`:\n## ℹ In argument: `NEW_VARIABLE_NAME = Age/BestSquatKg`.\n## Caused by error:\n## ! object 'BestSquatKg' not found\n\nAdapt the example above to create a new variable called SWR, where SWR is defined as TotalKg/BodyweightKg.\n\nlifts &lt;- lifts %&gt;% \n    mutate(SWR = TotalKg / BodyweightKg)",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-3-get-to-know-the-outcomeresponse-variable",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-3-get-to-know-the-outcomeresponse-variable",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 3: Get to know the outcome/response variable",
    "text": "Exercise 3: Get to know the outcome/response variable\nLet’s get acquainted with the SWR variable.\n\nConstruct an appropriate plot to visualize the distribution of this variable, and compute appropriate numerical summaries.\nWrite a good paragraph interpreting the plot and numerical summaries.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-4-data-visualization---two-quantitative-variables",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-4-data-visualization---two-quantitative-variables",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 4: Data visualization - two quantitative variables",
    "text": "Exercise 4: Data visualization - two quantitative variables\nWe’d like to visualize the relationship between body weight and the strength-to-weight ratio. A scatterplot (or informally, a “point cloud”) allows us to do this! The code below creates a scatterplot of body weight vs. SWR using ggplot().\n\n# scatterplot\n\n# The alpha = 0.5 in geom_point() adds transparency to the points\n# to make them easier to see. You can make this smaller for more transparency\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\n\nThis is your second (!) bivariate data visualization (visualization for two variables)! What differences do you notice in the code structure when creating a bivariate visualization, compared to univariate visualizations we’ve worked with before?\nWhat similarities do you notice in the code structure?\nDoes there appear to be some sort of pattern in the structure of the point cloud? Describe it, in no more than three sentences! Comment on the direction of the relationship between the two variables (positive? negative?) and the spread of the points (are they dispersed? close together?).",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-5-scatterplots---patterns-in-point-clouds",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-5-scatterplots---patterns-in-point-clouds",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 5: Scatterplots - patterns in point clouds",
    "text": "Exercise 5: Scatterplots - patterns in point clouds\nSometimes, it can be easier to see a pattern in a point cloud by adding a smoothing line to our scatterplots. The code below adapts the code in Exercise 4 to do this:\n\n# scatterplot with smoothing line\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nLook back at your answer to Exercise 4 (c). Does the smoothing line assist you in seeing a pattern, or change your answer at all? Why or why not?\nBased on the scatterplot with the smoothing line added above, does there appear to be a linear relationship between body weight and SWR (i.e. would a straight line do a decent job at summarizing the relationship between these two variables)? Why or why not?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-6-correlation",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-6-correlation",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 6: Correlation",
    "text": "Exercise 6: Correlation\nWe can quantify the linear relationship between two quantitative variables using a numerical summary known as correlation (sometimes known as a “correlation coefficient” or “Pearson’s correlation”). Correlation can range from -1 to 1, where a correlation of 0 indicates that there is no linear relationship between the two quantitative variables.\nBelow is an example of a “Math Box”. You’ll see these occasionally throughout the activities. You are not required to memorize, nor will you be assessed on, anything in the math boxes. If you plan on continuing with Statistics courses at Macalester (or are interested in the math behind everything!), these math boxes are for you!\n\n\n\n\n\n\nCorrelation\n\n\n\n\n\nThe Pearson correlation coefficient, \\(r_{x, y}\\), of \\(x\\) and \\(y\\) is the (almost) average of products of the z-scores of variables \\(x\\) and \\(y\\):\n\\[\nr_{x, y} = \\frac{\\sum z_x z_y}{n - 1}\n\\]\n\n\n\nIn general, we will want to be able to describe (qualitatively) two aspects of correlation:\n\nStrength\n\n\nIs the correlation between x and y strong, or weak, i.e. how closely do the points fit around a line? This has to do with how dispersed our point clouds are.\n\n\nDirection\n\n\nIs the correlation between x and y positive or negative, i.e. does y go “up” when x goes “up” (positive), or does y go “down” when x goes “up” (negative)?\n\nStronger correlations will be further from 0 (closer to -1 or 1), and positive and negative correlations will have the appropriate respective sign (above or below zero).\n\nRather than a smooth trend line, we can force the line we add to our scatterplots to be linear using geom_smooth(method = 'lm'), as below:\n\n\n# scatterplot with linear trend line\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\nBased on the above scatterplot, how would you describe the correlation between body weight and SWR, in terms of strength and direction?\nMake a guess as to what numerical value the correlation between body weight and SWR will have, based on your response to part (b).",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-7-computing-correlation-in-r",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-7-computing-correlation-in-r",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 7: Computing correlation in R",
    "text": "Exercise 7: Computing correlation in R\nWe can compute the correlation between body weight and SWR using summarize and cor functions:\n\n# correlation\n\n# Note: the order in which you put your two quantitative variables into the cor\n# function doesn't matter! Try switching them around to confirm this for yourself\n# Because of the missing data, we need to include the use = \"complete.obs\" - otherwise the correlation would be computed as NA\nlifts %&gt;%\n    summarize(cor(SWR, BodyweightKg, use = \"complete.obs\"))\n## # A tibble: 1 × 1\n##   `cor(SWR, BodyweightKg, use = \"complete.obs\")`\n##                                            &lt;dbl&gt;\n## 1                                        -0.0392\n\nIs the computed correlation close to what you guessed in Exercise 6 part (c)?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-8-limitations-of-correlation",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-8-limitations-of-correlation",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 8: Limitations of correlation",
    "text": "Exercise 8: Limitations of correlation\nWe previously noted that correlation was a numerical summary of the linear relationship between two variables. We’ll now go through some examples of relationships between quantitative variables to demonstrate why it is incredibly important to visualize our data in addition to just computing numerical summaries!\nFor this exercise, we’ll be working with the anscombe dataset, which is built in to R. To load this dataset into our environment, we run the following code:\n\n# load anscombe data\ndata(\"anscombe\")\n\nThe anscombe dataset contains four different pairs of quantitative variables:\n\nx1, y1\nx2, y2\nx3, y3\nx4, y4\n\nAdapt the code we used in Exercise 7 to compute the correlation between each of these four pairs of variables, below:\n\n# correlation between x1, y1\n\n# correlation between x2, y2\n\n# correlation between x3, y3\n\n# correlation between x4, y4\n\n\nWhat do you notice about each of these correlations (if the answer to this isn’t obvious, double-check your code)?\nDescribe these correlations in terms of strength and direction, using only the numerical summary to assist you in your description.\nDraw an example on the white board or at your tables of what you think the point clouds for these pairs of variables might look like. There are only 11 observations, so you can draw all 11 points if you’d like!\nAdapt the code for scatterplots given previously in this activity to make four distinct scatterplots for each pair of quantitative variables in the anscombe dataset. You do not need to add a smooth trend line or a linear trend line to these plots.\n\n\n# scatterplot: x1, y1\n\n# scatterplot: x2, y2\n\n# scatterplot: x3, y3\n\n# scatterplot: x4, y4\n\n\nBased on the correlations you calculated and scatterplots you made, what is the message of this last exercise as it relates to the limits of correlation?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#reflection",
    "href": "activities/03_04-slr-intro-formalization.html#reflection",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Reflection",
    "text": "Reflection\nMuch of statistics is about making (hopefully) reasonable assumptions in attempt to summarize observed relationships in data. Today we started considering assumptions of linear relationships between quantitative variables.\nReview the learning objectives at the top of this file and today’s activity. How do you imagine assumptions of linearity might be useful in terms of quantifying relationships between quantitative variables? How do you imagine these assumptions could sometimes fall short, or even be unethical in certain cases?\n\nResponse: Put your response here.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-9-lines-of-best-fit",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-9-lines-of-best-fit",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 9: Lines of best fit",
    "text": "Exercise 9: Lines of best fit\nIn this activity, we’ve learned how to fit straight lines to data, to help us visualize the relationship between two quantitative variables. So far, ggplot has chosen the line for us. How does it know which line is “best”, and what does “best” even mean?\nFor this exercise, we’ll consider the relationship between x1 and y1 in the anscombe dataset. Run the following code, which creates a scatterplot with a fitted line to our data using the function geom_abline:\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_point() +\n  geom_abline(slope = 0.4, intercept = 3, col = \"blue\", size = 1)\n\n\n\n\n\n\n\n\nDescribe the line that you see. Do you think the line is “good”? What are you using to define “good”?\nSome things to think about:\n\nHow many points are above the line?\nHow many points are below the line?\nAre the distances of the points above and below the line roughly similar, or is there meaningful difference?\n\nNow we’ll add another line to our plot. Which line do you think is better suited for this data? Why? Be specific!\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_point() +\n  geom_abline(slope = 0.4, intercept = 3, col = \"blue\", size = 1) +\n  geom_abline(slope = 0.5, intercept = 4, col = \"orange\", size = 1)\n\n\n\n\n\n\n\n\nIt’s usually quite simple to note when a line is bad, but more difficult to quantify when a line is a good fit for our data. Consider the following line:\n\n# scatterplot with a fitted line, whose slope is 0.4 and intercept is 3\nanscombe %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_point() +\n  geom_abline(slope = -0.5, intercept = 10, col = \"red\", size = 1) \n\n\n\n\n\n\n\n\nIn the next activity, we’ll formalize the principle of least squares, which will give us one particular definition of a line of best fit that is commonly used in statistics! We’ll take advantage of the vertical distances between each point and the fitted line (residuals), which will help us define (mathematically) a line that best fits our data:\n\nlibrary(broom)\nanscombe %&gt;%\n  lm(y1 ~ x1, data = .) %&gt;%\n  augment() %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_segment(aes(xend = x1, yend = .fitted), col = \"red\") +\n  geom_point()",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-10-correlation-and-extreme-values",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-10-correlation-and-extreme-values",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 10: Correlation and extreme values",
    "text": "Exercise 10: Correlation and extreme values\nIn this exercise, we’ll explore how correlation changes with the addition of extreme values, or observations. We’ll begin by generating a toy dataset called dat with two quantitative variables, x and y. Run the code below to create the dataset.\nwhile not required, recall that you can look up function documentation in R using the ? in front of a function name to figure out what that function is doing!\n\n# create a toy dataset\nset.seed(1234)\nx &lt;- rnorm(100, mean = 5, sd = 2)\ny &lt;- -3 * x + rnorm(100, sd = 4)\ndat &lt;- data.frame(x = x, y = y)\n\n\nMake a scatterplot of x vs. y.\n\n\n# scatterplot\n\n\nBased on your scatterplot, describe the correlation between x and y in terms of strength and direction.\nGuess the correlation (the numerical value) between x and y.\nCompute the correlation between x and y. Was your guess from part (c) close?\n\n\n# correlation\n\n\nSuppose we observe an additional observation with x = 15 and y = -45. We can create a new data frame, dat_new1, that contains this observation in addition to the original ones as follows:\n\n\n# creating dat_new1\nx1 &lt;- c(x, 15)\ny1 &lt;- c(y, -45)\ndat_new1 &lt;- data.frame(x = x1, y = y1)\n\n\nMake a scatterplot of x vs. y for this new data frame, and compute the correlation between x and y. Did your correlation change very much with the addition of this observation? Hypothesize why or why not.\n\n\n# scatterplot\n\n# correlation\n\n\nSuppose instead of our additional observation having values x = 15 and y = -45, we instead observe x = 15 and y = -15. We can create a new data frame, dat_new2, that contains this observation in addition to the original ones as follows:\n\n\n# creating dat_new1\nx2 &lt;- c(x, 15)\ny2 &lt;- c(y, 45)\ndat_new2 &lt;- data.frame(x = x2, y = y2)\n\n\nMake a scatterplot of x vs. y for this new data frame, and compute the correlation between x and y. Did your correlation change very much with the addition of this observation? Hypothesize why or why not.\n\n\n# scatterplot\n\n# correlation\n\n\nWhat do you think the takeaway message is of this exercise?\n\n\nChallenge Add linear trend lines to your scatterplots from parts (f) and (h). Does this give you any additional insight into why the correlations may have changed in different ways with the addition of a new observation?",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data-2",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data-2",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\ndim(bikes)\n## [1] 731  15\n\nhead(bikes)\n## # A tibble: 6 × 15\n##   date       season  year month day_of_week weekend holiday temp_actual\n##   &lt;date&gt;     &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;       &lt;lgl&gt;   &lt;chr&gt;         &lt;dbl&gt;\n## 1 2011-01-01 winter  2011 Jan   Sat         TRUE    no             57.4\n## 2 2011-01-02 winter  2011 Jan   Sun         TRUE    no             58.8\n## 3 2011-01-03 winter  2011 Jan   Mon         FALSE   no             46.5\n## 4 2011-01-04 winter  2011 Jan   Tue         FALSE   no             46.8\n## 5 2011-01-05 winter  2011 Jan   Wed         FALSE   no             48.7\n## 6 2011-01-06 winter  2011 Jan   Thu         FALSE   no             47.1\n## # ℹ 7 more variables: temp_feel &lt;dbl&gt;, humidity &lt;dbl&gt;, windspeed &lt;dbl&gt;,\n## #   weather_cat &lt;chr&gt;, riders_casual &lt;dbl&gt;, riders_registered &lt;dbl&gt;,\n## #   riders_total &lt;dbl&gt;\n\n\nA case represents a day of the year.\nWe have 15 variables broadly concerning weather, day of week information, whether the day is a holiday.\nLots of answers are reasonable here! When and where seem to be particularly relevant because this is for a rideshare based in Washington DC with data from 2011-2012. Ridership likely changes a lot from city to city and over time.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-2-get-to-know-the-outcomeresponse-variable-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-2-get-to-know-the-outcomeresponse-variable-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 2: Get to know the outcome/response variable",
    "text": "Exercise 2: Get to know the outcome/response variable\nThe distribution of the riders_registered variable looks fairly symmetric. On average there are about 3600 registered riders per day (mean = 3656, median = 3662). On any given day, the number of registered riders is about 1560 from the mean. There seem to be a small number of low outliers (minimum ridership was 20).\n\nggplot(bikes, aes(x = riders_registered)) +\n    geom_histogram()\n\n\n\n\n\n\n\n\nggplot(bikes, aes(y = riders_registered)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nsummary(bikes$riders_registered)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##      20    2497    3662    3656    4776    6946\n\nbikes %&gt;% \n    summarize(sd(riders_registered))\n## # A tibble: 1 × 1\n##   `sd(riders_registered)`\n##                     &lt;dbl&gt;\n## 1                   1560.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-3-explore-the-relationship-between-ridership-and-temperature-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-3-explore-the-relationship-between-ridership-and-temperature-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 3: Explore the relationship between ridership and temperature",
    "text": "Exercise 3: Explore the relationship between ridership and temperature\nWe’d like to understand how daily ridership among registered users relates with the temperature that it feels like that day (temp_feel).\n\nScatterplot (outcome and predictor are both quantitative)\n\n\n\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) +\n    geom_point()\n\n\n\n\n\n\n\n\n\nIf we only displayed the red line of best fit on the plot, we might miss the slight downward trend at the highest temperatures that we can see more clearly with the blue curve of best fit. A linear model is not appropriate if fit to the whole range of the data, but there does seem to be a linear relationship between ridership and temperature below 80 degrees Fahrenheit.\n\n\n# Add a red straight line of best fit and a blue curve of best fit\nggplot(bikes, aes(x = temp_feel, y = riders_registered)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    geom_smooth(color = \"blue\", se = FALSE)",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-4-filtering-our-data-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-4-filtering-our-data-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 4: Filtering our data",
    "text": "Exercise 4: Filtering our data\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (bikes data is \"fed into\" the filter() function)\nbikes_sub &lt;- bikes %&gt;% \n    filter(temp_feel &lt; 80)",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-5-model-fitting-and-coefficient-interpretation-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-5-model-fitting-and-coefficient-interpretation-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 5: Model fitting and coefficient interpretation",
    "text": "Exercise 5: Model fitting and coefficient interpretation\nLet’s fit a simple linear regression model and examine the results. Step through code chunk slowly, and make note of new code.\n\n# Construct and save the model as bike_mod\n# What's the purpose of \"riders_registered ~ temp_feel\"?\n# What's the purpose of \"data = bikes_sub\"?\nbike_mod &lt;- lm(riders_registered ~ temp_feel, data = bikes_sub)\n\n\n# A long summary of the model stored in bike_mod\nsummary(bike_mod)\n## \n## Call:\n## lm(formula = riders_registered ~ temp_feel, data = bikes_sub)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3681.8  -928.3   -98.6   904.9  3496.7 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) -2486.412    421.379  -5.901 7.37e-09 ***\n## temp_feel      86.493      6.464  13.380  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1267 on 428 degrees of freedom\n## Multiple R-squared:  0.2949, Adjusted R-squared:  0.2933 \n## F-statistic:   179 on 1 and 428 DF,  p-value: &lt; 2.2e-16\n\n\n# A simplified model summary\ncoef(summary(bike_mod))\n##                Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept) -2486.41180 421.379174 -5.900652 7.368345e-09\n## temp_feel      86.49251   6.464247 13.380135 2.349753e-34\n\n\nE[riders_registered | temp_feel] = -2486.41180 + 86.49251 * temp_feel\nIntercept interpretation: On days that feel like 0 degrees Fahrenheit, we can expect an average of -2486.41180 riders—a negative number of riders doesn’t make sense! This results because of extrapolation—0 degrees is so far below the minimum temperature in the data. We only have information on the relationship between ridership and temperature in the ~40-100 degree range and have no idea what that relationship looks like outside that range.\nSlope interpretation: Every 1 degree increase in feeling temperature is associated with an average of about 86 more riders.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-6-predictions-and-residuals-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-6-predictions-and-residuals-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 6: Predictions and residuals",
    "text": "Exercise 6: Predictions and residuals\nOn August 17, 2012, the temp_feel was 53.816 degrees and there were 5665 riders. We can get data for this day using the filter() and select() dplyr functions. Note, but don’t worry about the syntax – we haven’t learned this yet:\n\nbikes_sub %&gt;% \n    filter(date == \"2012-08-17\") %&gt;% \n    select(riders_registered, temp_feel) \n## # A tibble: 1 × 2\n##   riders_registered temp_feel\n##               &lt;dbl&gt;     &lt;dbl&gt;\n## 1              5665      53.8\n\n\nMore riders than expected – the point is far above the trend line\n-2486.41180 + 86.49251 * 53.816 = 2168.269\nWe get the same result with predict():\n\n\n# What is the purpose of newdata = ___???\npredict(bike_mod, newdata = data.frame(temp_feel = 53.816))\n##        1 \n## 2168.269\n\n\nresidual = 5665 - 2168.269 = 3496.731. On August 17, 2012, there were 3496.731 more riders than would be expected from our model.\n\nPositive residuals are above the trend line—we under-estimate ridership.\nNegative residuals are below the trend line—we over-estimate ridership.\n\nOn an 85 degree day, we would predict 4865.452 riders. Even though we can compute this prediction, it’s not a good idea because of extrapolation–the data that we used to fit our model was filtered to days less than 80 degrees.\n\n\n-2486.41180 + 86.49251 * 85\n## [1] 4865.452\npredict(bike_mod, newdata = data.frame(temp_feel = 85))\n##        1 \n## 4865.451",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-7-changing-temperature-units-challenge",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-7-changing-temperature-units-challenge",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 7: Changing temperature units (CHALLENGE)",
    "text": "Exercise 7: Changing temperature units (CHALLENGE)\nIf we had measured temperature in degrees Celsius rather than degrees Fahrenheit, both the intercept and slope should change. The intercept would now represent 0 degrees Celsius (32 degrees Fahrenheit) and a one unit change in temperature is now 1 degree Celsius (1.8 degrees Fahrenheit).",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-8-ridership-and-windspeed-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-8-ridership-and-windspeed-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 8: Ridership and windspeed",
    "text": "Exercise 8: Ridership and windspeed\nLet’s pull together everything that you’ve practiced in the preceding exercises to investigate the relationship between riders_registered and windspeed. Go back to using the bikes dataset (instead of bikes_sub) because we no longer need to only keep days less than 80 degrees.\n\n# Construct and interpret a visualization of this relationship\n# Include a representation of the relationship trend\nggplot(bikes, aes(x = windspeed, y = riders_registered)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    geom_smooth(color = \"blue\", se = FALSE)\n\n\n\n\n\n\n\n\n# Use lm to construct a model of riders_registered vs windspeed\n# Save this as bike_mod2\nbike_mod2 &lt;- lm(riders_registered ~ windspeed, data = bikes)\n\n# Get a short summary of this model\ncoef(summary(bike_mod2))\n##               Estimate Std. Error   t value      Pr(&gt;|t|)\n## (Intercept) 4490.09761  149.65992 30.002005 2.023179e-129\n## windspeed    -65.34145   10.86299 -6.015053  2.844453e-09\n\n\nThere’s a weak, negative relationship – ridership tends to be smaller on windier days.\nE[riders_registered | windspeed] = 4490.09761 - 65.34145 windspeed\n\nIntercept: On days with no wind, we’d expect around 4490 riders. (0 is a little below the minimum of the observed data, but not by much! So extrapolation in interpreting the intercept isn’t a huge concern.)\nSlope: Every 1mph increase in windspeed is associated with a ridership decrease of 65 riders on average.\n\nSee the code below to predict ridership on August 17, 2012 and calculate the corresponding residual. Note that this residual is smaller than the residual from the temperature model (that residual was 3496.731). This indicates that August 17 was more of an outlier in ridership given the temperature than the windspeed.\n\n\nbikes %&gt;% \n    filter(date == \"2012-08-17\") %&gt;% \n    select(riders_registered, windspeed)\n## # A tibble: 1 × 2\n##   riders_registered windspeed\n##               &lt;dbl&gt;     &lt;dbl&gt;\n## 1              5665      15.5\n\n# prediction\n4490.09761 - 65.34145 * 15.50072\n## [1] 3477.258\n\n# residual \n5665 - 3477.258\n## [1] 2187.742",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-9-data-drills-filter-select-summarize",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-9-data-drills-filter-select-summarize",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 9: Data drills (filter, select, summarize)",
    "text": "Exercise 9: Data drills (filter, select, summarize)\nThis exercise is designed to help you keep building your dplyr skills. These skills are important to data cleaning and digging, which in turn is important to really making meaning of our data. We’ll work with a simpler set of 10 data points:\n\nnew_bikes &lt;- bikes %&gt;% \n    select(date, temp_feel, humidity, riders_registered, day_of_week) %&gt;% \n    head(10)\n\n\nVerb 1: summarize\nsummarize() calculates numerical summaries of variables (columns).\n\nnew_bikes %&gt;% \n    summarize(mean(temp_feel), mean(humidity))\n## # A tibble: 1 × 2\n##   `mean(temp_feel)` `mean(humidity)`\n##               &lt;dbl&gt;            &lt;dbl&gt;\n## 1              52.0            0.544\n\n\n\nVerb 2: select\nselect() selects variables (columns).\n\nnew_bikes %&gt;%\n    select(date, temp_feel)\n## # A tibble: 10 × 2\n##    date       temp_feel\n##    &lt;date&gt;         &lt;dbl&gt;\n##  1 2011-01-01      64.7\n##  2 2011-01-02      63.8\n##  3 2011-01-03      49.0\n##  4 2011-01-04      51.1\n##  5 2011-01-05      52.6\n##  6 2011-01-06      53.0\n##  7 2011-01-07      50.8\n##  8 2011-01-08      46.6\n##  9 2011-01-09      42.5\n## 10 2011-01-10      45.6\n\n\nnew_bikes %&gt;% \n    select(-date, -temp_feel)\n## # A tibble: 10 × 3\n##    humidity riders_registered day_of_week\n##       &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n##  1    0.806               654 Sat        \n##  2    0.696               670 Sun        \n##  3    0.437              1229 Mon        \n##  4    0.590              1454 Tue        \n##  5    0.437              1518 Wed        \n##  6    0.518              1518 Thu        \n##  7    0.499              1362 Fri        \n##  8    0.536               891 Sat        \n##  9    0.434               768 Sun        \n## 10    0.483              1280 Mon\n\n\n\nVerb 3: filter\nfilter() keeps only days (rows) that meet the given condition(s).\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850)\n## # A tibble: 7 × 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-03      49.0    0.437              1229 Mon        \n## 2 2011-01-04      51.1    0.590              1454 Tue        \n## 3 2011-01-05      52.6    0.437              1518 Wed        \n## 4 2011-01-06      53.0    0.518              1518 Thu        \n## 5 2011-01-07      50.8    0.499              1362 Fri        \n## 6 2011-01-08      46.6    0.536               891 Sat        \n## 7 2011-01-10      45.6    0.483              1280 Mon\n\n\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sat\")\n## # A tibble: 2 × 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-01      64.7    0.806               654 Sat        \n## 2 2011-01-08      46.6    0.536               891 Sat\n\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850, day_of_week == \"Sat\")\n## # A tibble: 1 × 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-08      46.6    0.536               891 Sat",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-10-your-turn",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-10-your-turn",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 10: Your turn",
    "text": "Exercise 10: Your turn\nUse dplyr verbs to complete each task below.\n\n# Keep only information about the humidity and day of week\nnew_bikes %&gt;% \n    select(humidity, day_of_week)\n## # A tibble: 10 × 2\n##    humidity day_of_week\n##       &lt;dbl&gt; &lt;chr&gt;      \n##  1    0.806 Sat        \n##  2    0.696 Sun        \n##  3    0.437 Mon        \n##  4    0.590 Tue        \n##  5    0.437 Wed        \n##  6    0.518 Thu        \n##  7    0.499 Fri        \n##  8    0.536 Sat        \n##  9    0.434 Sun        \n## 10    0.483 Mon\n\n# Keep only information about the humidity and day of week using a different approach\nnew_bikes %&gt;% \n    select(-date, -temp_feel, -riders_registered)\n## # A tibble: 10 × 2\n##    humidity day_of_week\n##       &lt;dbl&gt; &lt;chr&gt;      \n##  1    0.806 Sat        \n##  2    0.696 Sun        \n##  3    0.437 Mon        \n##  4    0.590 Tue        \n##  5    0.437 Wed        \n##  6    0.518 Thu        \n##  7    0.499 Fri        \n##  8    0.536 Sat        \n##  9    0.434 Sun        \n## 10    0.483 Mon\n\n# Keep only information for Sundays\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sun\")\n## # A tibble: 2 × 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-02      63.8    0.696               670 Sun        \n## 2 2011-01-09      42.5    0.434               768 Sun\n\n# Keep only information for Sundays with temperatures below 50\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sun\", temp_feel &lt; 50)\n## # A tibble: 1 × 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-09      42.5    0.434               768 Sun\n\n# Calculate the maximum and minimum temperatures\nnew_bikes %&gt;% \n    summarize(min(temp_feel), max(temp_feel))\n## # A tibble: 1 × 2\n##   `min(temp_feel)` `max(temp_feel)`\n##              &lt;dbl&gt;            &lt;dbl&gt;\n## 1             42.5             64.7",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data-3",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-1-get-to-know-the-data-3",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\n\nUse an appropriate function to look at the first few rows of the data.\n\n\nhead(lifts)\n## # A tibble: 6 × 21\n##   Name        Sex   Event Equipment   Age BodyweightKg Best3SquatKg Best3BenchKg\n##   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n## 1 Natalya Po… F     D     Raw        37           58.4          NA          NA  \n## 2 Fatima Rod… F     SBD   Single-p…  NA           74.8          NA          NA  \n## 3 Josh Kelley M     SBD   Single-p…  NA           72.4         147.         97.5\n## 4 Timothy Ca… M     D     Raw        16           72.9          NA          NA  \n## 5 M Moynihan  M     B     Raw        NA           67.5          NA         100  \n## 6 Lucas Wegr… M     B     Raw        23.5        103.           NA         188. \n## # ℹ 13 more variables: Best3DeadliftKg &lt;dbl&gt;, TotalKg &lt;dbl&gt;, Place &lt;chr&gt;,\n## #   Dots &lt;dbl&gt;, Wilks &lt;dbl&gt;, Glossbrenner &lt;dbl&gt;, Goodlift &lt;dbl&gt;, Tested &lt;chr&gt;,\n## #   Country &lt;chr&gt;, State &lt;chr&gt;, Date &lt;date&gt;, MeetCountry &lt;chr&gt;, MeetState &lt;chr&gt;\n\n\nCreate a new code chunk, and use an appropriate function to learn how much data we have (in terms of cases and variables).\n\n\ndim(lifts)\n## [1] 100000     21\n\n\nA case represents an individual lifter at a single weightlifting competition.\nIt looks like some meets may be missing if they weren’t detected by the web scraper used by the maintainers of the Open Powerlifting database. They don’t describe in detail the process used for transferring PDFs of results to their database, so it’s unclear what errors in transcription might have resulted. Still, it’s worth taking a moment to appreciate the labor they put into making these results available for passionate powerlifters to explore.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-2-mutating-our-data-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-2-mutating-our-data-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 2: Mutating our data",
    "text": "Exercise 2: Mutating our data\nStrength-to-weight ratio (SWR) is defined as TotalKg/BodyweightKg. We can use the mutate() function from the dplyr package to create a new variable in our dataframe for SWR using the following code:\n\nlifts &lt;- lifts %&gt;% \n    mutate(SWR = TotalKg / BodyweightKg)\n\nAdapt the example above to create a new variable called SWR, where SWR is defined as TotalKg/BodyweightKg.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-3-get-to-know-the-outcomeresponse-variable-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-3-get-to-know-the-outcomeresponse-variable-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 3: Get to know the outcome/response variable",
    "text": "Exercise 3: Get to know the outcome/response variable\nLet’s get acquainted with the SWR variable.\n\nConstruct an appropriate plot to visualize the distribution of this variable, and compute appropriate numerical summaries.\n\n\nlifts %&gt;%\n  ggplot(aes(SWR)) +\n  geom_histogram(bins = 10, col = \"black\")\n\n\n\n\n\n\n\n\nlifts %&gt;% summarize(mean(SWR, na.rm = TRUE), min(SWR, na.rm = TRUE), max(SWR, na.rm = TRUE), sd(SWR, na.rm = TRUE))\n## # A tibble: 1 × 4\n##   `mean(SWR, na.rm = TRUE)` `min(SWR, na.rm = TRUE)` `max(SWR, na.rm = TRUE)`\n##                       &lt;dbl&gt;                    &lt;dbl&gt;                    &lt;dbl&gt;\n## 1                      4.42                    0.183                     12.5\n## # ℹ 1 more variable: `sd(SWR, na.rm = TRUE)` &lt;dbl&gt;\n\n\nWrite a good paragraph interpreting the plot and numerical summaries.\n\nStrength-to-weight (SWR) ratio ranges from 0.18 to 12.46, with a mean SWR of 4.4. SWR varies about 2.08 units above and below the mean. We observe that most SWRs appear to be centered between 4 and 7, with a slight right-skew to the data. The distribution of SWRs appears to be unimodal.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-4-data-visualization---two-quantitative-variables-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-4-data-visualization---two-quantitative-variables-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 4: Data visualization - two quantitative variables",
    "text": "Exercise 4: Data visualization - two quantitative variables\nWe’d like to visualize the relationship between body weight and the strength-to-weight ratio. A scatterplot (or informally, a “point cloud”) allows us to do this! The code below creates a scatterplot of body weight vs. SWR using ggplot().\n\n# scatterplot\n\n# The alpha = 0.5 in geom_point() adds transparency to the points\n# to make them easier to see. You can make this smaller for more transparency\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5)\n\n\n\n\n\n\n\n\na & b. In our plot aesthetics, we now have two variables listed (an “x” and a “y”) as opposed to just a single variable. The “geom” for a scatterplot is geom_point. Otherwise, the code structure remains very similar!\n\nIn general, it seems as though higher body weights are associated with lower SWRs. Once body weight (in kg) is greater than 50, the relationship between body weight and SWR appears to be weakly negative, and roughly linear. The points are very dispersed, indicating that there is a good amount of variation in this relationship (hence the term “weak”).",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-5-scatterplots---patterns-in-point-clouds-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-5-scatterplots---patterns-in-point-clouds-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 5: Scatterplots - patterns in point clouds",
    "text": "Exercise 5: Scatterplots - patterns in point clouds\nSometimes, it can be easier to see a pattern in a point cloud by adding a smoothing line to our scatterplots. The code below adapts the code in Exercise 4 to do this:\n\n# scatterplot with smoothing line\nlifts %&gt;%\n  ggplot(aes(x = BodyweightKg, y = SWR)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nThis doesn’t change my answer much (but it may have changed yours, and that’s okay!). It does appear as though there is a weakly negative relationship between body weight and SWR, particularly once body weight is above a certain value.\nI would say that yes, a linear relationship here seems reasonable! Even though there is some curvature in the smoothed trend line early on, that is based on very few data points. Those data points with low body weights aren’t enough to convince me that the relationship couldn’t be roughly linear between body weight and SWR.",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-6-correlation-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-6-correlation-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 6: Correlation",
    "text": "Exercise 6: Correlation\n\nI would describe the correlation between body weight and SWR as weak and negative.\nI’ll guess -0.1, since the line is negative, and the points are very dispersed around the line!",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-7-computing-correlation-in-r-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-7-computing-correlation-in-r-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 7: Computing correlation in R",
    "text": "Exercise 7: Computing correlation in R\n\n# correlation\n\n# Note: the order in which you put your two quantitative variables into the cor\n# function doesn't matter! Try switching them around to confirm this for yourself\n# Because of the missing data, we need to include the use = \"complete.obs\" - otherwise the correlation would be computed as NA\nlifts %&gt;%\n    summarize(cor(SWR, BodyweightKg, use = \"complete.obs\"))\n## # A tibble: 1 × 1\n##   `cor(SWR, BodyweightKg, use = \"complete.obs\")`\n##                                            &lt;dbl&gt;\n## 1                                        -0.0392\n\nSo close to our guess!",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-8-limitations-of-correlation-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-8-limitations-of-correlation-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 8: Limitations of correlation",
    "text": "Exercise 8: Limitations of correlation\n\n# correlation between x1, y1\nanscombe %&gt;% summarize(cor(x1, y1))\n##   cor(x1, y1)\n## 1   0.8164205\n\n# correlation between x2, y2\nanscombe %&gt;% summarize(cor(x2, y2))\n##   cor(x2, y2)\n## 1   0.8162365\n\n# correlation between x3, y3\nanscombe %&gt;% summarize(cor(x3, y3))\n##   cor(x3, y3)\n## 1   0.8162867\n\n# correlation between x4, y4\nanscombe %&gt;% summarize(cor(x4, y4))\n##   cor(x4, y4)\n## 1   0.8165214\n\n\nEach of these correlations are nearly the same!\nEach of these correlations is relatively strong, and positive, since 0.8 is positive and closer to 1 than 0.\n\n\n\n# scatterplot: x1, y1\nanscombe %&gt;%\n  ggplot(aes(x = x1, y = y1)) +\n  geom_point()\n\n\n\n\n\n\n\n\n# scatterplot: x2, y2\nanscombe %&gt;%\n  ggplot(aes(x = x2, y = y2)) +\n  geom_point()\n\n\n\n\n\n\n\n\n# scatterplot: x3, y3\nanscombe %&gt;%\n  ggplot(aes(x = x3, y = y3)) +\n  geom_point()\n\n\n\n\n\n\n\n\n# scatterplot: x4, y4\nanscombe %&gt;%\n  ggplot(aes(x = x4, y = y4)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nThe message of this exercise is that data visualization is important in addition to numerical summaries! Many different sets of points can have nearly the same correlation, but display very different patterns in point clouds upon closer inspection. Reporting correlation alone is not enough to summarize the relationship between two quantitative variables, and should be accompanied by a scatter plot!",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/03_04-slr-intro-formalization.html#exercise-10-correlation-and-extreme-values-1",
    "href": "activities/03_04-slr-intro-formalization.html#exercise-10-correlation-and-extreme-values-1",
    "title": "Simple Linear Regression - Intro+Formalization",
    "section": "Exercise 10: Correlation and extreme values",
    "text": "Exercise 10: Correlation and extreme values\n\n# create a toy dataset\nset.seed(1234)\nx &lt;- rnorm(100, mean = 5, sd = 2)\ny &lt;- -3 * x + rnorm(100, sd = 4)\ndat &lt;- data.frame(x = x, y = y)\n\n\n\n\n\n# scatterplot\ndat %&gt;% \n  ggplot(aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nThe correlation between x and y is moderately strong and negative.\nI’ll guess -0.6, since the relationship is negative and is sort of in-between weak and strong.\n\n\n\n# correlation\ndat %&gt;% summarize(cor(x, y))\n##    cor(x, y)\n## 1 -0.8295483\n\n\n\n\n\n# creating dat_new1\nx1 &lt;- c(x, 15)\ny1 &lt;- c(y, -45)\ndat_new1 &lt;- data.frame(x = x1, y = y1)\n\n\n\n\n\n# scatterplot\ndat_new1 %&gt;%\n  ggplot(aes(x1, y1)) +\n  geom_point()\n\n\n\n\n\n\n\n\n# correlation\ndat %&gt;% summarize(cor(x1, y1))\n##   cor(x1, y1)\n## 1  -0.8573567\n\nOur correlation stayed roughly the same with the addition of this new point!\n\n\n\n\n# creating dat_new1\nx2 &lt;- c(x, 15)\ny2 &lt;- c(y, 45)\ndat_new2 &lt;- data.frame(x = x2, y = y2)\n\n\n\n\n\n# scatterplot\ndat_new2 %&gt;%\n  ggplot(aes(x2, y2)) +\n  geom_point()\n\n\n\n\n\n\n\n\n# correlation\ndat_new2 %&gt;% summarize(cor(x2, y2))\n##   cor(x2, y2)\n## 1  -0.2924792\n\nThe correlation changes quite a bit with the addition of this new point! Something to note is that this new point does not follow the rough linear trend that the original points had, that the first point we considered adding also had. This line seems way off base, comparatively!\n\nThe takeaway message here is that even though both of these additional points might be considered “outliers” because they have extreme x values, one changes the relationship between x and y much more than the other. In this case, the second point we considered would be influential because it changes the observed relationship between all x’s and y’s much more than the first point we considered. Not all “outliers” are considered equal!\n\n\n\n\n\ndat_new1 %&gt;%\n  ggplot(aes(x1, y1)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\ndat_new2 %&gt;%\n  ggplot(aes(x2, y2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)",
    "crumbs": [
      "Simple Linear Regression - Intro+Formalization"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html",
    "href": "activities/06-slr-transformations.html",
    "title": "Simple Linear Regression - Transformation",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#these-activities-on-slr-transformation-are-optional--enjoy",
    "href": "activities/06-slr-transformations.html#these-activities-on-slr-transformation-are-optional--enjoy",
    "title": "Simple Linear Regression - Transformation",
    "section": "These activities on ‘SLR: Transformation’ are OPTIONAL- ENJOY!!!",
    "text": "These activities on ‘SLR: Transformation’ are OPTIONAL- ENJOY!!!",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#learning-goals",
    "href": "activities/06-slr-transformations.html#learning-goals",
    "title": "Simple Linear Regression - Transformation",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nDistinguish between the different motivations for transformations of variables (interpretation, regression assumptions, etc.)\nDetermine when a particular transformation (center, scale, or log) may be appropriate\nInterpret regression coefficients after a transformation has taken place",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#readings-and-videos",
    "href": "activities/06-slr-transformations.html#readings-and-videos",
    "title": "Simple Linear Regression - Transformation",
    "section": "Readings and videos",
    "text": "Readings and videos\nPlease watch the following video before class.\n\nVideo: Simple Linear Regression: Transformations\n\nThe following reading is optional.\n\nSection 3.8.4 in the STAT 155 Notes covers log transformations, and the “ladder of power,” which we will not cover in class.\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder.",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#exercise-1-location-transformations",
    "href": "activities/06-slr-transformations.html#exercise-1-location-transformations",
    "title": "Simple Linear Regression - Transformation",
    "section": "Exercise 1: Location transformations",
    "text": "Exercise 1: Location transformations\nLocation transformations are ones that shift a predictor variable up or down by a fixed amount. Using a location transformation is sometimes also called centering a predictor.\nWe’ll use the homes data in this exercise, which includes 2006 data on homes in Saratoga County, New York. Our goal is to understand the relationship of home Price ($) with Living.Area, the size of a home in square feet.\n\nFit a linear regression model of Price by Living.Area, and call this model home_mod.\n\n\n# Fit the model\n\n\n# Display model summary output\n\n\nInterpret the intercept and the coefficient for Living.Area. Is the interpretation of the intercept meaningful?\nWe can use a location transformation on Living.Area to “start” it at a more reasonable value. We can see from the summarize() code below that the smallest house is 616 quare feet, so let’s center this predictor at 600 square feet. There is no code to fill in here, but make note of the mutate() syntax.\n\n\nhomes %&gt;% \n    summarize(min(Living.Area))\n## # A tibble: 1 × 1\n##   `min(Living.Area)`\n##                &lt;dbl&gt;\n## 1                616\n\n# What is mutate() doing???\nhomes &lt;- homes %&gt;%\n    mutate(Living.Area.Shifted = Living.Area-600)\n\n\nWe can actually determine the coefficients of the Price ~ Living.Area.Shifted model by hand.\n\nFirst, write out in general terms (without specific numbers) how we would interpret the intercept and slope in this model.\nUse these general interpretations as well as the summary output of home_mod to determine what these new coefficients should be.\n\nNow check your answer to part d by fitting the model.\n\n\n# Fit a model of Price vs. Living.Area.Shifted\n\n\n# Display model summary output\n\n\nSo we now have 2 models of Price by the size of a home, as measured by Living.Area and Living.Area.Shifted:\n\n\nE[Price | Living.Area] = 13439.394 + 113.123 Living.Area\nE[Price | Living.Area.Shifted] = 81312.919 + 113.123 Living.Area.Shifted\n\nAs indicated by the equal slopes but differing intercepts, these models simply have different locations:\n\n# Don't worry about the code!!\nhomes %&gt;% \n  select(Price, Living.Area, Living.Area.Shifted) %&gt;% \n  pivot_longer(cols = -Price, names_to = \"Predictor\", values_to = \"Size\") %&gt;% \n  ggplot(aes(y = Price, x = Size)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  facet_wrap(~ Predictor)\n\n\n\n\n\n\n\n\nImportantly, they produce the same predictions! For example, use both models to predict the price of a 1000 square foot home (without using the predict() function). These two predictions should be the same, within rounding.\n\n# Predicting price with the home_mod\n\n# Predicting price with the home_mod_2",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#exercise-2-scale-transformations",
    "href": "activities/06-slr-transformations.html#exercise-2-scale-transformations",
    "title": "Simple Linear Regression - Transformation",
    "section": "Exercise 2: Scale transformations",
    "text": "Exercise 2: Scale transformations\nIn this exercise, we’ll explore the relationship between four-year graduation rate and admissions rate of colleges using 2024 data from the College Scorecard. In the code chunk below, construct a visualization comparing graduation rate (our outcome variable) and admissions rate (our predictor of interest). Remember that your outcome variable should be on the y-axis, in general!\n\n# Scatterplot of graduation rate vs. admissions rate\n\n\nDescribe the relationship you observe between the two quantitative variables, in terms of correlation (weak/strong, positive/negative). Does the relationship appear to be roughly linear?\nWrite a linear regression model formula of the form E[Y | X] = … (filling in Y and X appropriately).\nFit this model in R, and report (don’t interpret yet!) the slope coefficient and intercept coefficient estimates.\n\n\n# Linear regression model with GradRate as the outcome, AdmisRate as predictor of interest\n\n\nIntercept Estimate: Your response here\n\n\nSlope Estimate: Your response here\n\n\nConsidering the units of AdmisRate, what does it mean for AdmisRate to change by one unit? What are the units for AdmisRate (and GradRate, for that matter!)?\nSuppose I want the interpretation of my slope coefficient for AdmisRate in my linear model to be in terms a “1 percentage point increase in admissions rate.” To achieve this, we could mutate our AdmisRate variable to range from 0 to 100. Let’s do that for GradRate too (just because!):\n\n\n# Mutate\ncollege &lt;- college %&gt;%\n  mutate(AdmisRate = AdmisRate * ___,\n         GradRate = ___ * ___)\n## Error in parse(text = input): &lt;text&gt;:3:35: unexpected input\n## 2: college &lt;- college %&gt;%\n## 3:   mutate(AdmisRate = AdmisRate * __\n##                                      ^\n\n\nFit a new linear regression model with the updated AdmisRate and GradRate variables as your predictor of interest and outcome, respectively. Again, report the intercept and slope estimate from your model.\n\n\n# Linear regression model with updated GradRate as the outcome, updated AdmisRate as predictor of interest\n\n\nIntercept Estimate: Your response here\n\n\nSlope Estimate: Your response here\n\nHow have your intercept and slope estimates changed from the previous model, if at all?\n\nInterpret the regression coefficient that corresponds to the estimated linear relationship between admissions and graduation rates, in the context of the problem. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#exercise-3-log-transformations",
    "href": "activities/06-slr-transformations.html#exercise-3-log-transformations",
    "title": "Simple Linear Regression - Transformation",
    "section": "Exercise 3: Log transformations",
    "text": "Exercise 3: Log transformations\nThe Big Mac Index has been published by The Economist since 1986 as a metric for comparing purchasing power between countries, giving rise to the phrase Burgernomics. It was developed (sort of jokingly) as a way to explain exchange rates in digestible terms.\nAs an example, suppose a Big Mac in Switzerland costs 6.70 Swiss franc, and in the U.S. a Big Mac costs 5.58 USD. Then the Big Mac Index is 6.70/5.58 = 1.20, and is the implied exchange rate between Swiss franc and USD.\nIf you’d like to read more about the Big Mac index, here’s an article in The Economist (this may be behind a pay-wall for you, you can read up to 5 free articles in the Economist per month).\nFor this exercise, we’ll use the bigmac data (collected in 2006) to explore the relationship between average teaching salary in a country and the amount of time someone needs to work to be able to afford a Big Mac. The variables we’ll consider are:\n\nbigmac_mins: average minutes to earn 1 Big Mac\ngross_annual_teacher_income: average gross teacher salary in 1 year (USD)\n\n\nCreate an appropriate visualization that displays the relationship between average minutes to earn a Big Mac and gross annual, average teaching salary, and describe what you observe.\n\n\n# Visualization: Big Mac minutes vs. gross annual teacher income\n\n\nExplain why correlation might not be an appropriate numerical summary for the relationship between the two variables you plotted above.\nFit a linear regression model with bigmac_mins as the outcome and gross_annual_teacher_income as the predictor of interest, and interpret the coefficient for gross_annual_teacher_income, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\n\n\n# Linear regression code\n\n\nPlot residuals vs. fitted values for the model you fit, and describe what you observe. Are there any noticeable patterns in the residuals? Describe them!\n\n\n# Residuals vs. fitted values plot\n\n\nFor which observations do the residuals from the linear regression model appear to be relatively large (i.e. for which observations would predictions fall farthest from observed outcomes)? What possible consequences would this have for people using this model to predict the amount of time it takes for them to earn enough money to afford a Big Mac?\n\nWe’ll now consider a log transformation of teaching salary. In the code chunk below, create a new variable called log_sal that contains the logged values of gross_annual_teacher_income.\n\n# Creating new variable log_sal\nbigmac &lt;- bigmac %&gt;%\n  mutate(log_sal = log(___))\n## Error in parse(text = input): &lt;text&gt;:3:25: unexpected input\n## 2: bigmac &lt;- bigmac %&gt;%\n## 3:   mutate(log_sal = log(__\n##                            ^\n\n\nCreate an appropriate visualization that displays the relationship between average minutes to earn a Big Mac and logged gross annual, average teaching salary, and describe what you observe. Does correlation seem like it may be an appropriate numerical summary for the relationship between these two variables? Explain why or why not.\nFit a linear regression model with bigmac_mins as the outcome and log_sal as the predictor of interest, and interpret the coefficient for log_sal, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\nPlot residuals vs. fitted values for the model you fit, and describe what you observe. Are there any noticeable patterns in the residuals? Describe them!\n\n\n# Residuals vs. fitted values plot",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#reflection",
    "href": "activities/06-slr-transformations.html#reflection",
    "title": "Simple Linear Regression - Transformation",
    "section": "Reflection",
    "text": "Reflection\nTwo of the main motivations for transforming variables in our regression models is to (1) intentionally change the interpretation of regression coefficients, and (2) to better satisfy linear regression assumptions (e.g. remove “patterns” from our residual plots). The first is nearly always justified by the scientific context of the research questions you are trying to answer, while the second is a bit more muddy.\nThink about the pros and cons of transforming your variables to satisfy linear regression assumptions. Is there a limit to how much you would be willing to transform your variables? Would transforming too much leave you with un-interpretable regression coefficients?\n\nResponse: Put your response here.",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#exercise-1-location-transformations-1",
    "href": "activities/06-slr-transformations.html#exercise-1-location-transformations-1",
    "title": "Simple Linear Regression - Transformation",
    "section": "Exercise 1: Location transformations",
    "text": "Exercise 1: Location transformations\nLocation transformations are ones that shift a predictor variable up or down by a fixed amount. Using a location transformation is sometimes also called centering a predictor.\nWe’ll use the homes data in this exercise.\n\nFit a linear regression model of Price as a function of Living.Area, and call this model home_mod.\n\n\n# Fit the model\nhome_mod &lt;- lm(Price ~ Living.Area, data = homes)\n\n# Display model summary output\ncoef(summary(home_mod))\n##               Estimate  Std. Error   t value      Pr(&gt;|t|)\n## (Intercept) 13439.3940 4992.352849  2.691996  7.171207e-03\n## Living.Area   113.1225    2.682341 42.173065 9.486240e-268\n\n\n\nInterpretation of slope: Every 1 square foot increase in living area is associated with a $113.12 increase in average house price.\nInterpretation of intercept: The average/expected house price for a house with zero square feet is $13,439.39. Can a house ever be zero square feet??? Nope! The intercept is meaningless in this case.\n\n\n\n\nhomes %&gt;% \n    summarize(min(Living.Area))\n## # A tibble: 1 × 1\n##   `min(Living.Area)`\n##                &lt;dbl&gt;\n## 1                616\n\n# mutate() creates a new variable called Living.Area.Shifted that is equal to Living.Area - 600\nhomes &lt;- homes %&gt;%\n    mutate(Living.Area.Shifted = Living.Area-600)\n\n\n\nIn general terms, the intercept in this model should represent the average house price when Living.Area.Shifted is 0—in other words when Living.Area is 600 square feet. From the coefficient estimates in home_mod, we can calculate the expected / predicted house price for 600 square foot homes: 13439.394 + (113.123*600) = 81312.89. So we’re expecting the new intercept to be $81312.89.\nThe slope in this model represents the average price change for each unit change in Living.Area.Shifted (which is the same as a unit change in Living.Area). Based on this, the slope should be the same as in home_mod ($113.12 per square foot).\n\nLines up with work in part d!\n\n\n# Fit a model of Price vs. Living.Area.Shifted\nhome_mod_centered &lt;- lm(Price ~ Living.Area.Shifted, data = homes)\n\n# Display model summary output\ncoef(summary(home_mod_centered))\n##                       Estimate  Std. Error  t value      Pr(&gt;|t|)\n## (Intercept)         81312.9191 3515.879467 23.12733 2.638371e-103\n## Living.Area.Shifted   113.1225    2.682341 42.17307 9.486240e-268\n\n\n\n\n\n# Predicting price with the home_mod\n13439.394 + 113.123*1000\n## [1] 126562.4\n\n# Predicting price with the home_mod_2\n81312.919 + 113.123*(1000 - 600)\n## [1] 126562.1",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#exercise-2-scale-transformations-1",
    "href": "activities/06-slr-transformations.html#exercise-2-scale-transformations-1",
    "title": "Simple Linear Regression - Transformation",
    "section": "Exercise 2: Scale transformations",
    "text": "Exercise 2: Scale transformations\nIn the code chunk below, construct a visualization comparing graduation rate (our outcome variable) and admissions rate (our predictor of interest). Remember that your outcome variable should be on the y-axis, in general!\n\n# Scatterplot of graduation rate vs. admissions rate\ncollege %&gt;%\n  ggplot(aes(x = AdmisRate, y = GradRate)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nThe correlation between admissions and graduation rates appears to be weakly negative. Notably, there are hard boundaries to admissions and graduation rates, since both must fall between 0 and 100%! A few colleges hit up against these boundaries. I would say that, with the exception of the observations that have either 0% graduation rates or 0% admission rates, the relationship does appear to be roughly linear.\nE[GradRate | AdmisRate] = \\(\\beta_0\\) + \\(\\beta_1\\) AdmisRate\n\n\n\n# Linear regression model with GradRate as the outcome, AdmisRate as predictor of interest\nmod &lt;- lm(GradRate ~ AdmisRate, data = college)\nsummary(mod)\n## \n## Call:\n## lm(formula = GradRate ~ AdmisRate, data = college)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.68409 -0.13681  0.01296  0.15550  0.66204 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.68409    0.01759   38.89   &lt;2e-16 ***\n## AdmisRate   -0.34613    0.02330  -14.85   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2088 on 1649 degrees of freedom\n## Multiple R-squared:  0.118,  Adjusted R-squared:  0.1175 \n## F-statistic: 220.6 on 1 and 1649 DF,  p-value: &lt; 2.2e-16\n\n\nIntercept Estimate: 0.68409\n\n\nSlope Estimate: -0.34613\n\n\nOne unit of AdmisRate corresponds to a 100% change in admissions rates! The same goes for graduation rate. This is a huge change (in fact, the largest change possible).\nSuppose I want the interpretation of my slope coefficient for AdmisRate in my linear model to be in terms a “1% increase in admissions rate.” To achieve this, we could mutate our AdmisRate variable to range from 0 to 100. Let’s do that for GradRate too (just because!):\n\n\n# Mutate\ncollege &lt;- college %&gt;%\n  mutate(AdmisRate = AdmisRate * 100,\n         GradRate = GradRate * 100)\n\n\nFit a new linear regression model with the updated AdmisRate and GradRate variables as your predictor of interest and outcome, respectively. Again, report the intercept and slope estimate from your model.\n\n\n# Linear regression model with updated GradRate as the outcome, updated AdmisRate as predictor of interest\nmod_new &lt;- lm(GradRate ~ AdmisRate, data = college)\nsummary(mod_new)\n## \n## Call:\n## lm(formula = GradRate ~ AdmisRate, data = college)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -68.409 -13.681   1.296  15.550  66.204 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  68.4088     1.7592   38.89   &lt;2e-16 ***\n## AdmisRate    -0.3461     0.0233  -14.85   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 20.88 on 1649 degrees of freedom\n## Multiple R-squared:  0.118,  Adjusted R-squared:  0.1175 \n## F-statistic: 220.6 on 1 and 1649 DF,  p-value: &lt; 2.2e-16\n\n\nIntercept Estimate: 68.4088\n\n\nSlope Estimate: -0.3461\n\nOur intercept estimate is now 100x larger, and our slope estimate has remained the same! The slope remained the same because we multiplied our outcome and our predictor of interest by the same value, and the intercept is 100x larger because we multiplied our outcome by 100 (recall that the intercept is the average expected outcome when “x” is zero).\n\nOn average, we expect colleges that differ in admissions rate by 1% to have 0.35% different graduation rates, with colleges with higher admissions rates having lower graduation rates.",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/06-slr-transformations.html#exercise-3-log-transformations-1",
    "href": "activities/06-slr-transformations.html#exercise-3-log-transformations-1",
    "title": "Simple Linear Regression - Transformation",
    "section": "Exercise 3: Log transformations",
    "text": "Exercise 3: Log transformations\n\n\n\n\n# Visualization: Big Mac minutes vs. gross annual teacher income\nbigmac %&gt;%\n  ggplot(aes(x = gross_annual_teacher_income, y = bigmac_mins)) +\n  geom_point() \n\n\n\n\n\n\n\n\nAs annual teacher income gets higher, time it takes in minutes to earn a Big Mac decreases, though the relationship does not appear linear. The amount of time it takes to earn a Big Mac is very high when income is below about 10,000 where it sharply decreases, and then decreases at a much lower rate when income is above around 20,000.\n\nCorrelation is a summary of the linear relationship between two quantitative variables, and this relationship does not appear to be linear!\n\n\n\n# Linear regression code\nmod &lt;- lm(bigmac_mins ~ gross_annual_teacher_income, data = bigmac)\nsummary(mod)\n## \n## Call:\n## lm(formula = bigmac_mins ~ gross_annual_teacher_income, data = bigmac)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -29.649  -9.556  -1.784   4.512  43.715 \n## \n## Coefficients:\n##                               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)                  5.801e+01  3.104e+00   18.69  &lt; 2e-16 ***\n## gross_annual_teacher_income -9.092e-04  9.591e-05   -9.48 6.16e-14 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 15.4 on 66 degrees of freedom\n##   (2 observations deleted due to missingness)\n## Multiple R-squared:  0.5766, Adjusted R-squared:  0.5701 \n## F-statistic: 89.86 on 1 and 66 DF,  p-value: 6.164e-14\n\nA one dollar increase in gross annual teacher income is associated with a 9 x 10^(-4) minutes decrease in the average number of minutes it takes to earn a Big Mac. Stated differently, a ten-thousand dollar increase in gross annual teacher income is associated with a 9 minute decrease in the average number of minutes it takes to earn a Big Mac (note that here I did a scale transformation of gross annual teacher income to get this interpretation, which might make more sense when looking at the scale of salary!).\n\n\n\n\n# Residuals vs. fitted values plot\nggplot(mod, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\nThe residuals vs. fitted values plot shows a very clear, nonlinear pattern! As fitted values increase, residuals decrease for a while, and then sharply increase once fitted values are higher than around 40 minutes. The spread of residuals around zero also varies, with greater spread for higher fitted values.\n\nThe residuals appear to be large for people with negative fitted values and those with very high fitted values. Recall that a linear model does not “know” that number of minutes to earn a Big Mac can’t be negative, in context. If we look at the fitted line from our linear model on a scatterplot (see below)…\n\n\nbigmac %&gt;%\n  ggplot(aes(x = gross_annual_teacher_income, y = bigmac_mins)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nWe observe that negative fitted values and very large fitted values occur when annual teacher income is greater than around 70,000 and less than 10,000, respectively. This implies that the model does a worse job at predicting the number of minutes to earn a Big Mac in countries where annual teacher income is either very high or very low.\nWe’ll now consider a log transformation of teaching salary. In the code chunk below, create a new variable called log_sal that contains the logged values of gross_annual_teacher_income.\n\n# Creating new variable log_sal\nbigmac &lt;- bigmac %&gt;%\n  mutate(log_sal = log(gross_annual_teacher_income))\n\n\n\n\n\nbigmac %&gt;%\n  ggplot(aes(log_sal, bigmac_mins)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe relationship between logged annual teaching salary and minutes to earn a Big Mac appears roughly linear, with a weakly negative relationship. Correlation is likely an appropriate numerical summary for the relationship between these two quantitative variables, as the relationship is roughly linear!\n\n\n\n\nmod_log &lt;- lm(bigmac_mins ~ log_sal, data = bigmac)\nsummary(mod_log)\n## \n## Call:\n## lm(formula = bigmac_mins ~ log_sal, data = bigmac)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -36.817  -6.951  -1.241   6.032  41.357 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  210.875     14.687   14.36   &lt;2e-16 ***\n## log_sal      -18.142      1.502  -12.08   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 13.21 on 66 degrees of freedom\n##   (2 observations deleted due to missingness)\n## Multiple R-squared:  0.6886, Adjusted R-squared:  0.6838 \n## F-statistic: 145.9 on 1 and 66 DF,  p-value: &lt; 2.2e-16\n\nEach 1 unit increase in logged salary is associated with a 18.14 minute decrease in time to earn a Big Mac on average.\nWe can also use a property of logarithms to interpret the slope of -18.14 in a different way. Suppose we have two salaries: Salary1 and Salary2. If Salary2 is 10% higher than Salary1, then Salary2/Salary1 = 1.1. It is a property of logarithms that log(Salary2/Salary1) = log(Salary2) - log(Salary1). In this case log(Salary2/Salary1) = log(Salary2) - log(Salary1) = log(1.1) = 0.09531018. So a 10% increase in salary is a 0.09 unit increase in the log scale:\n\n# Multiplicative difference of 1.1, or 10% between salaries gives us the \nlog(1.1) * -18.142\n## [1] -1.729117\n\nWhile a 1 unit increase in log salary is associated with an average decrease of 18 Big Mac minutes, a 0.0953 unit increase in log salary (which corresponds to a 10% multiplicative increase), is associated with a 1.7 minute decrease in Big Mac minutes.\nUnderlying math:\nCase 1: Salary = x\n   E[bigmacmin_1] = beta0 + beta1 log(x)\nCase 2: Salary = m*x\n   E[bigmacmin_2] = beta0 + beta1 log(m*x)\n\nE[bigmacmin_2] - E[bigmacmin_1] = beta1 log(m)\n\n\n\n\n# Residuals vs. fitted values plot\nggplot(mod_log, aes(x = .fitted, y = .resid)) + \n    geom_point() + \n    geom_hline(yintercept = 0) +\n    geom_smooth(se = FALSE)\n\n\n\n\n\n\n\n\nThe residuals seem to lie roughly around zero for all possible fitted values, though the spread is still noticably larger for larger fitted values compared to smaller ones. This implies that the linearity assumption is likely satisfied for this model, but equal variance may be a concern.",
    "crumbs": [
      "Simple Linear Regression - Transformation"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html",
    "href": "activities/08-mlr-intro.html",
    "title": "Multiple Linear Regression - Intro",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#learning-goals",
    "href": "activities/08-mlr-intro.html#learning-goals",
    "title": "Multiple Linear Regression - Intro",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be familiar with:\n\nsome limitations of simple linear regression\nthe general goals behind multiple linear regression\nstrategies for visualizing and interpreting multiple linear regression models of \\(Y\\) vs 2 predictors, 1 quantitative and 1 categorical",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#readings-and-videos",
    "href": "activities/08-mlr-intro.html#readings-and-videos",
    "title": "Multiple Linear Regression - Intro",
    "section": "Readings and videos",
    "text": "Readings and videos\nToday is a day to discover ideas, so no readings or videos to go through before class.",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#motivation",
    "href": "activities/08-mlr-intro.html#motivation",
    "title": "Multiple Linear Regression - Intro",
    "section": "Motivation",
    "text": "Motivation\nEXAMPLE 1\nLet’s explore some data on penguins. First, enter install.packages(\"palmerpenguins\") in the console (not Rmd). Then load the penguins data. You can find a codebook for these data by typing ?penguins in your console (not qmd).\n\n# Load packages\nlibrary(tidyverse)\ndata(penguins)\npenguins &lt;- penguins %&gt;% \n  filter(species != \"Adelie\", bill_len &lt; 57)\n\n# Check it out\nhead(penguins)\n##   species island bill_len bill_dep flipper_len body_mass    sex year\n## 1  Gentoo Biscoe     46.1     13.2         211      4500 female 2007\n## 2  Gentoo Biscoe     50.0     16.3         230      5700   male 2007\n## 3  Gentoo Biscoe     48.7     14.1         210      4450 female 2007\n## 4  Gentoo Biscoe     50.0     15.2         218      5700   male 2007\n## 5  Gentoo Biscoe     47.6     14.5         215      5400   male 2007\n## 6  Gentoo Biscoe     46.5     13.5         210      4550 female 2007\n\nOur goal is to build a model that we can use to get good predictions of penguins’ flipper (“arm”) lengths.\nConsider 2 simple linear regression models of flipper_len by penguin sex and species:\n\n# Model 1: R-squared = 0.1205\nsummary(lm(flipper_len ~ sex, penguins))\n## \n## Call:\n## lm(formula = flipper_len ~ sex, data = penguins)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -27.22 -10.22   3.78   8.78  17.37 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  205.220      1.197 171.434  &lt; 2e-16 ***\n## sexmale        8.408      1.679   5.007  1.3e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 11.42 on 183 degrees of freedom\n##   (4 observations deleted due to missingness)\n## Multiple R-squared:  0.1205, Adjusted R-squared:  0.1157 \n## F-statistic: 25.07 on 1 and 183 DF,  p-value: 1.297e-06\n\n# Model 2: R-squared = 0.7014\nsummary(lm(flipper_len ~ species, penguins))\n## \n## Call:\n## lm(formula = flipper_len ~ species, data = penguins)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -18.045  -5.045  -1.045   3.955  15.955 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   196.0448     0.8065  243.07   &lt;2e-16 ***\n## speciesGentoo  21.0372     1.0039   20.96   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.602 on 187 degrees of freedom\n## Multiple R-squared:  0.7014, Adjusted R-squared:  0.6998 \n## F-statistic: 439.2 on 1 and 187 DF,  p-value: &lt; 2.2e-16\n\nHow might we improve our predictions of flipper_len using only these 2 predictors? What do you think is a reasonable range of possible values for the new R-squared?\nEXAMPLE 2\nConsider a simple linear regression model of flipper_len by bill_len:\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = bill_len)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nThoughts? What’s going on here? How does this highlight the limitations of a simple linear regression model?\nEXAMPLE 3\nThe cps dataset contains employment information collected by the U.S. Current Population Survey (CPS) in 2018. We can use these data to explore wages among 18-34 year olds. The original codebook is here.\n\n# Import data\ncps &lt;- read_csv(\"https://mac-stat.github.io/data/cps_2018.csv\") %&gt;% \n  select(-education, -hours) %&gt;% \n  filter(age &gt;= 18, age &lt;= 34) %&gt;% \n  filter(wage &lt; 250000)\n\n\n# Check it out\nhead(cps)\n## # A tibble: 6 × 6\n##    wage   age marital industry   health    education_level\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;          \n## 1 75000    33 single  management fair      bachelors      \n## 2 33000    19 single  management very_good bachelors      \n## 3 43000    33 married management good      bachelors      \n## 4 50000    32 single  management excellent HS             \n## 5 14400    28 single  service    excellent HS             \n## 6 33000    31 married management very_good bachelors\n\nWe can use a simple linear regression model to summarize the relationship of wage with marital status:\n\n# Build the model\nwage_mod &lt;- lm(wage ~ marital, data = cps)\n\n# Summarize the model\ncoef(summary(wage_mod))\n##                Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)    46145.23    921.062  50.10002 0.000000e+00\n## maritalsingle -17052.37   1127.177 -15.12839 5.636068e-50\n\nWhat do you / don’t you conclude from this model? How does it highlight the limitations of a simple linear regression model?\nReflection: Why are multiple regression models so useful?\nWe can put more than 1 predictor into a regression model! Adding predictors to models…\n\nPredictive viewpoint: Helps us better predict the response\nDescriptive viewpoint: Helps us better understand the isolated (causal) effect of a variable by holding constant confounders\n\nMultiple linear regression model formula\nIn general, a multiple linear regression model of \\(Y\\) with multiple predictors \\((X_1, X_2, ..., X_p)\\) is represented by the following formula:\n\\[E[Y \\mid X_1, X_2, ..., X_p] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\]",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-1-visualizing-the-relationship",
    "href": "activities/08-mlr-intro.html#exercise-1-visualizing-the-relationship",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 1: Visualizing the relationship",
    "text": "Exercise 1: Visualizing the relationship\nWe’ve learned how to visualize the relationship of flipper_len by bill_len alone:\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = bill_len)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nHow might we change the scatterplot points to also indicate information about penguin species?\n\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = bill_len, color = species)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = bill_len, shape = species)) + \n  geom_point()",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-2-visualizing-the-model",
    "href": "activities/08-mlr-intro.html#exercise-2-visualizing-the-model",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 2: Visualizing the model",
    "text": "Exercise 2: Visualizing the model\nWe’ve also learned that a simple linear regression model of flipper_len by bill_len alone can be represented by a line:\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = bill_len)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\nReflecting on your plot of flipper_len by bill_len and species in Exercise 1, how do you think a multiple regression model of flipper_len using both of these predictors would be represented? Check your intuition below by modifying the code below to include species in this plot, as you did in Exercise 1.\n\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = bill_len, color = species)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-3-intuition",
    "href": "activities/08-mlr-intro.html#exercise-3-intuition",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 3: Intuition",
    "text": "Exercise 3: Intuition\nYour plot in Exercise 2 demonstrated that the multiple linear regression model of flipper_len by bill_len and species is represented by 2 lines.\nLet’s interpret the punchlines!\nFor each question, provide an answer along with evidence from the model lines that supports your answer.\n\nWhat’s the relationship between flipper_len and species, no matter a penguin’s bill_len?\n\n\nResponse: Put your response here.\n\n\nWhat’s the relationship between flipper_len and bill_len, no matter a penguin’s species?\n\n\nResponse: Put your response here.\n\n\nDoes the rate of increase in flipper_len with bill_len differ between the two species?\n\n\nResponse: Put your response here.",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-4-model-formula",
    "href": "activities/08-mlr-intro.html#exercise-4-model-formula",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 4: Model formula",
    "text": "Exercise 4: Model formula\nOf course, there’s a formula behind the multiple regression model. We can obtain this using the usual lm() function.\n\n# Build the model\npenguin_mod &lt;- lm(flipper_len ~ bill_len + species, data = penguins)\n\n# Summarize the model\ncoef(summary(penguin_mod))\n##                 Estimate Std. Error  t value     Pr(&gt;|t|)\n## (Intercept)   127.753693  6.1174521 20.88348 1.194472e-50\n## bill_len        1.402367  0.1249665 11.22194 1.194932e-22\n## speciesGentoo  22.848036  0.7938292 28.78206 1.981732e-70\n\n\nIn the lm() function, how did we communicate that we wanted to model flipper_len by both bill_len and species? bill_len + species\nComplete the following model formula:\nE[flipper_len | bill_len, speciesGentoo] = 127.753693 + 1.402367 * bill_len + 22.848036 * speciesGentoo",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-5-sub-model-formulas",
    "href": "activities/08-mlr-intro.html#exercise-5-sub-model-formulas",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 5: Sub-model formulas",
    "text": "Exercise 5: Sub-model formulas\nOk. We now have a single formula for the model.\nAnd we observed earlier that this formula is represented by two lines: one describing the relationship between flipper_len and bill_len for Chinstrap penguins and the other for Gentoo penguins.\nLet’s bring these ideas together.\nUtilize the model formula to obtain the equations of these two lines, i.e. to obtain the sub-model formulas for the 2 species. Hint: Plug speciesGentoo = 1 and speciesGentoo = 0\nChinstrap: flipper_len = 127.753693 + 1.402367* bill_len\nGentoo: flipper_len = (127.753693 +22.848036) + 1.402367 * bill_len",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-6-coefficients-physical-interpretation",
    "href": "activities/08-mlr-intro.html#exercise-6-coefficients-physical-interpretation",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 6: coefficients – physical interpretation",
    "text": "Exercise 6: coefficients – physical interpretation\nReflecting on Exercise 5, let’s interpret what the model coefficients tell us about the physical properties of the two 2 sub-model lines. Choose the correct option given in parentheses:\n\nThe intercept coefficient, 127.75, is the intercept of the line for (Chinstrap / Gentoo) penguins.\nThe bill_len coefficient, 1.40, is the (intercept / slope) of both lines.\nThe speciesGentoo coefficient, 22.85, indicates that the (intercept / slope) of the line for Gentoo is 22.85mm higher than the (intercept / slope) of the line for Chinstrap. Similarly, since the lines are parallel, the line for Gentoo is 22.85mm higher than the line for Chinstrap at any bill_len.",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-7-coefficients-contextual-interpretation",
    "href": "activities/08-mlr-intro.html#exercise-7-coefficients-contextual-interpretation",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 7: coefficients – contextual interpretation",
    "text": "Exercise 7: coefficients – contextual interpretation\nNext, interpret each coefficient in a contextually meaningful way. What do they tell us about penguin flipper lengths?!\n\nInterpret 127.75 (intercept of the Chinstrap line).\nInterpret 1.40 (slope of both lines). For both Chinstrap and Gentoo penguins, we expect…\nInterpret 22.85. At any bill_len, we expect…",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-8-prediction",
    "href": "activities/08-mlr-intro.html#exercise-8-prediction",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 8: Prediction",
    "text": "Exercise 8: Prediction\nNow that we better understand the model, let’s use it to predict flipper lengths! Recall the model summary and visualization:\n\ncoef(summary(penguin_mod))\n##                 Estimate Std. Error  t value     Pr(&gt;|t|)\n## (Intercept)   127.753693  6.1174521 20.88348 1.194472e-50\n## bill_len        1.402367  0.1249665 11.22194 1.194932e-22\n## speciesGentoo  22.848036  0.7938292 28.78206 1.981732e-70\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = bill_len, color = species)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\nPredict the flipper length of a Chinstrap penguin with a 50mm long bill. Make sure your calculation is consistent with the plot.\n\n\n127.75 + 1.40*___ + 22.85*___\n## Error in parse(text = input): &lt;text&gt;:1:16: unexpected input\n## 1: 127.75 + 1.40*__\n##                    ^\n\n\nPredict the flipper length of a Gentoo penguin with a 50mm long bill. Make sure your calculation is consistent with the plot.\n\n\n127.75 + 1.40*___ + 22.85*___\n## Error in parse(text = input): &lt;text&gt;:1:16: unexpected input\n## 1: 127.75 + 1.40*__\n##                    ^\n\n\nUse the predict() function to confirm your predictions in parts a and b.\n\n\n# Confirm the calculation in part a\npredict(penguin_mod,\n        newdata = data.frame(bill_len = ___, species = \"___\"))\n\n# Confirm the calculation in part b\npredict(penguin_mod,\n        newdata = data.frame(bill_len = ___, species = \"___\"))\n## Error in parse(text = input): &lt;text&gt;:3:42: unexpected input\n## 2: predict(penguin_mod,\n## 3:         newdata = data.frame(bill_len = __\n##                                             ^",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-9-r-squared",
    "href": "activities/08-mlr-intro.html#exercise-9-r-squared",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 9: R-squared",
    "text": "Exercise 9: R-squared\nFinally, recall that improving our predictions was one motivation for multiple linear regression (using 2 predictors instead of 1). To this end, consider the R-squared values of the simple linear regression models that use just one predictor at a time:\n\nmod_bill &lt;- lm(flipper_len ~ bill_len, data = penguins)\nsummary(mod_bill)\n## \n## Call:\n## lm(formula = flipper_len ~ bill_len, data = penguins)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -30.441 -10.998   3.015   8.942  19.881 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept) 177.4971    13.6676  12.987   &lt;2e-16 ***\n## bill_len      0.6712     0.2850   2.355   0.0195 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 11.91 on 187 degrees of freedom\n## Multiple R-squared:  0.02881,    Adjusted R-squared:  0.02362 \n## F-statistic: 5.548 on 1 and 187 DF,  p-value: 0.01954\n\nmod_species &lt;- lm(flipper_len ~ species, data = penguins)\nsummary(mod_species)\n## \n## Call:\n## lm(formula = flipper_len ~ species, data = penguins)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -18.045  -5.045  -1.045   3.955  15.955 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   196.0448     0.8065  243.07   &lt;2e-16 ***\n## speciesGentoo  21.0372     1.0039   20.96   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 6.602 on 187 degrees of freedom\n## Multiple R-squared:  0.7014, Adjusted R-squared:  0.6998 \n## F-statistic: 439.2 on 1 and 187 DF,  p-value: &lt; 2.2e-16\n\n\nIf you had to use only 1 of our 2 predictors, which would give the better predictions of flipper_len?\nWhat do you guess is the R-squared of our multiple regression model that uses both of these predictors? Why?\nCheck your intuition. How does the R-squared of our multiple regression model compare to that of the 2 separate simple linear regression models?\n\n\nsummary(penguin_mod)\n## \n## Call:\n## lm(formula = flipper_len ~ bill_len + species, data = penguins)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -15.4763  -3.2260  -0.1525   3.1581  15.5303 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   127.7537     6.1175   20.88   &lt;2e-16 ***\n## bill_len        1.4024     0.1250   11.22   &lt;2e-16 ***\n## speciesGentoo  22.8480     0.7938   28.78   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 5.112 on 186 degrees of freedom\n## Multiple R-squared:  0.8219, Adjusted R-squared:   0.82 \n## F-statistic: 429.3 on 2 and 186 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-1-visualizing-the-relationship-1",
    "href": "activities/08-mlr-intro.html#exercise-1-visualizing-the-relationship-1",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 1: Visualizing the relationship",
    "text": "Exercise 1: Visualizing the relationship\n\nThere are multiple options!\n\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = bill_len, color = species)) + \n  geom_point()\n\n\n\n\n\n\n\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = bill_len, shape = species)) + \n  geom_point()",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-2-visualizing-the-model-1",
    "href": "activities/08-mlr-intro.html#exercise-2-visualizing-the-model-1",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 2: Visualizing the model",
    "text": "Exercise 2: Visualizing the model\n\n\n\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_len, x = bill_len, color = species)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-3-intuition-1",
    "href": "activities/08-mlr-intro.html#exercise-3-intuition-1",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 3: Intuition",
    "text": "Exercise 3: Intuition\n\nGentoo tend to have longer flippers.\nFlipper length is positively associated with bill length.\nNo. the lines are parallel / have the same slopes.",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-4-model-formula-1",
    "href": "activities/08-mlr-intro.html#exercise-4-model-formula-1",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 4: Model formula",
    "text": "Exercise 4: Model formula\n\nbill_len + species\nE[flipper_len | bill_len, speciesGentoo] = 127.75 + 1.40 * bill_len + 22.85 * speciesGentoo",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-5-sub-model-formulas-1",
    "href": "activities/08-mlr-intro.html#exercise-5-sub-model-formulas-1",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 5: Sub-model formulas",
    "text": "Exercise 5: Sub-model formulas\nChinstrap: flipper_len = 127.75 + 1.40 bill_len\nGentoo: flipper_len = (127.75 + 22.85) + 1.40 bill_len = 150.6 + 1.40 bill_len",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-6-coefficients-physical-interpretation-1",
    "href": "activities/08-mlr-intro.html#exercise-6-coefficients-physical-interpretation-1",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 6: coefficients – physical interpretation",
    "text": "Exercise 6: coefficients – physical interpretation\n\nThe intercept coefficient, 127.75, is the intercept of the line for Chinstrap penguins.\nThe bill_len coefficient, 1.40, is the slope of both lines.\nThe speciesGentoo coefficient, 22.85, indicates that the intercept of the line for Gentoo is 22.85mm higher than the intercept of the line for Chinstrap. Similarly, since the lines are parallel, the line for Gentoo is 22.85mm higher than the line for Chinstrap at any bill_len.",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-7-coefficients-contextual-interpretation-1",
    "href": "activities/08-mlr-intro.html#exercise-7-coefficients-contextual-interpretation-1",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 7: coefficients – contextual interpretation",
    "text": "Exercise 7: coefficients – contextual interpretation\n\nFor both Chinstrap and Gentoo penguins, average flipper lengths increase by 1.40mm for every additional mm in bill length.\nAt any bill_len, the average flipper length for Gentoo penguins is 22.85mm longer than that for Chinstrap penguins.",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-8-prediction-1",
    "href": "activities/08-mlr-intro.html#exercise-8-prediction-1",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 8: Prediction",
    "text": "Exercise 8: Prediction\n\n# a\n127.75 + 1.40*50 + 22.85*0\n## [1] 197.75\n\n# b\n127.75 + 1.40*50 + 22.85*1\n## [1] 220.6\n\n# c\npredict(penguin_mod,\n        newdata = data.frame(bill_len = 50, \n                             species = \"Chinstrap\"))\n##       1 \n## 197.872\npredict(penguin_mod,\n        newdata = data.frame(bill_len = 50, \n                             species = \"Gentoo\"))\n##        1 \n## 220.7201",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/08-mlr-intro.html#exercise-9-r-squared-1",
    "href": "activities/08-mlr-intro.html#exercise-9-r-squared-1",
    "title": "Multiple Linear Regression - Intro",
    "section": "Exercise 9: R-squared",
    "text": "Exercise 9: R-squared\n\nspecies\nno wrong answer\nIt’s higher than the R-squared when we use either predictor alone!",
    "crumbs": [
      "Multiple Linear Regression - Intro"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html",
    "href": "activities/10-mlr-confounding.html",
    "title": "Multiple Linear Regression - Confounding",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#learning-goals",
    "href": "activities/10-mlr-confounding.html#learning-goals",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be familiar with:\n\nconfounding variables\nhow to control for confounding variables in our models\nhow to represent the role of confounding variables using causal diagrams",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#readings-and-videos",
    "href": "activities/10-mlr-confounding.html#readings-and-videos",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Readings and videos",
    "text": "Readings and videos\nBefore class you should have read and watched:\n\nSections 3.9.2 in the STAT 155 Notes\nConfounding (and other causal diagrams)\n\nWatch from 0:00 - 6:54",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-1-review",
    "href": "activities/10-mlr-confounding.html#exercise-1-review",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 1: Review",
    "text": "Exercise 1: Review\nThe peaks data includes information on hiking trails in the 46 “high peaks” in the Adirondack mountains of northern New York state:\n\n# Load useful packages and data\nlibrary(tidyverse)\npeaks &lt;- read_csv(\"https://mac-stat.github.io/data/high_peaks.csv\") %&gt;%\n    mutate(ascent = ascent / 1000)\n\n# Check it out \nhead(peaks)\n## # A tibble: 6 × 7\n##   peak           elevation difficulty ascent length  time rating   \n##   &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n## 1 Mt. Marcy           5344          5   3.17   14.8  10   moderate \n## 2 Algonquin Peak      5114          5   2.94    9.6   9   moderate \n## 3 Mt. Haystack        4960          7   3.57   17.8  12   difficult\n## 4 Mt. Skylight        4926          7   4.26   17.9  15   difficult\n## 5 Whiteface Mtn.      4867          4   2.54   10.4   8.5 easy     \n## 6 Dix Mtn.            4857          5   2.8    13.2  10   moderate\n\nBelow is a model of the time (in hours) that it takes to complete a hike by the hike’s length (in miles), vertical ascent(in 1000s of feet), and rating (easy, moderate, or difficult):\n\npeaks_model &lt;- lm(time ~ length + ascent + rating, data = peaks)\ncoef(summary(peaks_model))\n\nInterpret the length and ratingeasy coefficients in the model formula below by using our strategy:\n\nStrategy: When interpreting a coefficient for a variable x, compare two units whose values of x differ by 1 but who are identical for all other variables.\n\nE[time | length, ascent, rating] = 6.511 + 0.459 length + 0.187 ascent - 3.169 ratingeasy - 2.477 ratingmoderate\n\nSynthesis:\n\nInterpreting the coefficient \\(\\beta_Q\\) for a quantitative variable Q:\n\nHolding all other variables constant, each unit increase in Q is associated with \\(\\beta_Q\\) change (note if it’s an increase or decrease) in Y on average.\n\nInterpreting the coefficient \\(\\beta_C\\) for an indicator(or categorical) variable:\n\nHolding all other variables constant, the average outcome for the group referenced by this indicator (group for whom indicator = 1), is \\(\\beta_C\\) higher/lower than that of the reference group.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#live-note-taking",
    "href": "activities/10-mlr-confounding.html#live-note-taking",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Live Note Taking!",
    "text": "Live Note Taking!",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-2-confounders",
    "href": "activities/10-mlr-confounding.html#exercise-2-confounders",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 2: Confounders",
    "text": "Exercise 2: Confounders\n\nResearch question: Is there a wage gap, hence possibly discrimination, by marital status among 18-34 year olds?\n\nTo explore, we can revisit the cps data with employment information collected by the U.S. Current Population Survey (CPS) in 2018. View the codebook here.\n\n# Import data\ncps &lt;- read_csv(\"https://mac-stat.github.io/data/cps_2018.csv\") %&gt;% \n    filter(age &gt;= 18, age &lt;= 34) %&gt;% \n    filter(wage &lt; 250000)\n\n# Check it out\nhead(cps)\n## # A tibble: 6 × 8\n##    wage   age education marital industry   health    hours education_level\n##   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;          \n## 1 75000    33        16 single  management fair         40 bachelors      \n## 2 33000    19        16 single  management very_good    40 bachelors      \n## 3 43000    33        16 married management good         40 bachelors      \n## 4 50000    32        12 single  management excellent    40 HS             \n## 5 14400    28        12 single  service    excellent    40 HS             \n## 6 33000    31        16 married management very_good    45 bachelors\n\nRecall that a simple linear regression model of wage by marital suggests that single workers make $17,052 less than married workers:\n\nwage_model_1 &lt;- lm(wage ~ marital, data = cps)\ncoef(summary(wage_model_1))\n\nThat’s a big gap!!\nBUT this model ignores important confounding variables that might help explain this gap.\nA confounding variable is a cause of both the predictor of interest (marital) and of the response variable (wage).\nWe can represent this idea with a causal diagram:\n\n\n\n\n\n\n\n\n\nAnother definition of a confounding variable is one that\n\nis a cause of the outcome (wage)\nis associated with the main variable of interest (marital status)\nNOT caused by the variable of interest\n\nWe can represent this on the causal diagram with a line from the confounder to the variable of interest (instead of an arrow):\n\n\n\n\n\n\n\n\n\nName at least 2 potential confounders.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-2b-how-why-do-confounders-bias-results",
    "href": "activities/10-mlr-confounding.html#exercise-2b-how-why-do-confounders-bias-results",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 2b: How & why do confounders bias results?",
    "text": "Exercise 2b: How & why do confounders bias results?\nUnaccounted-for confounders are often a source of bias in our models, meaning that when we ignore them, we often over- or under-estimate the true underlying relationship between a predictor and response variable. To explore why this is important, let’s first look at how our focal predictor marital is associated with our response variable, wage:\n\ncps %&gt;%\n  ggplot(aes(x=marital, y=wage))+\n  geom_boxplot()+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow, let’s consider age as a potential confounder. The following plot shows how age is associated with marital status:\n\ncps %&gt;%\n  ggplot(aes(x=age, y=marital))+\n  geom_boxplot()+\n  theme_classic()\n\n\n\n\n\n\n\n\n…this should make sense, because the older a person is, the more likely they are to be married. Similarly, we can show how age is associated with wage:\n\ncps %&gt;%\n  ggplot(aes(x=age, y=wage))+\n  geom_point()+\n  geom_smooth(method=\"lm\", se=F)+\n  theme_classic()\n\n\n\n\n\n\n\n\nHere we see that there is a positive correlation between age and wages (which again makes sense, because people who have been in the workforce longer typically earn more).\nLet’s revisit our initial plot showing the relationship between marital status and wages:\n\ncps %&gt;%\n  ggplot(aes(x=marital, y=wage))+\n  geom_boxplot()+\n  theme_classic()\n\n\n\n\n\n\n\n\nSince we now know that age is associated with both being married and higher wages, this plot doesn’t tell the full story–people who are married could simply be earning higher wages because they tend to be older, not necessarily because they are married! Age is therefore a confounder in the relationship between marital status and wages.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-3-controlling-for-confounders",
    "href": "activities/10-mlr-confounding.html#exercise-3-controlling-for-confounders",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 3: Controlling for confounders",
    "text": "Exercise 3: Controlling for confounders\nThe exercise above illustrates that it is important to control or adjust for confounding variables when trying to understand the actual causal relationship between a predictor (e.g. marital) and response (e.g. wage).\n\nSometimes, we can control (adjust) for confounding variables through a carefully designed experiment. For example, in comparing the effectiveness (y) of 2 different cold remedies (x), we might want to control for the age, general health, and severity of symptoms among the participants. How might we do that?\nBUT we’re often working with observational, not experimental, data. Why? Well, explain what an experiment might look like if we wanted to explore the relationship between wage (y) and marital status (x) while controlling for age.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-4-age",
    "href": "activities/10-mlr-confounding.html#exercise-4-age",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 4: Age",
    "text": "Exercise 4: Age\nWe’re in luck.\nWe can control (adjust) for confounding variables by including them in our model!\nThat’s one of the superpowers of multiple linear regression.\nLet’s start simple, by controlling for age in our model of wages by marital status:\n\n# Construct the model\nwage_model_2 &lt;- lm(wage ~ marital + age, cps)\ncoef(summary(wage_model_2))\n\n\nVisualize this model by modifying the code below.\n\n(Note: The last line where we add a geom_line layer adds in trend lines similar to what we might obtain using geom_smooth, but it uses the exact fitted values from our model. geom_smooth, on the other hand, adds in trend lines based on fitting two separate models to the married and single subsets of the data. Try adding geom_smooth(method=\"lm\", se=F, linetype=\"dashed\") to the plot to see how they compare).\n\nggplot(cps, aes(y = wage, x = age, color = marital)) +\n    geom(size = 0.1, alpha = 0.5) +\n    geom_line(aes(y = wage_model_2$fitted.values), linewidth = 0.5)\n\n\nSuppose 2 workers are the same age, but one is married and one is single. By how much do we expect the single worker’s wage to differ from the married worker’s wage? (How does this compare to the $17,052 marital gap among all workers?)\nHow can we interpret the maritalsingle coefficient?",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-5-more-confounders",
    "href": "activities/10-mlr-confounding.html#exercise-5-more-confounders",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 5: More confounders",
    "text": "Exercise 5: More confounders\nLet’s control for even more potential confounders!\nModel wages by marital status while controlling for age and years of education:\n\nwage_model_3 &lt;- lm(wage ~ marital + age + education, cps)\ncoef(summary(wage_model_3))\n\n\nWith so many variables, this is a tough model to visualize. If you had to draw it, how would the model trend appear: 1 point, 2 points, 2 lines, 1 plane, or 2 planes? Explain your rationale. Hint: pay attention to whether your predictors are quantitative or categorical.\nGiven our research question, which coefficient is of primary interest? Interpret this coefficient.\nInterpret the two other coefficients in this model.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-6-even-more",
    "href": "activities/10-mlr-confounding.html#exercise-6-even-more",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 6: Even more",
    "text": "Exercise 6: Even more\nLet’s control for another potential confounder, the job industry in which one works (categorical):\n\nwage_model_4 &lt;- lm(wage ~ marital + age + education + industry, cps)\ncoef(summary(wage_model_4))\n\nIf we had to draw it, this model would appear as 12 planes.\nThe original plane explains the relationship between wage and the 2 quantitative predictors, age and education.\nThen this plane is split into 12 (2*6) individual planes, 1 for each possible combination of marital status (2 possibilities) and industry (6 possibilities).\n\nInterpret the main coefficient of interest for our research question.\nWhen controlling for a worker’s age, marital status, and education level, which industry tends to have the highest wages? The lowest? Note: the following table shows the 6 industries:\n\n\ncps %&gt;% count(industry)",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-7-biggest-model-yet",
    "href": "activities/10-mlr-confounding.html#exercise-7-biggest-model-yet",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 7: Biggest model yet",
    "text": "Exercise 7: Biggest model yet\nBuild a model that helps us explore wage by marital status while controlling for: age, education, job industry, typical number of work hours, and health status.\nStore this model as wage_model_5.\n\nwage_model_5 &lt;- lm(wage ~ marital + age + education + industry + hours + health, cps)\ncoef(summary(wage_model_5))",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-8-reflection",
    "href": "activities/10-mlr-confounding.html#exercise-8-reflection",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 8: Reflection",
    "text": "Exercise 8: Reflection\nTake two workers – one is married and the other is single.\nThe models above provided the following insights into the typical difference in wages for these two groups:\n\n\n\nModel\nAssume the two people have the same…\nWage difference\n\n\n\n\nwage_model_1\nNA\n-$17,052\n\n\nwage_model_2\nage\n-$7,500\n\n\nwage_model_3\nage, education\n-$6,478\n\n\nwage_model_4\nage, education, industry\n-$5,893\n\n\nwage_model_5\nage, education, industry, hours, health\n-$4,993\n\n\n\n\nThough not the case in every analysis, the marital coefficient got closer and closer to 0 as we controlled for more confounders. Explain the significance of this phenomenon, in context - what does it mean?\nDo you still find the wage gap for single vs married people to be meaningfully “large”? Can you think of any remaining factors that might explain part of this remaining gap? Or do you think we’ve found evidence of inequitable wage practices for single vs married workers?",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-9-a-new-extreme-example",
    "href": "activities/10-mlr-confounding.html#exercise-9-a-new-extreme-example",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 9: A new (extreme) example",
    "text": "Exercise 9: A new (extreme) example\nFor a more extreme example of why it’s important to control for confounding variables, let’s return to the diamonds data:\n\n# Import and wrangle the data\ndata(diamonds)\ndiamonds &lt;- diamonds %&gt;% \n    mutate(\n        cut = factor(cut, ordered = FALSE),\n        color = factor(color, ordered = FALSE),\n        clarity = factor(clarity, ordered = FALSE)\n    ) %&gt;% \n    select(price, clarity, cut, color, carat)\nhead(diamonds)\n## # A tibble: 6 × 5\n##   price clarity cut       color carat\n##   &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt; &lt;dbl&gt;\n## 1   326 SI2     Ideal     E      0.23\n## 2   326 SI1     Premium   E      0.21\n## 3   327 VS1     Good      E      0.23\n## 4   334 VS2     Premium   I      0.29\n## 5   335 SI2     Good      J      0.31\n## 6   336 VVS2    Very Good J      0.24\n\nOur goal is to explore how the price of a diamond depends upon its clarity (a measure of quality).\nClarity is classified as follows, in order from best to worst:\n\n\n\nclarity\ndescription\n\n\n\n\nIF\nflawless (no internal imperfections)\n\n\nVVS1\nvery very slightly imperfect\n\n\nVVS2\n” ”\n\n\nVS1\nvery slightly imperfect\n\n\nVS2\n” ”\n\n\nSI1\nslightly imperfect\n\n\nSI2\n” ”\n\n\nI1\nimperfect\n\n\n\n\nCheck out a model of price by clarity. What clarity has the highest average price? The lowest? (This is surprising!)\n\n\ndiamond_model_1 &lt;- lm(price ~ clarity, data = diamonds)\n\n# Get a model summary\ncoef(summary(diamond_model_1))\n\n\nWhat confounding variable might explain these results? What’s your rationale?",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-10-size",
    "href": "activities/10-mlr-confounding.html#exercise-10-size",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 10: Size",
    "text": "Exercise 10: Size\nIt turns out that carat, the size of a diamond, is an important confounding variable.\nLet’s explore what happens when we control for this in our model:\n\ndiamond_model_2 &lt;- lm(price ~ clarity + carat, data = diamonds)\n\n# Get a model summary\ncoef(summary(diamond_model_2))\n\n# Plot the model\ndiamonds %&gt;% \n    ggplot(aes(y = price, x = carat, color = clarity)) + \n    geom_line(aes(y = diamond_model_2$fitted.values))\n\nWhat do you think now?\nWhich clarity has the highest expected price?\nThe lowest?\nProvide numerical evidence from the model.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-11-simpsons-paradox",
    "href": "activities/10-mlr-confounding.html#exercise-11-simpsons-paradox",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 11: Simpson’s Paradox",
    "text": "Exercise 11: Simpson’s Paradox\nControlling for carat didn’t just change the clarity coefficients, hence our understanding of the relationship between price and clarity… It flipped the signs of many of these coefficients.\nThis extreme scenario has a name: Simpson’s paradox.\nCHALLENGE: Explain why this happened and support your argument with graphical evidence.\nHINTS: Think about the causal diagram below. How do you think carat influences clarity? How do you think carat influences price? Make 2 ggplot() that support your answers.\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;% \n    ggplot(aes(y = price, x = carat)) + \n    geom_point()\n\nThe bigger the diamond the bigger the price\n\ndiamonds %&gt;% \n    ggplot(aes(y = carat, x = clarity)) + \n    geom_boxplot()\n\nBUT the bigger the diamond, the more flawed it tends to be. Thus flawed diamonds looked more expensive, but only because they also tend to be bigger (and size is a bigger driver of price).",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-12-final-conclusion",
    "href": "activities/10-mlr-confounding.html#exercise-12-final-conclusion",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 12: Final conclusion",
    "text": "Exercise 12: Final conclusion\nWhat’s your final conclusion about diamond prices?\nWhich diamonds are more expensive: flawed ones or flawless ones?",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-1-review-1",
    "href": "activities/10-mlr-confounding.html#exercise-1-review-1",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 1: Review",
    "text": "Exercise 1: Review\n\npeaks_model &lt;- lm(time ~ length + ascent + rating, data = peaks)\ncoef(summary(peaks_model))\n##                  Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)     6.5106514 1.62983740  3.994663 2.627176e-04\n## length          0.4590819 0.08158314  5.627166 1.465288e-06\n## ascent          0.1874830 0.34215350  0.547950 5.866973e-01\n## ratingeasy     -3.1685224 0.86219113 -3.674965 6.827232e-04\n## ratingmoderate -2.4767827 0.61058560 -4.056405 2.177589e-04\n\n\nlength coefficient:\n\nAmong hikes with the same vertical ascent and challenge rating, each additional mile of the hike is associated with a 0.46 hour increase in completion time on average.\nHolding vertical ascent and challenge rating constant (fixed), each additional mile of the hike is associated with a 0.46 hour increase in completion time on average.\n\nratingeasy coefficient:\n\nAmong hikes with the same length and vertical ascent, the average completion time of easy hikes is 3.2 hours less than that of difficult hikes (reference category).\nHolding constant hike length and vertical ascent, the average completion time of easy hikes is 3.2 hours less than that of difficult hikes.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-2-confounders-1",
    "href": "activities/10-mlr-confounding.html#exercise-2-confounders-1",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 2: Confounders",
    "text": "Exercise 2: Confounders\nage, education, job industry, …\nmarital vs wage:\n\ncps %&gt;%\n  ggplot(aes(x=marital, y=wage))+\n  geom_boxplot()+\n  theme_classic()\n\n\n\n\n\n\n\n\nage vs marital:\n\ncps %&gt;%\n  ggplot(aes(x=age, y=marital))+\n  geom_boxplot()+\n  theme_classic()\n\n\n\n\n\n\n\n\nage vs wage:\n\ncps %&gt;%\n  ggplot(aes(x=age, y=wage))+\n  geom_point()+\n  geom_smooth(method=\"lm\", se=F)+\n  theme_classic()",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-3-controlling-for-confounders-1",
    "href": "activities/10-mlr-confounding.html#exercise-3-controlling-for-confounders-1",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 3: Controlling for confounders",
    "text": "Exercise 3: Controlling for confounders\n\ncreate 2 separate groups that are as similar as possible with respect to these variables. give the groups different remedies.\nwe’d have to get 2 groups that are similar with respect to age, and assign 1 group to get married and 1 group to be single. that would be weird (and unethical).",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-4-age-1",
    "href": "activities/10-mlr-confounding.html#exercise-4-age-1",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 4: Age",
    "text": "Exercise 4: Age\n\n# Construct the model\nwage_model_2 &lt;- lm(wage ~ marital + age, cps)\ncoef(summary(wage_model_2))\n##                 Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)   -19595.948  3691.6998 -5.308110 1.184066e-07\n## maritalsingle  -7500.146  1191.8526 -6.292847 3.545964e-10\n## age             2213.869   120.7701 18.331265 2.035782e-71\n\n\n.\n\n\ncps %&gt;%\nggplot(aes(y = wage, x = age, color = marital)) +\n    geom_point(size = 0.1, alpha = 0.5) +\n    geom_line(aes(y = wage_model_2$fitted.values), size = 0.5)\n\n\n\n\n\n\n\n\nbonus! adding in the geom_smooth layer:\n\ncps %&gt;%\nggplot(aes(y = wage, x = age, color = marital)) +\n  geom_point(size = 0.1, alpha = 0.5) +\n  geom_line(aes(y = wage_model_2$fitted.values), size = 0.5)+\n  geom_smooth(method=\"lm\", se=F, linetype=\"dashed\")\n\n\n\n\n\n\n\n\n\n-$7500\n\nWhen controlling for (“holding constant”) age, single workers make $7500 less than married workers on average.\nAmong workers of the same age, single workers make $7500 less than married workers on average.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-5-more-confounders-1",
    "href": "activities/10-mlr-confounding.html#exercise-5-more-confounders-1",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 5: More confounders",
    "text": "Exercise 5: More confounders\n\nwage_model_3 &lt;- lm(wage ~ marital + age + education, cps)\ncoef(summary(wage_model_3))\n##                 Estimate Std. Error    t value     Pr(&gt;|t|)\n## (Intercept)   -64898.607  4099.8737 -15.829416 2.254709e-54\n## maritalsingle  -6478.094  1119.9345  -5.784351 7.988760e-09\n## age             1676.796   116.3086  14.416777 1.102113e-45\n## education       4285.259   207.2158  20.680173 3.209448e-89\n\n\n2 planes: There are 2 quantitative predictors which form the dimensions of a regression plane (including response, it’s a 3D!). Because marital status is a categorical predictor with two levels, the model produces two such planes—one for each group.\nThe maritalsingle coefficient is of main interest:\n\nAmong workers of the same age and years of education, single workers earn $6478 less than married workers.\n\n\nage coefficient: Among workers of the same marital status and years of education, each additional year of age is associated with a $1677 increase in salary on average.\neducation coefficient: Among workers of the same marital status and age, each additional year of education is associated with a $4285 increase in salary on average.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-6-even-more-1",
    "href": "activities/10-mlr-confounding.html#exercise-6-even-more-1",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 6: Even more",
    "text": "Exercise 6: Even more\n\nwage_model_4 &lt;- lm(wage ~ marital + age + education + industry, cps)\ncoef(summary(wage_model_4))\n##                                   Estimate Std. Error    t value     Pr(&gt;|t|)\n## (Intercept)                     -52498.857  7143.8481 -7.3488206 2.533275e-13\n## maritalsingle                    -5892.842  1105.6898 -5.3295615 1.053631e-07\n## age                               1493.360   116.1673 12.8552586 6.651441e-37\n## education                         3911.117   243.0192 16.0938565 4.500408e-56\n## industryconstruction              5659.082  6218.5649  0.9100302 3.628760e-01\n## industryinstallation_production   1865.650  6109.2613  0.3053806 7.600964e-01\n## industrymanagement                1476.884  6031.2901  0.2448704 8.065727e-01\n## industryservice                  -7930.403  5945.6509 -1.3338158 1.823603e-01\n## industrytransportation           -1084.176  6197.2462 -0.1749448 8.611342e-01\n\n\nAmong workers of the same job industry, education, and age, single workers make $5893 less than a married worker on average.\nhighest = construction (because it has the highest positive coefficient), lowest = service (because it has the most negative coefficient)",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-7-biggest-model-yet-1",
    "href": "activities/10-mlr-confounding.html#exercise-7-biggest-model-yet-1",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 7: Biggest model yet",
    "text": "Exercise 7: Biggest model yet\n\nwage_model_5 &lt;- lm(wage ~ marital + age + education + industry + hours + health, cps)\ncoef(summary(wage_model_5))\n##                                    Estimate Std. Error     t value     Pr(&gt;|t|)\n## (Intercept)                     -64886.5747 6914.18198 -9.38456275 1.171028e-20\n## maritalsingle                    -4992.7685 1061.84882 -4.70195794 2.687274e-06\n## age                               1061.1410  115.83503  9.16079518 9.031462e-20\n## education                         3443.7625  236.12723 14.58435151 1.128646e-46\n## industryconstruction              5381.3857 5959.05620  0.90306007 3.665630e-01\n## industryinstallation_production   2951.0372 5854.23981  0.50408547 6.142365e-01\n## industrymanagement                5107.6364 5782.95334  0.88322283 3.771832e-01\n## industryservice                  -3074.5127 5705.56537 -0.53886207 5.900201e-01\n## industrytransportation            -207.3439 5940.02074 -0.03490626 9.721567e-01\n## hours                              732.1340   43.72488 16.74410733 2.340115e-60\n## healthfair                       -7407.7981 2901.71339 -2.55290483 1.072955e-02\n## healthgood                       -2470.8096 1259.44276 -1.96182766 4.987035e-02\n## healthpoor                       -9086.9110 7657.43781 -1.18667774 2.354441e-01\n## healthvery_good                    292.5278 1020.89213  0.28654136 7.744823e-01",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-8-reflection-1",
    "href": "activities/10-mlr-confounding.html#exercise-8-reflection-1",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 8: Reflection",
    "text": "Exercise 8: Reflection\n\nThese confounders explained away more and more of the wage gap between single and married workers.\nAnswers will vary. A potential factor that we haven’t considered is a worker’s role within a given industry.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-9-a-new-extreme-example-1",
    "href": "activities/10-mlr-confounding.html#exercise-9-a-new-extreme-example-1",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 9: A new (extreme) example",
    "text": "Exercise 9: A new (extreme) example\n\n\n\nclarity\ndescription\n\n\n\n\nIF\nflawless (no internal imperfections)\n\n\nVVS1\nvery very slightly imperfect\n\n\nVVS2\n” ”\n\n\nVS1\nvery slightly imperfect\n\n\nVS2\n” ”\n\n\nSI1\nslightly imperfect\n\n\nSI2\n” ”\n\n\nI1\nimperfect\n\n\n\n\ndiamond_model_1 &lt;- lm(price ~ clarity, data = diamonds)\n\n# Get a model summary\ncoef(summary(diamond_model_1))\n##                  Estimate Std. Error      t value      Pr(&gt;|t|)\n## (Intercept)  3924.1686910   144.5619 27.145247517 3.513547e-161\n## claritySI2   1138.8599147   150.2746  7.578526239  3.550711e-14\n## claritySI1     71.8324571   148.6049  0.483378837  6.288287e-01\n## clarityVS2      0.8207037   148.8672  0.005512992  9.956013e-01\n## clarityVS1    -84.7132999   150.9746 -0.561109670  5.747251e-01\n## clarityVVS2  -640.4316203   154.7737 -4.137858008  3.510944e-05\n## clarityVVS1 -1401.0540535   158.5401 -8.837224284  1.010097e-18\n## clarityIF   -1059.3295848   171.8990 -6.162510636  7.210567e-10\n\n\nhighest = SI2, lowest = VVS1\nwill vary.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-10-size-1",
    "href": "activities/10-mlr-confounding.html#exercise-10-size-1",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 10: Size",
    "text": "Exercise 10: Size\n\ndiamond_model_2 &lt;- lm(price ~ clarity + carat, data = diamonds)\n\n# Get a model summary\ncoef(summary(diamond_model_2))\n##              Estimate Std. Error    t value Pr(&gt;|t|)\n## (Intercept) -6911.566   50.22456 -137.61327        0\n## claritySI2   2879.180   49.47264   58.19742        0\n## claritySI1   3729.449   49.16156   75.86108        0\n## clarityVS2   4388.904   49.38115   88.87813        0\n## clarityVS1   4613.765   50.13112   92.03393        0\n## clarityVVS2  5163.323   51.62127  100.02318        0\n## clarityVVS1  5186.619   53.04830   97.77163        0\n## clarityIF    5513.139   57.36530   96.10582        0\n## carat        8440.057   12.65126  667.13154        0\n\n# Plot the model\ndiamonds %&gt;% \n    ggplot(aes(y = price, x = carat, color = clarity)) + \n    geom_line(aes(y = diamond_model_2$fitted.values))\n\n\n\n\n\n\n\n\nhighest = IF, lowest = I1 (reference category)\nThis is what we would have expected!",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-11-simpsons-paradox-1",
    "href": "activities/10-mlr-confounding.html#exercise-11-simpsons-paradox-1",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 11: Simpson’s Paradox",
    "text": "Exercise 11: Simpson’s Paradox\nThe bigger the diamond the bigger the price:\n\ndiamonds %&gt;% \n    ggplot(aes(y = price, x = carat)) + \n    geom_point()\n\n\n\n\n\n\n\n\nBUT the bigger the diamond, the more flawed it tends to be:\n\ndiamonds %&gt;% \n    ggplot(aes(y = carat, x = clarity)) + \n    geom_boxplot()\n\n\n\n\n\n\n\n\nThus flawed diamonds looked more expensive, but only because they also tend to be bigger (and size is a bigger driver of price).",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/10-mlr-confounding.html#exercise-12-final-conclusion-1",
    "href": "activities/10-mlr-confounding.html#exercise-12-final-conclusion-1",
    "title": "Multiple Linear Regression - Confounding",
    "section": "Exercise 12: Final conclusion",
    "text": "Exercise 12: Final conclusion\nFlawless diamonds are more expensive.",
    "crumbs": [
      "Multiple Linear Regression - Confounding"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html",
    "href": "activities/13+14-mlr-model-building.html",
    "title": "Multiple Linear Regression - Model Building",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#learning-goals",
    "href": "activities/13+14-mlr-model-building.html#learning-goals",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nExplain when variables are redundant or multicollinear.\nRelate redundancy and multicollinearity to coefficient estimates and \\(R^2\\).\nExplain why adjusted \\(R^2\\) is preferable to multiple \\(R^2\\) when comparing models with different numbers of predictors.",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#readings-and-videos",
    "href": "activities/13+14-mlr-model-building.html#readings-and-videos",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Readings and videos",
    "text": "Readings and videos\nToday is a day to discover ideas, so no readings or videos to go through before class, but if you want to see today’s ideas presented in a different way, you can take a look at the following after class:\n\nReading: Section 3.9.5 in the STAT 155 Notes\nVideo: Redundancy and Multicollinearity\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder.",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-1-modeling-bill-length-by-flipper-length",
    "href": "activities/13+14-mlr-model-building.html#exercise-1-modeling-bill-length-by-flipper-length",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 1: Modeling bill length by flipper length",
    "text": "Exercise 1: Modeling bill length by flipper length\nWhat can a penguin’s flipper (arm) length tell us about their bill length? To answer this question, we’ll consider 3 of our models:\n\n\n\nmodel\npredictors\n\n\n\n\npenguin_model_1\nflipper_len\n\n\npenguin_model_2\nflipper_len_cm\n\n\npenguin_model_3\nflipper_len + flipper_len_cm\n\n\n\nPlots of the first two models are below:\n\nlibrary(ggplot2)\nggplot(penguins, aes(y = bill_len, x = flipper_len)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(penguins, aes(y = bill_len, x = flipper_len_cm)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\nBefore examining the model summaries, check your intuition. Do you think the penguin_model_2 R-squared will be less than, equal to, or more than that of penguin_model_1? Similarly, how do you think the penguin_model_3 R-squared will compare to that of penguin_model_1?\nCheck your intuition: Examine the R-squared values for the three penguin models and summarize how these compare.\n\n\nsummary(penguin_model_1)$r.squared\n## [1] 0.430574\nsummary(penguin_model_2)$r.squared\n## [1] 0.430574\nsummary(penguin_model_3)$r.squared\n## [1] 0.430574\n\n\nExplain why your observation in part b makes sense. Support your reasoning with a plot of just the 2 predictors: flipper_len vs flipper_len.\n\n\nggplot(penguins, aes(x = flipper_len, y = flipper_len_cm)) +\n    geom_point()\n\n\n\n\n\n\n\n\n\nOPTIONAL challenge: In summary(penguin_model_3), the flipper_len coefficient is NA. Explain why this makes sense. HINT: Thinking about what you learned about controlling for covariates, why wouldn’t it make sense to interpret this coefficient? BONUS: For those of you that have taken MATH 236, this has to do with matrices that are not of full rank!",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-2-incorporating-body_mass",
    "href": "activities/13+14-mlr-model-building.html#exercise-2-incorporating-body_mass",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 2: Incorporating body_mass",
    "text": "Exercise 2: Incorporating body_mass\nIn this exercise you’ll consider 3 models of bill_len:\n\n\n\nmodel\npredictors\n\n\n\n\npenguin_model_1\nflipper_len\n\n\npenguin_model_4\nbody_mass\n\n\npenguin_model_5\nflipper_len + body_mass\n\n\n\n\nsummary(penguin_model_1)$r.squared\n## [1] 0.430574\nsummary(penguin_model_4)$r.squared\n## [1] 0.3541557\n\n\nWhich is the better predictor of bill_len: flipper_len or body_mass? Provide some numerical evidence.\npenguin_model_5 incorporates both flipper_len and body_mass as predictors. Before examining a model summary, ask your gut: Will the penguin_model_5 R-squared be close to 0.35, close to 0.43, or greater than 0.6?\nCheck your intuition. Report the penguin_model_5 R-squared and summarize how this compares to that of penguin_model_1 and penguin_model_4.\n\n\nsummary(penguin_model_5)$r.squared\n## [1] 0.4328544\n\n\nExplain why your observation in part c makes sense. Support your reasoning with a plot of the 2 predictors: flipper_len vs body_mass.\n\n\nggplot(penguins, aes(x = flipper_len, y = body_mass)) +\n    geom_point()",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-3-redundancy-and-multicollinearity",
    "href": "activities/13+14-mlr-model-building.html#exercise-3-redundancy-and-multicollinearity",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 3: Redundancy and Multicollinearity",
    "text": "Exercise 3: Redundancy and Multicollinearity\nThe exercises above have illustrated special phenomena in multivariate modeling:\n\ntwo predictors are redundant if they contain the same exact information\ntwo predictors are multicollinear if they are strongly associated (they contain very similar information) but are not completely redundant.\n\nRecall that we examined 5 models:\n\n\n\nmodel\npredictors\n\n\n\n\npenguin_model_1\nflipper_len\n\n\npenguin_model_2\nflipper_len_cm\n\n\npenguin_model_3\nflipper_len + flipper_len_cm\n\n\npenguin_model_4\nbody_mass\n\n\npenguin_model_5\nflipper_len + body_mass\n\n\n\n\nWhich model had redundant predictors and which predictors were these?\nWhich model had multicollinear predictors and which predictors were these?\nIn general, what happens to the R-squared value if we add a redundant predictor to a model: will it decrease, stay the same, increase by a small amount, or increase by a significant amount?\nSimilarly, what happens to the R-squared value if we add a multicollinear predictor to a model: will it decrease, stay the same, increase by a small amount, or increase by a significant amount?",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-4-considerations-for-strong-models",
    "href": "activities/13+14-mlr-model-building.html#exercise-4-considerations-for-strong-models",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 4: Considerations for strong models",
    "text": "Exercise 4: Considerations for strong models\nLet’s dive deeper into important considerations when building a strong model. We’ll use a subset of the penguins data for exploring these ideas.\n\n# For illustration purposes only, take a sample of 10 penguins.\n# We'll discuss this code later in the course!\nset.seed(155)\npenguins_small &lt;- sample_n(penguins, size = 10) %&gt;%\n  mutate(flipper_len = jitter(flipper_len))\n\nConsider 3 models of bill length:\n\n# A model with one predictor (flipper_len)\npoly_mod_1 &lt;- lm(bill_len ~ flipper_len, penguins_small)\n\n# A model with two predictors (flipper_len and flipper_len^2)\npoly_mod_2 &lt;- lm(bill_len ~ poly(flipper_len, 2), penguins_small)\n\n# A model with nine predictors (flipper_len, flipper_len^2, ... on up to flipper_len^9)\npoly_mod_9 &lt;- lm(bill_len ~ poly(flipper_len, 9), penguins_small)\n\n\nBefore doing any analysis, which of the three models do you think will be best?\nCalculate the R-squared values of these 3 models. Which model do you think is best?\n\n\nsummary(poly_mod_1)$r.squared\n## [1] 0.7341412\nsummary(poly_mod_2)$r.squared\n## [1] 0.7630516\nsummary(poly_mod_9)$r.squared\n## [1] 1\n\n\nCheck out plots depicting the relationship estimated by these 3 models. Which model do you think is best?\n\n\n# A plot of model 1\nggplot(penguins_small, aes(y = bill_len, x = flipper_len)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n# A plot of model 2\nggplot(penguins_small, aes(y = bill_len, x = flipper_len)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = FALSE)\n\n\n\n\n\n\n\n\n\n# A plot of model 9\nggplot(penguins_small, aes(y = bill_len, x = flipper_len)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", formula = y ~ poly(x, 9), se = FALSE)",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-5-reflecting-on-these-investigations",
    "href": "activities/13+14-mlr-model-building.html#exercise-5-reflecting-on-these-investigations",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 5: Reflecting on these investigations",
    "text": "Exercise 5: Reflecting on these investigations\n\nList 3 of your favorite foods. Now imagine making a dish that combines all of these foods. Do you think it would taste good?\nToo many good things doesn’t make necessarily make a better thing. Model 9 demonstrates that it’s always possible to get a perfect R-squared of 1, but there are drawbacks to putting more and more predictors into our model. Answer the following about model 9:\n\nHow easy would it be to interpret this model?\nWould you say that this model captures the general trend of the relationship between bill_len and flipper_len?\nHow well do you think this model would generalize to penguins that were not included in the penguins_small sample? For example, would you expect these new penguins to fall on the wiggly model 9 curve?",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-6-overfitting",
    "href": "activities/13+14-mlr-model-building.html#exercise-6-overfitting",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 6: Overfitting",
    "text": "Exercise 6: Overfitting\nModel 9 provides an example of a model that is overfit to our sample data. That is, it picks up the tiny details of our data at the cost of losing the more general trends of the relationship of interest. Check out the following xkcd comic. Which plot pokes fun at overfitting?\n\nSome other goodies:",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-7-questioning-r-squared",
    "href": "activities/13+14-mlr-model-building.html#exercise-7-questioning-r-squared",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 7: Questioning R-squared",
    "text": "Exercise 7: Questioning R-squared\nZooming out, explain some limitations of relying on R-squared to measure the strength / usefulness of a model.",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-8-adjusted-r-squared",
    "href": "activities/13+14-mlr-model-building.html#exercise-8-adjusted-r-squared",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 8: Adjusted R-squared",
    "text": "Exercise 8: Adjusted R-squared\nWe’ve seen that, unless a predictor is redundant with another, R-squared will increase. Even if that predictor is strongly multicollinear with another. Even if that predictor isn’t a good predictor! Thus if we only look at R-squared we might get overly greedy. We can check our greedy impulses a few ways. We take a more in depth approach in STAT 253, but one quick alternative is reported right in our model summary() tables. Adjusted R-squared includes a penalty for incorporating more and more predictors. Mathematically (where \\(n\\) is the sample size and \\(p\\) is the number of non-intercept coefficients):\n\\[\n\\text{Adjusted } R^2 = 1 - (1 - R^2) \\left( \\frac{n-1}{n-p-1} \\right)\n\\]\nThus unlike R-squared, Adjusted R-squared can decrease when the information that a predictor contributes to a model isn’t enough to offset the complexity it adds to that model. Consider two models:\n\nexample_1 &lt;- lm(bill_len ~ species, penguins)\nexample_2 &lt;- lm(bill_len ~ species + island, penguins)\nsummary(example_1)\n## \n## Call:\n## lm(formula = bill_len ~ species, data = penguins)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.9338 -2.2049  0.0086  2.0662 12.0951 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)       38.7914     0.2409  161.05   &lt;2e-16 ***\n## speciesChinstrap  10.0424     0.4323   23.23   &lt;2e-16 ***\n## speciesGentoo      8.7135     0.3595   24.24   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.96 on 339 degrees of freedom\n##   (2 observations deleted due to missingness)\n## Multiple R-squared:  0.7078, Adjusted R-squared:  0.7061 \n## F-statistic: 410.6 on 2 and 339 DF,  p-value: &lt; 2.2e-16\nsummary(example_2)\n## \n## Call:\n## lm(formula = bill_len ~ species + island, data = penguins)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -7.9338 -2.2049 -0.0049  2.0951 12.0951 \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)      38.97500    0.44697  87.198   &lt;2e-16 ***\n## speciesChinstrap 10.33204    0.53502  19.312   &lt;2e-16 ***\n## speciesGentoo     8.52988    0.52082  16.378   &lt;2e-16 ***\n## islandDream      -0.47321    0.59729  -0.792    0.429    \n## islandTorgersen  -0.02402    0.61004  -0.039    0.969    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.965 on 337 degrees of freedom\n##   (2 observations deleted due to missingness)\n## Multiple R-squared:  0.7085, Adjusted R-squared:  0.7051 \n## F-statistic: 204.8 on 4 and 337 DF,  p-value: &lt; 2.2e-16\n\n\nCheck out the summaries for the 2 example models. In general, how does a model’s Adjusted R-squared compare to the R-squared? Is it greater, less than, or equal to the R-squared?\nHow did the R-squared change from example model 1 to model 2? How did the Adjusted R-squared change?\nExplain what it is about island that resulted in a decreased Adjusted R-squared. Note: it’s not necessarily the case that island is a bad predictor on its own!",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-1-modeling-bill-length-by-flipper-length-1",
    "href": "activities/13+14-mlr-model-building.html#exercise-1-modeling-bill-length-by-flipper-length-1",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 1: Modeling bill length by flipper length",
    "text": "Exercise 1: Modeling bill length by flipper length\n\n\n\nmodel\npredictors\n\n\n\n\npenguin_model_1\nflipper_len\n\n\npenguin_model_2\nflipper_len\n\n\npenguin_model_3\nflipper_len + flipper_len\n\n\n\nPlots of the first two models are below:\n\nggplot(penguins, aes(y = bill_len, x = flipper_len)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\nggplot(penguins, aes(y = bill_len, x = flipper_len)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\nYour intuition–answers will vary\nThe R-squared values are all the same!\n\n\nsummary(penguin_model_1)$r.squared\n## [1] 0.430574\nsummary(penguin_model_2)$r.squared\n## [1] 0.430574\nsummary(penguin_model_3)$r.squared\n## [1] 0.430574\n\n\nThe two variables are perfectly linearly correlated—they contain exactly the same information!\n\n\nggplot(penguins, aes(x = flipper_len, y = flipper_len)) +\n    geom_point()\n\n\n\n\n\n\n\n\n\nAn NA means that the coefficient couldn’t be estimated. In penguin_model_3, the interpretation of the flipper_len coefficient is the average change in bill length per centimeter change in flipper length, while holding flipper length in millimeters constant…this is impossible! We can’t hold flipper length in millimeters fixed while varying flipper length in centimeters—if one changes the other must. (In linear algebra terms, the matrix underlying our data is not of full rank.)",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-2-incorporating-body_mass-1",
    "href": "activities/13+14-mlr-model-building.html#exercise-2-incorporating-body_mass-1",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 2: Incorporating body_mass",
    "text": "Exercise 2: Incorporating body_mass\nIn this exercise you’ll consider 3 models of bill_len:\n\n\n\nmodel\npredictors\n\n\n\n\npenguin_model_1\nflipper_len\n\n\npenguin_model_4\nbody_mass\n\n\npenguin_model_5\nflipper_len + body_mass\n\n\n\n\nflipper_len is a better predictor than body_mass because penguin_model_1 has an R-squared value of 0.4306 vs 0.3542 for penguin_model_4.\nIntuition check–answers will vary\nR-squared is for penguin_model_5 which is slightly higher than that of penguin_model_1 and penguin_model_4.\n\nd.flipper_len and body_mass are positively correlated and thus contain related information, but not completely redundant information. There’s some information in flipper length in explaining bill length that isn’t captured by body mass, and vice-versa.\n\nggplot(penguins, aes(x = flipper_len, y = body_mass)) +\n    geom_point()",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-3-redundancy-and-multicollinearity-1",
    "href": "activities/13+14-mlr-model-building.html#exercise-3-redundancy-and-multicollinearity-1",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 3: Redundancy and Multicollinearity",
    "text": "Exercise 3: Redundancy and Multicollinearity\n\n\n\nmodel\npredictors\n\n\n\n\npenguin_model_1\nflipper_len\n\n\npenguin_model_2\nflipper_len\n\n\npenguin_model_3\nflipper_len + flipper_len\n\n\npenguin_model_4\nbody_mass\n\n\npenguin_model_5\nflipper_len + body_mass\n\n\n\n\npenguin_model_3 had redundant predictors: `flipper_len and flipper_len\npenguin_model_5 had multicollinear predictors: flipper_len and body_mass were related but not redundant\nR-squared will stay the same if we add a redundant predictor to a model.\nR-squared will increase by a small amount if we add a multicollinear predictor to a model.",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-4-considerations-for-strong-models-1",
    "href": "activities/13+14-mlr-model-building.html#exercise-4-considerations-for-strong-models-1",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 4: Considerations for strong models",
    "text": "Exercise 4: Considerations for strong models\n\nA gut check! Answers will vary\nBased on R-squared: recall that R-squared is interpreted as the proportion of variation in the outcome that our model explains. It would seem that higher is better, so poly_mod_9 might seem to be the best. BUT we’ll see where this reasoning is flawed soon!\nBased on the plots: Answers will vary",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-5-reflecting-on-these-investigations-1",
    "href": "activities/13+14-mlr-model-building.html#exercise-5-reflecting-on-these-investigations-1",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 5: Reflecting on these investigations",
    "text": "Exercise 5: Reflecting on these investigations\n\nsalmon, chocolate, samosas. Together? Yuck!\nRegarding model 9:\n\nNOT easy to interpret.\nNO. It’s much more wiggly than the general trend.\nNOT WELL. It is too tailored to our data.",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-6-overfitting-1",
    "href": "activities/13+14-mlr-model-building.html#exercise-6-overfitting-1",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 6: Overfitting",
    "text": "Exercise 6: Overfitting\nThe bottom left plot pokes fun at overfitting.",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-7-questioning-r-squared-1",
    "href": "activities/13+14-mlr-model-building.html#exercise-7-questioning-r-squared-1",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 7: Questioning R-squared",
    "text": "Exercise 7: Questioning R-squared\nIt measures how well our model explains / predicts our sample data, not how well it explains / predicts the broader population. It also has the feature that any non-redundant predictor added to a model will increase the R-squared.",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/13+14-mlr-model-building.html#exercise-8-adjusted-r-squared-1",
    "href": "activities/13+14-mlr-model-building.html#exercise-8-adjusted-r-squared-1",
    "title": "Multiple Linear Regression - Model Building",
    "section": "Exercise 8: Adjusted R-squared",
    "text": "Exercise 8: Adjusted R-squared\n\nAdjusted R-squared is less than the R-squared\nFrom model 1 to 2, R-squared increased and Adjusted R-squared decreased.\nisland didn’t provide useful information about bill length beyond what was already provided by species.",
    "crumbs": [
      "Multiple Linear Regression - Model Building"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html",
    "href": "activities/17-logistic-regression-II.html",
    "title": "Multiple Logistic Regression",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#learning-goals",
    "href": "activities/17-logistic-regression-II.html#learning-goals",
    "title": "Multiple Logistic Regression",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nConstruct multiple logistic regression models in R\nInterpret coefficients in multiple logistic regression models\nUse multiple logistic regression models to make predictions\nEvaluate the quality of logistic regression models by using predicted probability boxplots and by computing and interpreting accuracy, sensitivity, specificity, false positive rate, and false negative rate",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#readings-and-videos",
    "href": "activities/17-logistic-regression-II.html#readings-and-videos",
    "title": "Multiple Logistic Regression",
    "section": "Readings and videos",
    "text": "Readings and videos\nPlease go through the following reading or videos before class.\n\nReading: Section 4.4 in the STAT 155 Notes\nVideos:\n\nPart 1: Concepts (script)\nPart 2: R Code (script)\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder.",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#exercise-1-graphical-and-numerical-summaries",
    "href": "activities/17-logistic-regression-II.html#exercise-1-graphical-and-numerical-summaries",
    "title": "Multiple Logistic Regression",
    "section": "Exercise 1: Graphical and numerical summaries",
    "text": "Exercise 1: Graphical and numerical summaries\nOur research question involves three categorical variables: received_callback (1 = yes, 0 = no), gender (f = female, m = male), and race (Black, White). Let’s start by creating a mosaic plot to visually compare inferred binary gender and callbacks:\n\n# create mosaic plot of callback vs gender\nggplot(resume) + \n    geom_mosaic(aes(x = product(gender), fill = received_callback)) +\n    scale_fill_manual(\"Received Callback? \\n(1 = yes, 0 = no)\", values = c(\"lightblue\", \"steelblue\")) + \n    labs(x = \"Inferred Binary Gender (f = female, m = male)\", y = \"Received Callback? (1 = yes, 0 = no)\")\n\n\n\n\n\n\n\n\nIn this activity, we’re also interested in looking at the relationship between inferred race and callbacks. One way we can add a third variable to a plot is to use the facet_grid function, particularly when that third variable is categorical. Let’s try that now:\n\n# create mosaic plot of callback vs gender and race\nggplot(resume) + \n    geom_mosaic(aes(x = product(gender), fill = received_callback)) +\n    facet_grid(. ~ race) +\n    scale_fill_manual(\"Received Callback? \\n(1 = yes, 0 = no)\", values = c(\"lightblue\", \"steelblue\")) + \n    labs(x = \"Inferred Binary Gender (f = female, m = male)\", y = \"Received Callback? (1 = yes, 0 = no)\")\n\n\n\n\n\n\n\n\nHere’s another way of looking at the relationship between these three variables, switching the placement of gender and race in the mosaic plot:\n\n# create mosaic plot of callback vs gender and race\nggplot(resume) + \n    geom_mosaic(aes(x = product(received_callback, race), fill = received_callback)) +\n    facet_grid(. ~ gender) +\n    scale_fill_manual(\"Received Callback? \\n(1 = yes, 0 = no)\", values = c(\"lightblue\", \"steelblue\")) + \n    labs(x = \"Inferred Race\", y = \"Received Callback? (1 = yes, 0 = no)\")\n\n\n\n\n\n\n\n\nWhen we are comparing three categorical variables, a useful numerical summary is to calculate relative frequencies/proportions of cases falling into each category of the outcome variable, conditional on which categories of the explanatory variables they fall into. Run this code chunk to calculate the conditional proportion of resumes that did nor did not receive a callback, given the inferred gender and race of the applicant:\n\n# corresponding numerical summaries\nresume %&gt;%\n    group_by(race, gender) %&gt;%\n    count(received_callback) %&gt;%\n    group_by(race, gender) %&gt;%\n    mutate(condprop = n/sum(n))\n## # A tibble: 8 × 5\n## # Groups:   race, gender [4]\n##   race  gender received_callback     n condprop\n##   &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n## 1 black f                      0  1761   0.934 \n## 2 black f                      1   125   0.0663\n## 3 black m                      0   517   0.942 \n## 4 black m                      1    32   0.0583\n## 5 white f                      0  1676   0.901 \n## 6 white f                      1   184   0.0989\n## 7 white m                      0   524   0.911 \n## 8 white m                      1    51   0.0887\n\nWrite a short description that summarizes the information you gain from these visualizations and numerical summaries. Write this summary using good sentences that tell a story and do not resemble a checklist. Don’t forget to consider the context of the data, and make sure that your summary addresses our research question: does an applicant’s inferred gender or race have an effect on the chance that they receive a callback?",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#exercise-2-logistic-regression-modeling",
    "href": "activities/17-logistic-regression-II.html#exercise-2-logistic-regression-modeling",
    "title": "Multiple Logistic Regression",
    "section": "Exercise 2: Logistic regression modeling",
    "text": "Exercise 2: Logistic regression modeling\nNext, we’ll fit a logistic regression model to these data, modeling the log odds of receiving a callback as a function of the applicant’s inferred gender and race:\n\\[\\log(Odds[ReceivedCallback = 1 \\mid gender, race]) = \\beta_0 + \\beta_1 genderm + \\beta_2 racewhite\\]\nFill in the blanks in the code below to fit this logistic regression model.\n\n# fit logistic model and save it as object called \"mod1\"\nmod1 &lt;- glm(received_callback ~ gender + race, data = resume, family = \"binomial\")\n\nThen, run the code chunk below to get the coefficient estimates and exponentiated estimates:\n\n# Original estimates\ncoef(mod1)\n## (Intercept)     genderm   racewhite \n##  -2.6473729  -0.1270306   0.4395841\n\n# Exponentiated estimates\nexp(coef(mod1))\n## (Intercept)     genderm   racewhite \n##  0.07083706  0.88070675  1.55206159\n\nWrite an interpretation of each of the exponentiated coefficients in your logistic regression model.",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#exercise-3-interaction-terms",
    "href": "activities/17-logistic-regression-II.html#exercise-3-interaction-terms",
    "title": "Multiple Logistic Regression",
    "section": "Exercise 3: Interaction terms",
    "text": "Exercise 3: Interaction terms\n\nDo you think it would make sense to add an interaction term (between gender and race) to our logistic regression model? Why/why not?\nLet’s try adding an interaction between gender and race. Update the code below to fit this new interaction model.\n\n\n# fit logistic model and save it as object called \"mod2\"\nmod2 &lt;- glm(received_callback ~ gender * race, data = resume, family = \"binomial\")\n\nThen, run the code chunk below to get the coefficient estimates and exponentiated estimates for this interaction model:\n\n# Original estimates\ncoef(mod2)\n##       (Intercept)           genderm         racewhite genderm:racewhite \n##       -2.64532337       -0.13698360        0.43609385        0.01654707\n\n# Exponentiated estimates\nexp(coef(mod2))\n##       (Intercept)           genderm         racewhite genderm:racewhite \n##         0.0709824         0.8719845         1.5466539         1.0166847\n\n\n(CHALLENGE) Write out the logistic regression model formula separately for males and for females. Based on this how would we interpret the exponentiated coefficients in this model?",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#exercise-4-prediction",
    "href": "activities/17-logistic-regression-II.html#exercise-4-prediction",
    "title": "Multiple Logistic Regression",
    "section": "Exercise 4: Prediction",
    "text": "Exercise 4: Prediction\nWe can use our models to predict whether or not a resume will receive a call back based on the inferred gender and race of the applicant. Run the code below to use the predict() function to predict the probability of getting a call back for four job applicants: a person inferred to be a black female, a person inferred to be black male, a person inferred to be a white female, and a person inferred to be a white male.\n\n# set up data frame with people we want to predict for\npredict_data &lt;- data.frame(\n    gender = c(\"f\", \"m\", \"f\", \"m\"),\n    race = c(\"black\", \"black\", \"white\", \"white\")\n)\nprint(predict_data)\n##   gender  race\n## 1      f black\n## 2      m black\n## 3      f white\n## 4      m white\n\n# prediction based on model without interaction\nmod1 %&gt;%\n    predict(newdata = predict_data, type = \"response\")\n##          1          2          3          4 \n## 0.06615111 0.05872314 0.09905323 0.08828000\n\n# prediction based on model with interaction\nmod2 %&gt;%\n    predict(newdata = predict_data, type = \"response\")\n##          1          2          3          4 \n## 0.06627784 0.05828780 0.09892473 0.08869565\n\nReport and compare the predictions we get from predict(). Do they make sense to you based on your understanding of the data? Combine insights from visualizations and modeling to write a few sentences summarizing findings for our research question: does an applicant’s inferred gender and race have an effect on the chance that they receive a callback after submitting their resume for an open job posting?",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#exercise-5-evaluating-logistic-models-with-plots",
    "href": "activities/17-logistic-regression-II.html#exercise-5-evaluating-logistic-models-with-plots",
    "title": "Multiple Logistic Regression",
    "section": "Exercise 5: Evaluating logistic models with plots",
    "text": "Exercise 5: Evaluating logistic models with plots\nWe’ll fit one more model that adds on to the interaction model to also include years of college, years of work experience, and resume quality. The code below takes our fitted models and stores the predicted probabilities in a variable called .fitted. Then we use boxplots to show the predicted probabilities of receiving a callback in those who actually did and did not receive a callback.\n\nmod3 &lt;- glm(received_callback ~ gender*race + years_college + years_experience + resume_quality, data = resume, family = \"binomial\")\n\n# mod1 predicted probabilities\nresume %&gt;% \n  mutate(.fitted = predict(mod1, newdata = ., type = \"response\")) %&gt;% \n  ggplot(aes(x = factor(received_callback), y = .fitted)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n  \n# mod2 predicted probabilities\nresume %&gt;% \n  mutate(.fitted = predict(mod2, newdata = ., type = \"response\")) %&gt;% \n  ggplot(aes(x = factor(received_callback), y = .fitted)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n \n# mod3 predicted probabilities\nresume %&gt;% \n  mutate(.fitted = predict(mod3, newdata = ., type = \"response\")) %&gt;% \n  ggplot(aes(x = factor(received_callback), y = .fitted)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nY-axis (.fitted): The predicted probability of getting a callback (from logistic model). So, each box represents the distribution of the model’s predicted probabilities for cases that actually had outcome 0 vs outcome 1.]\n\nSummarize what you learn about the ability of the 3 models to differentiate those who actually did and did not receive a callback. What model seems best, and why?\n\n\nAll 3 models show that those who actually received a callback had higher predicted probabilities of a callback. Models 1 and 2 are very similar–although predicted probabilites of callback are high for those who did actually receive a callback, there is substantial overlap in the boxplots. There is more separation between the boxplots in the third model, perhaps model 3 is best in terms of accuracy.\n\n\nIf you had to draw a horizontal line across each of the boxplots that vertically separates the left and right boxplots well, where would you place them?\n\n\nWe would want to place the horizontal lines such that as much of the left boxplot was below the line (low predicted probabilities for those with Y = 0) and as much of the right boxplot was above the line (high predicted probabilities for those with Y = 1).",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#exercise-6-evaluating-logistic-models-with-evaluation-metrics",
    "href": "activities/17-logistic-regression-II.html#exercise-6-evaluating-logistic-models-with-evaluation-metrics",
    "title": "Multiple Logistic Regression",
    "section": "Exercise 6: Evaluating logistic models with evaluation metrics",
    "text": "Exercise 6: Evaluating logistic models with evaluation metrics\nSometimes we may need to go beyond the predicted probabilities from our model and try to classify individuals into one of the two binary outcomes (received or did not receive a callback). How high of a predicted probability would we need from our model in order to be convinced that the person actually got a callback? This is the idea behind the horizontal lines that we drew in the previous exercise.\nLet’s explore using a probability threshold of 0.08 (8%) to make a binary prediction for each case:\n\nIf a model’s predicted probability of getting a callback is greater than or equal to 8.5%, we’ll predict they got a callback.\nIf the predicted probability is below 8%, we’ll predict they didn’t get a callback.\n\nWe can visualize this threshold on our predicted probability boxplots:\n\nmod1 &lt;- glm(received_callback ~ gender + race, data = resume, family = \"binomial\")\nmod2 &lt;- glm(received_callback ~ gender * race, data = resume, family = \"binomial\")\nmod3 &lt;- glm(received_callback ~ gender*race + years_college + years_experience + resume_quality, data = resume, family = \"binomial\")\n# Model 1 output\nmod1_output &lt;- resume %&gt;%\n  mutate(.fitted = predict(mod1, newdata = ., type = \"response\"))\n\n# Model 2 output\nmod2_output &lt;- resume %&gt;%\n  mutate(.fitted = predict(mod2, newdata = ., type = \"response\"))\n\n# Model 3 output\nmod3_output &lt;- resume %&gt;%\n  mutate(.fitted = predict(mod3, newdata = ., type = \"response\"))\nggplot(mod1_output, aes(x = factor(received_callback), y = .fitted)) +\n    geom_boxplot() +\n    geom_hline(yintercept = 0.08, color = \"red\")\n\n\n\n\n\n\n\nggplot(mod2_output, aes(x = factor(received_callback), y = .fitted)) +\n    geom_boxplot() +\n    geom_hline(yintercept = 0.08, color = \"red\")\n\n\n\n\n\n\n\nggplot(mod3_output, aes(x = factor(received_callback), y = .fitted)) +\n    geom_boxplot() +\n    geom_hline(yintercept = 0.08, color = \"red\")\n\n\n\n\n\n\n\n\nNext, we can use our threshold to classify each person in our dataset based on their predicted probability of getting a callback: we’ll predict that everyone with a predicted probability higher than our threshold got a callback, and otherwise they did not. Then, we’ll compare our model’s prediction to the true outcome (whether or not they actually did get a callback).\n\n# get binary predictions for mod1 and compare to truth\nthreshold &lt;- 0.08\nmod1_output %&gt;%\n    mutate(predictCallback = .fitted &gt;= threshold) %&gt;% ## predict callback if probability greater than or equal to threshold\n    count(received_callback, predictCallback) ## compare actual and predicted callbacks\n## # A tibble: 4 × 3\n##   received_callback predictCallback     n\n##               &lt;dbl&gt; &lt;lgl&gt;           &lt;int&gt;\n## 1                 0 FALSE            2278\n## 2                 0 TRUE             2200\n## 3                 1 FALSE             157\n## 4                 1 TRUE              235\n\nmod2_output %&gt;%\n    mutate(predictCallback = .fitted &gt;= threshold) %&gt;%\n    count(received_callback, predictCallback)\n## # A tibble: 4 × 3\n##   received_callback predictCallback     n\n##               &lt;dbl&gt; &lt;lgl&gt;           &lt;int&gt;\n## 1                 0 FALSE            2278\n## 2                 0 TRUE             2200\n## 3                 1 FALSE             157\n## 4                 1 TRUE              235\n\nmod3_output %&gt;%\n    mutate(predictCallback = .fitted &gt;= threshold) %&gt;%\n    count(received_callback, predictCallback)\n## # A tibble: 4 × 3\n##   received_callback predictCallback     n\n##               &lt;dbl&gt; &lt;lgl&gt;           &lt;int&gt;\n## 1                 0 FALSE            2465\n## 2                 0 TRUE             2013\n## 3                 1 FALSE             159\n## 4                 1 TRUE              233\n\nWe can use the count() output to fill create contingency tables of the results. (These tables are also called confusion matrices.)\n\nFill in the confusion matrix for Model 3.\n\n\nModels 1 and 2: (Both models result in the same confusion matrix.)\n\n\n\n\n\nPredict callback\nPredict no callback\nTotal\n\n\n\n\nActually got callback\n235\n157\n392\n\n\nActually did not\n2200\n2278\n4478\n\n\nTotal\n2435\n2435\n4870\n\n\n\n\nModel 3:\n\n\n\n\n\nPredict callback\nPredict no callback\nTotal\n\n\n\n\nActually got callback\n____\n____\n____\n\n\nActually did not\n____\n____\n____\n\n\nTotal\n____\n____\n____\n\n\n\n\nIn-class Notes!\n\n\nNow compute the following evaluation metrics for the models:\n\nModels 1 and 2:\n\nAccuracy: P(Predict Y Correctly)\nSensitivity: P(Predict Y = 1 | Actual Y = 1)\nSpecificity: P(Predict Y = 0 | Actual Y = 0)\nFalse negative rate: P(Predict Y = 0 | Actual Y = 1)\nFalse positive rate: P(Predict Y = 1 | Actual Y = 0)\n\nModel 3:\n\nAccuracy: P(Predict Y Correctly)\nSensitivity: P(Predict Y = 1 | Actual Y = 1)\nSpecificity: P(Predict Y = 0 | Actual Y = 0)\nFalse negative rate: P(Predict Y = 0 | Actual Y = 1)\nFalse positive rate: P(Predict Y = 1 | Actual Y = 0)\n\n\nImagine that we are a career center on a college campus and we want to use this model to help students that are looking for jobs. Consider the consequences of incorrectly predicting whether or not an individual will get a callback. What are the consequences of a false negative? What about a false positive? Which one is worse?\n\n\nFalse Negatives (predicting no callback, but actually got callback): this would be a lost opportunity if a student decided not to submit their resume, thinking they wouldn’t get a callback, when actually they would have. False Positives (predicting callback, but actually didn’t get callback): this would be a disappointment for the student, thinking they were going to get a callback but they ended up not getting one.",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#exercise-1-graphical-and-numerical-summaries-1",
    "href": "activities/17-logistic-regression-II.html#exercise-1-graphical-and-numerical-summaries-1",
    "title": "Multiple Logistic Regression",
    "section": "Exercise 1: Graphical and numerical summaries",
    "text": "Exercise 1: Graphical and numerical summaries\n\nggplot(resume) + \n    geom_mosaic(aes(x = product(gender), fill = received_callback)) +\n    scale_fill_manual(\"Received Callback? \\n(1 = yes, 0 = no)\", values = c(\"lightblue\", \"steelblue\")) + \n    labs(x = \"Inferred Binary Gender (f = female, m = male)\", y = \"Received Callback? (1 = yes, 0 = no)\")\n\n\n\n\n\n\n\n\nggplot(resume) + \n    geom_mosaic(aes(x = product(gender), fill = received_callback)) +\n    facet_grid(. ~ race) +\n    scale_fill_manual(\"Received Callback? \\n(1 = yes, 0 = no)\", values = c(\"lightblue\", \"steelblue\")) + \n    labs(x = \"Inferred Binary Gender (f = female, m = male)\", y = \"Received Callback? (1 = yes, 0 = no)\")\n\n\n\n\n\n\n\n\nggplot(resume) + \n    geom_mosaic(aes(x = product(received_callback, race), fill = received_callback)) +\n    facet_grid(. ~ gender) +\n    scale_fill_manual(\"Received Callback? \\n(1 = yes, 0 = no)\", values = c(\"lightblue\", \"steelblue\")) + \n    labs(x = \"Inferred Race\", y = \"Received Callback? (1 = yes, 0 = no)\")\n\n\n\n\n\n\n\n\nresume %&gt;%\n    group_by(race, gender) %&gt;%\n    count(received_callback) %&gt;%\n    group_by(race, gender) %&gt;%\n    mutate(condprop = n/sum(n))\n## # A tibble: 8 × 5\n## # Groups:   race, gender [4]\n##   race  gender received_callback     n condprop\n##   &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n## 1 black f                      0  1761   0.934 \n## 2 black f                      1   125   0.0663\n## 3 black m                      0   517   0.942 \n## 4 black m                      1    32   0.0583\n## 5 white f                      0  1676   0.901 \n## 6 white f                      1   184   0.0989\n## 7 white m                      0   524   0.911 \n## 8 white m                      1    51   0.0887\n\nOverall, a small proportion of applicants received a callback, with those who were inferred to be black males being least likely to get a callback (5.8%) and those inferred to be white females being most likely to get a callback (9.9%). In general, job applicants whose race was inferred to be white were more likely to receive a callback than those whose race was inferred to be black, regardless of their inferred gender. On the other hand, inferred gender does not seem to have as much of an effect on the chance of receiving a callback, with perhaps just a slight advantage for females.",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#exercise-2-logistic-regression-modeling-1",
    "href": "activities/17-logistic-regression-II.html#exercise-2-logistic-regression-modeling-1",
    "title": "Multiple Logistic Regression",
    "section": "Exercise 2: Logistic regression modeling",
    "text": "Exercise 2: Logistic regression modeling\n\nmod1 &lt;- glm(received_callback ~ gender + race, data = resume, family = \"binomial\")\n\ncoef(mod1)\n## (Intercept)     genderm   racewhite \n##  -2.6473729  -0.1270306   0.4395841\n\nexp(coef(mod1))\n## (Intercept)     genderm   racewhite \n##  0.07083706  0.88070675  1.55206159\n\n\nexp(Intercept): We estimate the odds of getting a callback among those inferred to be black females is only 0.07, meaning that the chance of getting a callback is 0.07 times as large as the chance of not getting a callback (or, inversely, the chance of not getting a callback is 1/0.07 = 14.29) times greater than the chance of getting a callback).\nexp(genderm): Comparing applicants of the same inferred race, we estimate that those inferred to be male have an odds of getting a callback that is 0.88 times as high as (or, equivalently, 12% lower than) the odds of getting a callback for those inferred to be female.\nexp(racewhite): We estimate that the odds of getting a callback are 1.55 times higher for applicants whose race was inferred to be white as compared to those who were inferred to be black but the same gender.",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#exercise-3-interaction-terms-1",
    "href": "activities/17-logistic-regression-II.html#exercise-3-interaction-terms-1",
    "title": "Multiple Logistic Regression",
    "section": "Exercise 3: Interaction terms",
    "text": "Exercise 3: Interaction terms\n\nIncluding an interaction term in our model would allow us to investigate whether the effect of race on getting a callback depends on your gender or, vice versa, if the effect of gender on getting a callback depends on a race. In other words, we could ask questions like: is there more of a discrepancy in callbacks between black and white males than there is among black and white females? Is there more of a discrepancy in callbacks between male and female blacks than there is among male and female whites?\nLet’s try adding an interaction between gender and race. Update the code below to fit this new interaction model.\n\n\nmod2 &lt;- glm(received_callback ~ gender * race, data = resume, family = \"binomial\")\n\ncoef(mod2)\n##       (Intercept)           genderm         racewhite genderm:racewhite \n##       -2.64532337       -0.13698360        0.43609385        0.01654707\n\nexp(coef(mod2))\n##       (Intercept)           genderm         racewhite genderm:racewhite \n##         0.0709824         0.8719845         1.5466539         1.0166847\n\n\nOverall logistic regression model formula in terms of beta coefficients:\n\n\\[\\log(Odds[ReceivedCallback = 1 \\mid gender, race]) = \\beta_0 + \\beta_1 genderm + \\beta_2 racewhite + \\beta_3 genderm \\times racewhite\\]\nThe model formula for males:\n\\[\\log(Odds[ReceivedCallback = 1 \\mid gender=m, race]) = (\\beta_0 + \\beta_1) + (\\beta_2 + \\beta_3) racewhite\\]\nThe model formula for females:\n\\[\\log(Odds[ReceivedCallback = 1 \\mid gender=f, race]) = \\beta_0 + \\beta_2 racewhite\\] Focusing first on the female model formula, we can see that this is a simple logistic regression model.\n\nexp(beta0): Odds of callback for black females\nexp(beta2): This is the odds ratio for race among females. That is, white females have exp(beta2) times the odds of callback than black females.\n\nThen focusing on the male model formula, we can see that this is also a simple logistic regression model.\n\nexp(beta0+beta1): Odds of callback for black males\nexp(beta2+beta3): This is the odds ratio for race among males. White males have exp(beta2+beta3) times the odds of callback than black males.\n\nComparing the male to the female model formula, we have:\n\nexp(beta1): Black males have exp(beta1) times the odds of a callback than black females\nexp(beta3): This tells us how many times higher the odds ratio for race is in males as compared to females.",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#exercise-4-prediction-1",
    "href": "activities/17-logistic-regression-II.html#exercise-4-prediction-1",
    "title": "Multiple Logistic Regression",
    "section": "Exercise 4: Prediction",
    "text": "Exercise 4: Prediction\n\n# set up data frame with people we want to predict for\npredict_data &lt;- data.frame(\n    gender = c(\"f\", \"m\", \"f\", \"m\"),\n    race = c(\"black\", \"black\", \"white\", \"white\")\n)\nprint(predict_data)\n##   gender  race\n## 1      f black\n## 2      m black\n## 3      f white\n## 4      m white\n\n# prediction based on model without interaction\nmod1 %&gt;%\n    predict(newdata = predict_data, type = \"response\")\n##          1          2          3          4 \n## 0.06615111 0.05872314 0.09905323 0.08828000\n\n# prediction based on model with interaction\nmod2 %&gt;%\n    predict(newdata = predict_data, type = \"response\")\n##          1          2          3          4 \n## 0.06627784 0.05828780 0.09892473 0.08869565\n\nThe predicted probabilities from our logistic regression models show that we estimate those inferred to be black males have the lowest chance of receiving a callback (5.87% based on mod1 and 5.83% based on mod2), followed by black females (6.62% and 6.63%), white males (8.83% and 8.87%), and then white females (9.91% and 9.89%). This matches the trend we observed that those inferred to be white have a greater chance of getting a callback, regardless of gender, and that those who are inferred to be female have a slightly higher chance of getting a callback than those inferred to be male.",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#exercise-5-evaluating-logistic-models-with-plots-1",
    "href": "activities/17-logistic-regression-II.html#exercise-5-evaluating-logistic-models-with-plots-1",
    "title": "Multiple Logistic Regression",
    "section": "Exercise 5: Evaluating logistic models with plots",
    "text": "Exercise 5: Evaluating logistic models with plots\n\nmod3 &lt;- glm(received_callback ~ gender*race + years_college + years_experience + resume_quality, data = resume, family = \"binomial\")\n\nresume %&gt;% \n  mutate(.fitted = predict(mod1, newdata = ., type = \"response\")) %&gt;% \n  ggplot(aes(x = factor(received_callback), y = .fitted)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n  \nresume %&gt;% \n  mutate(.fitted = predict(mod2, newdata = ., type = \"response\")) %&gt;% \n  ggplot(aes(x = factor(received_callback), y = .fitted)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nresume %&gt;% \n  mutate(.fitted = predict(mod3, newdata = ., type = \"response\")) %&gt;% \n  ggplot(aes(x = factor(received_callback), y = .fitted)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nAll 3 models show that those who actually received a callback had higher predicted probabilities of a callback. Models 1 and 2 are very similar–although predicted probabilites of callback are high for those who did actually receive a callback, there is substantial overlap in the boxplots. There is more separation between the boxplots in the third model, perhaps model 3 is best in terms of accuracy.\nWe would want to place the vertical lines such that as much of the left boxplot was below the line (low predicted probabilities for those with Y = 0) and as much of the right boxplot was above the line (high predicted probabilities for those with Y = 1).",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/17-logistic-regression-II.html#exercise-6-evaluating-logistic-models-with-evaluation-metrics-1",
    "href": "activities/17-logistic-regression-II.html#exercise-6-evaluating-logistic-models-with-evaluation-metrics-1",
    "title": "Multiple Logistic Regression",
    "section": "Exercise 6: Evaluating logistic models with evaluation metrics",
    "text": "Exercise 6: Evaluating logistic models with evaluation metrics\n\nggplot(mod1_output, aes(x = factor(received_callback), y = .fitted)) +\n    geom_boxplot() +\n    geom_hline(yintercept = 0.08, color = \"red\")\n\n\n\n\n\n\n\nggplot(mod2_output, aes(x = factor(received_callback), y = .fitted)) +\n    geom_boxplot() +\n    geom_hline(yintercept = 0.08, color = \"red\")\n\n\n\n\n\n\n\nggplot(mod3_output, aes(x = factor(received_callback), y = .fitted)) +\n    geom_boxplot() +\n    geom_hline(yintercept = 0.08, color = \"red\")\n\n\n\n\n\n\n\n\nNext, we can use our threshold to classify each person in our dataset based on their predicted probability of getting a callback: we’ll predict that everyone with a predicted probability higher than our threshold got a callback, and otherwise they did not. Then, we’ll compare our model’s prediction to the true outcome (whether or not they actually did get a callback).\n\nthreshold &lt;- 0.08\nmod1_output %&gt;%\n    mutate(predictCallback = .fitted &gt;= threshold) %&gt;% \n    count(received_callback, predictCallback) \n## # A tibble: 4 × 3\n##   received_callback predictCallback     n\n##               &lt;dbl&gt; &lt;lgl&gt;           &lt;int&gt;\n## 1                 0 FALSE            2278\n## 2                 0 TRUE             2200\n## 3                 1 FALSE             157\n## 4                 1 TRUE              235\n\nmod2_output %&gt;%\n    mutate(predictCallback = .fitted &gt;= threshold) %&gt;%\n    count(received_callback, predictCallback)\n## # A tibble: 4 × 3\n##   received_callback predictCallback     n\n##               &lt;dbl&gt; &lt;lgl&gt;           &lt;int&gt;\n## 1                 0 FALSE            2278\n## 2                 0 TRUE             2200\n## 3                 1 FALSE             157\n## 4                 1 TRUE              235\n\nmod3_output %&gt;%\n    mutate(predictCallback = .fitted &gt;= threshold) %&gt;%\n    count(received_callback, predictCallback)\n## # A tibble: 4 × 3\n##   received_callback predictCallback     n\n##               &lt;dbl&gt; &lt;lgl&gt;           &lt;int&gt;\n## 1                 0 FALSE            2465\n## 2                 0 TRUE             2013\n## 3                 1 FALSE             159\n## 4                 1 TRUE              233\n\n\n\n\n\nModels 1 and 2: (Both models result in the same confusion matrix.)\n\n\n\n\n\nPredict callback\nPredict no callback\nTotal\n\n\n\n\nActually got callback\n235\n157\n392\n\n\nActually did not\n2200\n2278\n4478\n\n\nTotal\n2435\n2435\n4870\n\n\n\n\nModel 3:\n\n\n\n\n\nPredict callback\nPredict no callback\nTotal\n\n\n\n\nActually got callback\n233\n159\n392\n\n\nActually did not\n2013\n2465\n4478\n\n\nTotal\n2246\n2624\n4870\n\n\n\n\nNow compute the following evaluation metrics for the models:\n\n\nModels 1 and 2:\n\nAccuracy: P(Predict Y Correctly) = (235 + 2278)/(235 + 157 + 2200 + 2278) = 0.5160164\nSensitivity: P(Predict Y = 1 | Actual Y = 1) = 235/(235 + 157) = 0.5994898\nSpecificity: P(Predict Y = 0 | Actual Y = 0) = 2278/(2200 + 2278) = 0.5087092\nFalse negative rate: P(Predict Y = 0 | Actual Y = 1) = 157/(235 + 157) = 0.4005102 (notice that this is equal to 1 - Sensitivity)\nFalse positive rate: P(Predict Y = 1 | Actual Y = 0) = 2200/(2200 + 2278) = 0.4912908 (notice that this is equal to 1 - Specificity)\n\n\n\nModel 3:\n\nAccuracy: P(Predict Y Correctly) = (233 + 2465)/(233 + 159 + 2013 + 2465) = 0.5540041\nSensitivity: P(Predict Y = 1 | Actual Y = 1) = 233/(233 + 159) = 0.5943878\nSpecificity: P(Predict Y = 0 | Actual Y = 0) = 2465/(2013 + 2465) = 0.550469\nFalse negative rate: P(Predict Y = 0 | Actual Y = 1) = 159/(233 + 159) = 0.4056122 (notice that this is equal to 1 - Sensitivity)\nFalse positive rate: P(Predict Y = 1 | Actual Y = 0) = 2013/(2013 + 2465) = 0.449531 (notice that this is equal to 1 - Specificity)\n\n\n\nImagine that we are a career center on a college campus and we want to use this model to help students that are looking for jobs. Consider the consequences of incorrectly predicting whether or not an individual will get a callback. What are the consequences of a false negative? What about a false positive? Which one is worse?\n\nFalse Negatives (predicting no callback, but actually got callback): this would be a lost opportunity if a student decided not to submit their resume, thinking they wouldn’t get a callback, when actually they would have.\nFalse Positives (predicting callback, but actually didn’t get callback): this would be a disappointment for the student, thinking they were going to get a callback but they ended up not getting one.",
    "crumbs": [
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#learning-goals",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#learning-goals",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Learning goals",
    "text": "Learning goals\nLet \\(\\beta\\) be some population parameter and \\(\\hat{\\beta}\\) be a sample estimate of \\(\\beta\\). Our goals for the day are to:\n\nuse simulation to solidify our understanding of sampling distributions and standard errors\nexplore and compare two approaches to approximating the sampling distribution of \\(\\hat{\\beta}\\):\n\nCentral Limit Theorem (CLT)\nbootstrapping\n\nexplore the impact of sample size on sampling distributions and standard errors",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#readings-and-videos",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#readings-and-videos",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Readings and videos",
    "text": "Readings and videos\nPlease watch/do the following videos and readings before class:\n\nReading: Section 6.7 in the STAT 155 Notes\nVideo 1: sampling distributions\nVideo 2: Central Limit Theorem\nVideo 3: bootstrapping",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#reflect",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#reflect",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "REFLECT",
    "text": "REFLECT\nGreat! We have two options. Here are some things to think about / reflect on:\n\nWe can approximate the sampling distribution and standard error using the CLT. BUT:\n\nthe quality of this approximation hinges upon the validity of the Central Limit theorem which hinges upon the validity of the theoretical model assumptions, as well as a large sample size\nthe CLT uses theoretical formulas for the standard error estimates, thus can feel a little mysterious without a solid foundation in probability theory\n\nWe can approxiate the sampling distribution and standard error using bootstrapping. BUT:\n\nit feels magical. The statistical theory behind bootstrapping is quite complicated, and there are certain obscure cases (none that we will encounter in Stat 155) where the assumptions underlying bootstrapping fail to hold\n\n\nNeither approach is perfect, but they complement one another. Bootrapping in particular, while it cannot and should not replace the CLT, gives us some nice intuition behind the idea of resampling, which is fundamental for hypothesis testing (which we’ll get to shortly!).",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-1-500-samples-of-size-10",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-1-500-samples-of-size-10",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 1: 500 samples of size 10",
    "text": "Exercise 1: 500 samples of size 10\nRecall that we can sample 10 observations from our dataset using sample_n():\n\n# Run this chunk a few times to explore the different samples you get\nfish %&gt;% \n  sample_n(size = 10, replace = TRUE)\n## # A tibble: 10 × 5\n##    River   Station Length Weight Concen\n##    &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n##  1 Wacamaw      10   28.5    303   0.61\n##  2 Wacamaw       8   33      440   0.78\n##  3 Lumber        1   32      894   0.61\n##  4 Wacamaw      10   47.3   1632   1.5 \n##  5 Lumber        4   44     1455   0.73\n##  6 Wacamaw      12   38.2    573   0.59\n##  7 Lumber        4   56     3421   3.1 \n##  8 Wacamaw       8   35.5    550   0.65\n##  9 Lumber        0   47     1616   1.6 \n## 10 Wacamaw      10   47.1   1725   1.8\n\nWe can take a sample and then use the data to estimate the model:\n\n# Run this chunk a few times to explore the different sample models you get\nfish %&gt;% \n  sample_n(size = 10, replace = TRUE) %&gt;% \n  with(lm(Concen ~ Length))\n## \n## Call:\n## lm(formula = Concen ~ Length)\n## \n## Coefficients:\n## (Intercept)       Length  \n##    -1.46680      0.06278\n\nWe can also take multiple unique samples and build a sample model from each.\nThe code below obtains 500 separate samples of 10 fish, and stores the model estimates from each:\n\n# Set the seed so that we all get the same results\nset.seed(155)\n\n# Store the sample models\nsample_models_10 &lt;- mosaic::do(500)*(\n  fish %&gt;% \n    sample_n(size = 10, replace = TRUE) %&gt;% \n    with(lm(Concen ~ Length))\n)\n\n# Check it out\nhead(sample_models_10)\n##    Intercept     Length     sigma r.squared          F numdf dendf .row .index\n## 1 -2.7811151 0.09635286 0.5737186 0.7273700  21.343798     1     8    1      1\n## 2 -3.0684939 0.10953496 0.3797853 0.9347767 114.655478     1     8    1      2\n## 3 -1.2880671 0.05933243 0.7457468 0.3776855   4.855237     1     8    1      3\n## 4 -0.9248832 0.05411299 0.6251621 0.4796674   7.374782     1     8    1      4\n## 5 -1.0882523 0.05349699 0.3441272 0.4985134   7.952571     1     8    1      5\n## 6 -0.4558309 0.04074788 0.7785389 0.1719987   1.661820     1     8    1      6\ndim(sample_models_10)\n## [1] 500   9\n\n\nWhat’s the point of the do() function?!? If you’ve taken any COMP classes, what process do you think do() is a shortcut for?\nWhat is stored in the Intercept, Length, and r.squared columns of the results?\nWe’ll obtain a bootstrapping distribution of \\(\\hat{\\beta}_1\\) by taking many (500, in this case) different samples of every fish in our dataset (171 of them) and exploring the degree to which \\(\\hat{\\beta}_1\\) varies from sample to sample.\n\nEdit the code below to obtain a bootstrapping distribution.\n\n# Set the seed so that we all get the same results\nset.seed(155)\n\n# Store the sample models\nsample_models_boot &lt;- mosaic::do(___)*(\n  fish %&gt;% \n    sample_n(size = ___, replace = TRUE) %&gt;% \n    with(lm(Concen ~ Length))\n)\n## Error in parse(text = input): &lt;text&gt;:5:35: unexpected input\n## 4: # Store the sample models\n## 5: sample_models_boot &lt;- mosaic::do(__\n##                                      ^",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-2-why-resampling-replace-true",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-2-why-resampling-replace-true",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 2: Why “resampling” (replace = TRUE)?",
    "text": "Exercise 2: Why “resampling” (replace = TRUE)?\nLet’s wrap our minds around the idea of resampling, before coming back to our boostrapping distribution, using a small example of 5 fish:\n\n# Define data\nsmall_sample &lt;- data.frame(\n  id = 1:5,\n  Length = c(44, 43, 54, 52, 40))\n\nsmall_sample\n##   id Length\n## 1  1     44\n## 2  2     43\n## 3  3     54\n## 4  4     52\n## 5  5     40\n\nThis sample has a mean Length of 46.6 cm:\n\nsmall_sample %&gt;% \n  summarize(mean(Length))\n##   mean(Length)\n## 1         46.6\n\n\nThe chunk below samples 5 fish without replacement from our small_sample of 5 fish, and calculates their mean length. Run it several times. How do the sample and resulting mean change?\n\n\nsample_1 &lt;- sample_n(small_sample, size = 5, replace = FALSE)\nsample_1\n##   id Length\n## 1  4     52\n## 2  2     43\n## 3  3     54\n## 4  5     40\n## 5  1     44\n\nsample_1 %&gt;% \n  summarize(mean(Length))\n##   mean(Length)\n## 1         46.6\n\n\nSampling our sample without replacement merely returns our original sample. Instead, resample 5 fish from our small_sample with replacement. Run it several times. What do you notice about the samples? About their mean lengths?\n\n\nsample_2 &lt;- sample_n(small_sample, size = 5, replace = TRUE)\nsample_2\n##   id Length\n## 1  1     44\n## 2  5     40\n## 3  1     44\n## 4  4     52\n## 5  5     40\n\nsample_2 %&gt;% \n  summarize(mean(Length))\n##   mean(Length)\n## 1           44\n\nResampling our sample provides insight into the variability, hence potential error, in our sample estimates. (This works better when we have a sample bigger than 5!) As you observed in part b, each resample might include some fish from the original sample several times and others not at all.\nBonus Fact: Sampling with replacement also ensures that our resampled observations are independent, which we need in order for bootstrapping to “work”!",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-3-sampling-distribution",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-3-sampling-distribution",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 3: Sampling distribution",
    "text": "Exercise 3: Sampling distribution\nCheck out the resulting 500 bootstrapped sample models:\n\n# Set the seed so that we all get the same results\nset.seed(155)\n\n# Store the sample models\nsample_models_boot &lt;- mosaic::do(500)*(\n  fish %&gt;% \n    sample_n(size = 171, replace = TRUE) %&gt;% \n    with(lm(Concen ~ Length))\n)\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen)) + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_abline(data = sample_models_boot, \n              aes(intercept = Intercept, slope = Length), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nLet’s focus on the slopes of these 500 sample models.\nA plot of the 500 slopes approximates the sampling distribution of the sample slopes.\n\nsample_models_boot %&gt;% \n  ggplot(aes(x = Length)) + \n  geom_density() + \n  geom_vline(xintercept = 0.05813, color = \"red\") \n\n\n\n\n\n\n\n\nDescribe the sampling distribution:\n\nWhat’s its general shape?\nWhere is it roughly centered?\nRoughly what’s its spread / i.e. what’s the range of estimates you observed?",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-4-standard-error",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-4-standard-error",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 4: Standard error",
    "text": "Exercise 4: Standard error\nFor a more rigorous assessment of the spread among the sample slopes, let’s calculate their standard deviation:\n\nsample_models_boot %&gt;% \n  summarize(sd(Length))\n##    sd(Length)\n## 1 0.005664586\n\nRecall: The standard deviation of sample estimates is called a “standard error”.\nIt measures the typical distance of a sample estimate from the actual population value.\nCompare the bootstrapped standard error to the standard error reported from our regression model (see the Std. Error column):\n\ncoef(summary(fish_model))\n##                Estimate  Std. Error   t value     Pr(&gt;|t|)\n## (Intercept) -1.13164542 0.213614796 -5.297598 3.617750e-07\n## Length       0.05812749 0.005227593 11.119359 6.641225e-22\n\nAre they roughly equivalent?\n\nYour response here",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-5-central-limit-theorem-clt",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-5-central-limit-theorem-clt",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 5: Central Limit Theorem (CLT)",
    "text": "Exercise 5: Central Limit Theorem (CLT)\nRecall that the CLT assumes that, so long as our sample size is “big enough”, the sampling distribution of the sample slope will be Normal.\nSpecifically, all possible sample slopes will vary Normally around the population slope.\n\nDo your simulation results support this assumption? Why or why not?\nWant more intuition into the CLT? Watch this video explanation using bunnies and dragons: https://www.youtube.com/watch?v=jvoxEYmQHNM",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-6-using-the-clt",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-6-using-the-clt",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 6: Using the CLT",
    "text": "Exercise 6: Using the CLT\nLet \\(\\hat{\\beta}_1\\) be an estimate of the (super)population slope parameter \\(\\beta_1\\) calculated from a sample of 10 fish (sample_models_10).\nEstimate the standard error of the slope from these resampled estimates\n\nsample_models_10 %&gt;% \n  summarize(sd(Length))\n##   sd(Length)\n## 1 0.02532877\n\nYou should get a SE of roughly 0.026.\nThus, by the CLT, the sampling distribution of \\(\\hat{\\beta}_1\\) is:\n\\[\\hat{\\beta}_1 \\sim N(\\beta_1, 0.026^2)\\]\nUse this result with the 68-95-99.7 property of the Normal model to understand the potential error in a slope estimate.\n\nThere are many possible samples of 10 fish. What percent of these will produce an estimate \\(\\hat{\\beta}_1\\) that’s within 0.052, i.e. 2 standard errors, of the actual population slope \\(\\beta_1\\)?\nMore than 2 standard errors from \\(\\beta_1\\)?\nMore than 0.079, i.e. 3 standard errors, above \\(\\beta_1\\)?",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-7-clt-and-the-68-95-99.7-rule",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-7-clt-and-the-68-95-99.7-rule",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 7: CLT and the 68-95-99.7 Rule",
    "text": "Exercise 7: CLT and the 68-95-99.7 Rule\nFill in the blanks below to complete some general properties assumed by the CLT:\n\n___% of samples will produce \\(\\hat{\\beta}_1\\) estimates within 1 st. err. of \\(\\beta_1\\)\n___% of samples will produce \\(\\hat{\\beta}_1\\) estimates within 2 st. err. of \\(\\beta_1\\)\n___% of samples will produce \\(\\hat{\\beta}_1\\) estimates within 3 st. err. of \\(\\beta_1\\)",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-8-increasing-sample-size",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-8-increasing-sample-size",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 8: Increasing sample size",
    "text": "Exercise 8: Increasing sample size\nNow that we have a sense of the potential variability and error in sample estimates, let’s consider the impact of sample size.\nSuppose we were to increase our sample size from n = 10 to n = 50 or n = 100 fish. What impact do you anticipate this having on our sample estimates of the population parameters:\n\nDo you expect there to be more or less variability among the sample model lines?\nAround what value would you expect the sampling distribution of sample slopes to be centered?\nWhat general shape would you expect that sampling distribution to have?\nIn comparison to estimates based on the samples of size 10, do you think the estimates based on samples of size 50 will be closer to or farther from the true slope (on average)?",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-9-500-samples-of-size-n",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-9-500-samples-of-size-n",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 9: 500 samples of size n",
    "text": "Exercise 9: 500 samples of size n\nLet’s increase the sample size in our simulation.\n\nset.seed(155)\nsample_models_50 &lt;- mosaic::do(500)*(\n  fish %&gt;% \n    sample_n(size = 50, replace = FALSE) %&gt;% \n    with(lm(Concen ~ Length))\n)\n\n# Check it out\nhead(sample_models_50)\n##   Intercept     Length     sigma r.squared        F numdf dendf .row .index\n## 1 -1.310864 0.06393644 0.5646473 0.4846643 45.14316     1    48    1      1\n## 2 -1.503983 0.06887405 0.6112830 0.5333146 54.85302     1    48    1      2\n## 3 -1.730214 0.07140026 0.5940694 0.4561970 40.26725     1    48    1      3\n## 4 -1.213346 0.05948225 0.5561451 0.4712989 42.78853     1    48    1      4\n## 5 -1.221643 0.05925447 0.5471569 0.5190808 51.80887     1    48    1      5\n## 6 -1.384057 0.06532957 0.5610290 0.5400105 56.35021     1    48    1      6\n\nSimilarly, take 500 samples of size 100, and build a sample model from each.\n\nset.seed(155)\nsample_models_100 &lt;- mosaic::do(500)*(\n  fish %&gt;% \n    sample_n(size = 100, replace = FALSE) %&gt;% \n    with(lm(Concen ~ Length))\n)\n\n# Check it out\nhead(sample_models_100)\n##    Intercept     Length     sigma r.squared         F numdf dendf .row .index\n## 1 -0.9745758 0.05400227 0.5965267 0.3361746  49.62917     1    98    1      1\n## 2 -1.1187614 0.05655020 0.5864780 0.4173361  70.19301     1    98    1      2\n## 3 -1.3507522 0.06512980 0.5629381 0.5332227 111.95023     1    98    1      3\n## 4 -1.5201200 0.06712467 0.5220515 0.5113787 102.56432     1    98    1      4\n## 5 -1.0181036 0.05525131 0.5746367 0.4031468  66.19447     1    98    1      5\n## 6 -1.1369532 0.05868566 0.5773007 0.4378596  76.33367     1    98    1      6",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-10-impact-of-sample-size-part-i",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-10-impact-of-sample-size-part-i",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 10: Impact of sample size (part I)",
    "text": "Exercise 10: Impact of sample size (part I)\nCompare and contrast the 500 sets of sample models when using samples of size 10, 50, and 100.\n\n# 500 sample models using samples of size 10\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_abline(data = sample_models_10, \n              aes(intercept = Intercept, slope = Length), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\n\n# 500 sample models using samples of size 50\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_abline(data = sample_models_50, \n              aes(intercept = Intercept, slope = Length), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\n\n# 500 sample models using samples of size 100\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_abline(data = sample_models_100, \n              aes(intercept = Intercept, slope = Length), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\n\nWhat happens to our sample models as sample size increases? Was this what you expected?",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-11-impact-of-sample-size-part-ii",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-11-impact-of-sample-size-part-ii",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 11: Impact of sample size (part II)",
    "text": "Exercise 11: Impact of sample size (part II)\nLet’s focus on just the sampling distributions of our 500 slope estimates \\(\\hat{\\beta}_1\\).\nFor easy comparison, plot the estimates based on samples of size 10, 50, and 100 on the same frame:\n\n# Don't think too hard about this code!\n# Combine the estimates & sample size into a new data set\n# Then plot it\n\ndata.frame(estimates = c(sample_models_10$Length, sample_models_50$Length, sample_models_100$Length),\n           sample_size = rep(c(\"10\",\"50\",\"100\"), each = 500)) %&gt;% \n  mutate(sample_size = fct_relevel(sample_size, c(\"10\", \"50\", \"100\"))) %&gt;% \n  ggplot(aes(x = estimates, color = sample_size)) + \n  geom_density() + \n  geom_vline(xintercept = 0.05813, color = \"red\", linetype = \"dashed\") + \n  labs(title = \"Sampling distributions of the sample slope\")\n\n\n\n\n\n\n\n\n\nHow do the shapes, centers, and spreads of these sampling distributions compare? Was this what you expected?",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-12-properties-of-sampling-distributions",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-12-properties-of-sampling-distributions",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 12: Properties of sampling distributions",
    "text": "Exercise 12: Properties of sampling distributions\nIn light of your observations, complete the following statements about the sampling distribution of the sample slope.\n\nFor all sample sizes, the shape of the sampling distribution is roughly ___ and the sampling distribution is roughly centered around ___, the sample estimate from our original data.\nAs sample size increases:\nThe average sample slope estimate INCREASES / DECREASES / IS FAIRLY STABLE.\nThe standard error of the sample slopes INCREASES / DECREASES / IS FAIRLY STABLE.\nThus, as sample size increases, our sample slopes become MORE RELIABLE / LESS RELIABLE.",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-1-500-samples-of-size-10-1",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-1-500-samples-of-size-10-1",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 1: 500 samples of size 10",
    "text": "Exercise 1: 500 samples of size 10\n\ndo() repeats the code within the parentheses as many times as you tell it. do()` is a shortcut for a for loop.\n500 different sample estimates of the model\n\n\n\n# Set the seed so that we all get the same results\nset.seed(155)\n\n# Store the sample models\nsample_models_boot &lt;- mosaic::do(500)*(\n  fish %&gt;% \n    sample_n(size = 171, replace = TRUE) %&gt;% \n    with(lm(Concen ~ Length))\n)",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-2-why-resampling-replace-true-1",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-2-why-resampling-replace-true-1",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 2: Why “resampling” (replace = TRUE)?",
    "text": "Exercise 2: Why “resampling” (replace = TRUE)?\n\nThe sample and the mean are the same every time!\nIf we rerun the code below multiple times, we’ll get different samples every time! Note that some of the observations are repeated (this is because of replace = TRUE), but we actually obtain variation in our samples and their mean lengths.\n\n\nsample_2 &lt;- sample_n(small_sample, size = 5, replace = TRUE)\nsample_2\n##   id Length\n## 1  2     43\n## 2  4     52\n## 3  2     43\n## 4  5     40\n## 5  2     43\n\nsample_2 %&gt;% \n  summarize(mean(Length))\n##   mean(Length)\n## 1         44.2",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-3-sampling-distribution-1",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-3-sampling-distribution-1",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 3: Sampling distribution",
    "text": "Exercise 3: Sampling distribution\n\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen)) + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_abline(data = sample_models_boot, \n              aes(intercept = Intercept, slope = Length), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\nsample_models_boot %&gt;% \n  ggplot(aes(x = Length)) + \n  geom_density() + \n  geom_vline(xintercept = 0.05813, color = \"red\") \n\n\n\n\n\n\n\n\n\nThe sampling distribution is symmetric, unimodal, and shaped like a bell curve!\nIt is roughly centered at the slope calculated from our entire sample!\nMost of the estimates lie within the range 0.04 to 0.075.",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-5-standard-error",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-5-standard-error",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 5: Standard error",
    "text": "Exercise 5: Standard error\n\n# boostrapped se\nsample_models_boot %&gt;% \n  summarize(sd(Length))\n##    sd(Length)\n## 1 0.005664586\n\n# CLT se\ncoef(summary(fish_model))\n##                Estimate  Std. Error   t value     Pr(&gt;|t|)\n## (Intercept) -1.13164542 0.213614796 -5.297598 3.617750e-07\n## Length       0.05812749 0.005227593 11.119359 6.641225e-22\n\nThey are basically identical! Both are about 0.005.",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-5-central-limit-theorem-clt-1",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-5-central-limit-theorem-clt-1",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 5: Central Limit Theorem (CLT)",
    "text": "Exercise 5: Central Limit Theorem (CLT)\nRecall that the CLT assumes that, so long as our sample size is “big enough”, the sampling distribution of the sample slope will be Normal.\nSpecifically, all possible sample slopes will vary Normally around the population slope.\n\nDo your simulation results support this assumption? Why or why not?\n\n\nYes! They support this assumption because the shape of sampling distribution is roughly normal (i.e. bell-shaped).",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-6-using-the-clt-1",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-6-using-the-clt-1",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 6: Using the CLT",
    "text": "Exercise 6: Using the CLT\n\n# Hint: Adapt the code from Exercise 5...\nsample_models_10 %&gt;% \n  summarize(sd(Length))\n##   sd(Length)\n## 1 0.02532877\n\n\n95%\n100% - 95% = 5%\n(100 - 99.7)/2 = 0.15% (Note that we divide by two here, because we only want those above 3 SEs, not either above or below!)",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-7-clt-and-the-68-95-99.7-rule-1",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-7-clt-and-the-68-95-99.7-rule-1",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 7: CLT and the 68-95-99.7 Rule",
    "text": "Exercise 7: CLT and the 68-95-99.7 Rule\n\n68% of samples will produce \\(\\hat{\\beta}_1\\) estimates within 1 st. err. of \\(\\beta_1\\)\n95% of samples will produce \\(\\hat{\\beta}_1\\) estimates within 2 st. err. of \\(\\beta_1\\)\n99.7% of samples will produce \\(\\hat{\\beta}_1\\) estimates within 3 st. err. of \\(\\beta_1\\)",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-8-increasing-sample-size-1",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-8-increasing-sample-size-1",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 8: Increasing sample size",
    "text": "Exercise 8: Increasing sample size\nIntuition, no wrong answer.",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-9-500-samples-of-size-n-1",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-9-500-samples-of-size-n-1",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 9: 500 samples of size n",
    "text": "Exercise 9: 500 samples of size n\n\nset.seed(155)\nsample_models_50 &lt;- mosaic::do(500)*(\n  fish %&gt;% \n    sample_n(size = 50, replace = FALSE) %&gt;% \n    with(lm(Concen ~ Length))\n)\n\n# Check it out\nhead(sample_models_50)\n##   Intercept     Length     sigma r.squared        F numdf dendf .row .index\n## 1 -1.310864 0.06393644 0.5646473 0.4846643 45.14316     1    48    1      1\n## 2 -1.503983 0.06887405 0.6112830 0.5333146 54.85302     1    48    1      2\n## 3 -1.730214 0.07140026 0.5940694 0.4561970 40.26725     1    48    1      3\n## 4 -1.213346 0.05948225 0.5561451 0.4712989 42.78853     1    48    1      4\n## 5 -1.221643 0.05925447 0.5471569 0.5190808 51.80887     1    48    1      5\n## 6 -1.384057 0.06532957 0.5610290 0.5400105 56.35021     1    48    1      6\n\nSimilarly, take 500 samples of size 100, and build a sample model from each.\n\nset.seed(155)\nsample_models_100 &lt;- mosaic::do(500)*(\n  fish %&gt;% \n    sample_n(size = 100, replace = FALSE) %&gt;% \n    with(lm(Concen ~ Length))\n)\n\n# Check it out\nhead(sample_models_100)\n##    Intercept     Length     sigma r.squared         F numdf dendf .row .index\n## 1 -0.9745758 0.05400227 0.5965267 0.3361746  49.62917     1    98    1      1\n## 2 -1.1187614 0.05655020 0.5864780 0.4173361  70.19301     1    98    1      2\n## 3 -1.3507522 0.06512980 0.5629381 0.5332227 111.95023     1    98    1      3\n## 4 -1.5201200 0.06712467 0.5220515 0.5113787 102.56432     1    98    1      4\n## 5 -1.0181036 0.05525131 0.5746367 0.4031468  66.19447     1    98    1      5\n## 6 -1.1369532 0.05868566 0.5773007 0.4378596  76.33367     1    98    1      6",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-10-impact-of-sample-size-part-i-1",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-10-impact-of-sample-size-part-i-1",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 10: Impact of sample size (part I)",
    "text": "Exercise 10: Impact of sample size (part I)\n\n# 500 sample models using samples of size 10\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_abline(data = sample_models_10, \n              aes(intercept = Intercept, slope = Length), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\n\n# 500 sample models using samples of size 50\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_abline(data = sample_models_50, \n              aes(intercept = Intercept, slope = Length), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\n\n# 500 sample models using samples of size 100\nfish %&gt;% \n  ggplot(aes(x = Length, y = Concen)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_abline(data = sample_models_100, \n              aes(intercept = Intercept, slope = Length), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n\n\n\n\n\n\n\nThe sample model lines become less and less variable from sample to sample.",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-11-impact-of-sample-size-part-ii-1",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-11-impact-of-sample-size-part-ii-1",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 11: Impact of sample size (part II)",
    "text": "Exercise 11: Impact of sample size (part II)\n\n# Don't think too hard about this code!\n# Combine the estimates & sample size into a new data set\n# Then plot it\n\ndata.frame(estimates = c(sample_models_10$Length, sample_models_50$Length, sample_models_100$Length),\n           sample_size = rep(c(\"10\",\"50\",\"100\"), each = 500)) %&gt;% \n  mutate(sample_size = fct_relevel(sample_size, c(\"10\", \"50\", \"100\"))) %&gt;% \n  ggplot(aes(x = estimates, color = sample_size)) + \n  geom_density() + \n  geom_vline(xintercept = 0.05813, color = \"red\", linetype = \"dashed\") + \n  labs(title = \"Sampling distributions of the sample slope\")\n\n\n\n\n\n\n\n\n\nNo matter the sample size, the sample estimates are normally distributed around the population slope. But as sample size increases, the variability of the sample estimates decreases.",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-12-properties-of-sampling-distributions-1",
    "href": "activities/19-20-sampling-dist-clt-bootstrap.html#exercise-12-properties-of-sampling-distributions-1",
    "title": "Sampling Distribution, Central Limit Theorem, and Bootstrapping",
    "section": "Exercise 12: Properties of sampling distributions",
    "text": "Exercise 12: Properties of sampling distributions\nIn light of your observations, complete the following statements about the sampling distribution of the sample slope.\n\nFor all sample sizes, the shape of the sampling distribution is roughly normal and the sampling distribution is roughly centered around 0.05813, the sample estimate from our original data.\nAs sample size increases:\nThe average sample slope estimate IS FAIRLY STABLE.\nThe standard error of the sample slopes DECREASES.\nThus, as sample size increases, our sample slopes become MORE RELIABLE.",
    "crumbs": [
      "Sampling Distribution, Central Limit Theorem, and Bootstrapping"
    ]
  },
  {
    "objectID": "activities/22-hypothesis-testing-discovery.html",
    "href": "activities/22-hypothesis-testing-discovery.html",
    "title": "Hypothesis Testing- Discovery",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Hypothesis Testing- Discovery"
    ]
  },
  {
    "objectID": "activities/22-hypothesis-testing-discovery.html#learning-goals",
    "href": "activities/22-hypothesis-testing-discovery.html#learning-goals",
    "title": "Hypothesis Testing- Discovery",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nUnderstand how standard errors and confidence intervals enable us to make statistical inferences\nArticulate how we can formalize a research question as a testable, statistical hypothesis",
    "crumbs": [
      "Hypothesis Testing- Discovery"
    ]
  },
  {
    "objectID": "activities/22-hypothesis-testing-discovery.html#readings-and-videos",
    "href": "activities/22-hypothesis-testing-discovery.html#readings-and-videos",
    "title": "Hypothesis Testing- Discovery",
    "section": "Readings and videos",
    "text": "Readings and videos\nThis is a discovery activity, so no assigned readings/videos today.",
    "crumbs": [
      "Hypothesis Testing- Discovery"
    ]
  },
  {
    "objectID": "activities/22-hypothesis-testing-discovery.html#exercise-1",
    "href": "activities/22-hypothesis-testing-discovery.html#exercise-1",
    "title": "Hypothesis Testing- Discovery",
    "section": "Exercise 1",
    "text": "Exercise 1\nResearch question: Is there evidence that the mercury concentration in fish (Concen) differs according to the River they were sampled from?\n\npart a: fit the model\nFit a simple linear regression model that would address our research question\n\nmod_fish &lt;- lm(Concen ~ River, data=fish)\nsummary(mod_fish)\n## \n## Call:\n## lm(formula = Concen ~ River, data = fish)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1664 -0.5681 -0.1764  0.4219  2.4219 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   1.07808    0.08866  12.160   &lt;2e-16 ***\n## RiverWacamaw  0.19835    0.11712   1.694   0.0922 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.7575 on 169 degrees of freedom\n## Multiple R-squared:  0.01669,    Adjusted R-squared:  0.01087 \n## F-statistic: 2.868 on 1 and 169 DF,  p-value: 0.09218\n\nInterpret the intercept from this model.\n\nOur model estimates an average mercury concentration of 1.078ppm among fish in the Lumber River.\n\n\n\npart b: construct a CI\nUsing the 68-95-99.7 rule, construct an approximate 95% confidence interval for the intercept term, and provide an appropriate interpretation.\n\n# approximate 95% confidence interval\n\n\nResponse\n\nCompare your CI to an exact 95% confidence interval for the model coefficients:\n\nconfint(mod_fish, level=0.95)\n##                    2.5 %    97.5 %\n## (Intercept)   0.90305825 1.2531061\n## RiverWacamaw -0.03285077 0.4295435\n\n\n\npart c: what can we conclude from multiple samples?\nSuppose we take 200 different samples of fish from the Lumber River. Based on these results, in how many of those samples would you expect to observe mean mercury concentration greater than 1.25ppm?\n\nResponse\n\n\n\npart d: intuition for constructing & interpreting test statistics\nSuppose previous environmental studies have found little evidence of mercury pollution in other rivers in the area, so perhaps our “default” assumption is that fish from the Lumber river should have an expected mercury concentration of 0ppm. How many standard errors is our sample estimate (1.078ppm) away from this expectation? What are three possible conclusions?\n\nResponse\n\n\n\npart e: do individual observations contradict our conclusions?\nNow suppose we sample a single fish from the Lumber River and find it has a mercury concentration of 2.5ppm. Are you surprised by this result? Why or why not? (Hint: create a code chunk that calculates the mean, standard deviation, and maximum of the Concen variable in each river in our original sample)\n\nfish %&gt;% \n  group_by(River) %&gt;% \n  summarise(mean=mean(Concen), \n            sd=sd(Concen), \n            max=max(Concen))\n## # A tibble: 2 × 4\n##   River    mean    sd   max\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 Lumber   1.08 0.649   3.5\n## 2 Wacamaw  1.28 0.829   3.6\n\n\nResponse\n\n\nClass Notes",
    "crumbs": [
      "Hypothesis Testing- Discovery"
    ]
  },
  {
    "objectID": "activities/22-hypothesis-testing-discovery.html#exercise-2-finish-after-the-class",
    "href": "activities/22-hypothesis-testing-discovery.html#exercise-2-finish-after-the-class",
    "title": "Hypothesis Testing- Discovery",
    "section": "Exercise 2: Finish after the class",
    "text": "Exercise 2: Finish after the class\nLet’s look at the model summary output again:\n\nsummary(mod_fish)\n## \n## Call:\n## lm(formula = Concen ~ River, data = fish)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1664 -0.5681 -0.1764  0.4219  2.4219 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   1.07808    0.08866  12.160   &lt;2e-16 ***\n## RiverWacamaw  0.19835    0.11712   1.694   0.0922 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.7575 on 169 degrees of freedom\n## Multiple R-squared:  0.01669,    Adjusted R-squared:  0.01087 \n## F-statistic: 2.868 on 1 and 169 DF,  p-value: 0.09218\n\n\npart a: interpret model coefficient\nNow, let’s interpret the RiverWacamaw coefficient. Based only on the coefficient (don’t think about the standard error yet), what can we say about the difference in mercury concentration among fish in the two rivers?\n\nResponse\n\n\n\npart b: construct a CI\nUsing the 68-95-99.7 rule, construct an approximate 95% confidence interval for the RiverWacamaw coefficient, and provide an appropriate interpretation.\n\nResponse\n\n\n\npart c: interpreting the CI\nDo you believe it plausible that the mean mercury concentration of the fish population in the Wacamaw River is approximately the same as that of the fish population in the Lumber River? How would you confirm this? What assumptions are you making?\n\nResponse\n\n\n\npart d: effect of sample size on our conclusions\nSuppose we sample 10 times as many fish from the Wacamaw River, and get a similar coefficient estimate (0.2). Thinking back to the Central Limit Theorem, what should happen to the standard error of the RiverWacamaw coefficient? How small of a standard error would we need to more conclusively say that there is an actual difference in mean mercury concentrations of the Lumber River and Wacamaw River fish populations?\n\nResponse\n\n\n\npart e: reconciling parameter estimates and uncertainty\nSuppose the true population coefficient for the RiverWacamawparameter is 0.02 (i.e. the average mercury concentration is 0.02ppm higher for the Wacamaw River fish population compared to that of the Lumber River). Is this meaningful?\n\nResponse\n\n\n\npart f (CHALLENGE)\nUsing the model summary output, report the mean mercury concentration for our sample of fish from the Wacamaw River:\n\nsummary(mod_fish)\n## \n## Call:\n## lm(formula = Concen ~ River, data = fish)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1664 -0.5681 -0.1764  0.4219  2.4219 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   1.07808    0.08866  12.160   &lt;2e-16 ***\n## RiverWacamaw  0.19835    0.11712   1.694   0.0922 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.7575 on 169 degrees of freedom\n## Multiple R-squared:  0.01669,    Adjusted R-squared:  0.01087 \n## F-statistic: 2.868 on 1 and 169 DF,  p-value: 0.09218\n\n\nResponse:\n\nWhich of the following values do you think is the standard error of the sample mean for the Wacamaw River?\n\n0.11712\n0.08866\n0.11712 + 0.08866 = 0.20578\n0.11712 - 0.08866 = 0.02846\nsomething else\n\nTo answer this question, look at the code chunk below, which fits the same model, but uses the Wacamaw River as our reference category instead of the Lumber River:\n\nmod_fish2 &lt;- lm(Concen ~ River, data=fish %&gt;% mutate(River=ifelse(River == \"Wacamaw\", paste0(\"_\", River), River)))\nsummary(mod_fish2)\n## \n## Call:\n## lm(formula = Concen ~ River, data = fish %&gt;% mutate(River = ifelse(River == \n##     \"Wacamaw\", paste0(\"_\", River), River)))\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1664 -0.5681 -0.1764  0.4219  2.4219 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  1.27643    0.07652  16.681   &lt;2e-16 ***\n## RiverLumber -0.19835    0.11712  -1.694   0.0922 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.7575 on 169 degrees of freedom\n## Multiple R-squared:  0.01669,    Adjusted R-squared:  0.01087 \n## F-statistic: 2.868 on 1 and 169 DF,  p-value: 0.09218\n\nCompare this to the output for mod_fish. What do you notice about the standard errors of the intercepts (i.e., the standard errors of the means for each river) compared to the standard errors of the RiverWacamaw and RiverLumber coefficients (i.e., the standard errors of the differences between the means)?\n\nResponse:",
    "crumbs": [
      "Hypothesis Testing- Discovery"
    ]
  },
  {
    "objectID": "activities/22-hypothesis-testing-discovery.html#exercise-1-1",
    "href": "activities/22-hypothesis-testing-discovery.html#exercise-1-1",
    "title": "Hypothesis Testing- Discovery",
    "section": "Exercise 1",
    "text": "Exercise 1\nResearch question: Is there evidence that the mercury concentration in fish (Concen) differs according to the River they were sampled from?\n\npart a: fit the model\nFit a simple linear regression model that would address our research question\n\nmod_fish &lt;- lm(Concen ~ River, data=fish)\nsummary(mod_fish)\n## \n## Call:\n## lm(formula = Concen ~ River, data = fish)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1664 -0.5681 -0.1764  0.4219  2.4219 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   1.07808    0.08866  12.160   &lt;2e-16 ***\n## RiverWacamaw  0.19835    0.11712   1.694   0.0922 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.7575 on 169 degrees of freedom\n## Multiple R-squared:  0.01669,    Adjusted R-squared:  0.01087 \n## F-statistic: 2.868 on 1 and 169 DF,  p-value: 0.09218\n\nInterpret the intercept from this model.\n\nOur model estimates an average mercury concentration of 1.078ppm among fish in the Lumber River.\n\n\n\npart b: construct a CI\nUsing the 68-95-99.7 rule, construct an approximate 95% confidence interval for the intercept term, and provide an appropriate interpretation.\n\n1.078 +/- 2*0.089 –&gt; [0.90, 1.256]\n\n\nPreferred interpretation: It is plausible that the true mean mercury concentration among fish in the Lumber River is between 0.90ppm and 1.25ppm.\n\n\n(technical addendum to this interpretation): …specifically, we expect that if we take many different samples and obtain a set of corresponding parameter estimates and confidence intervals, we expect that 95% of the resulting intervals will contain the true mean mercury concentration of the entire Lumber River fish population. We hope that our interval is one of the lucky 95% and not one of the unlucky 5% that don’t contain the true population parameter.\n\n\nNot as preferred interpretation: We are 95% confident that the mean mercury concentration among fish in the Lumber River is between 0.90ppm and 1.25ppm.\n\nCompare your CI to an exact 95% confidence interval for the model coefficients:\n\nconfint(mod_fish, level=0.95)\n##                    2.5 %    97.5 %\n## (Intercept)   0.90305825 1.2531061\n## RiverWacamaw -0.03285077 0.4295435\n\n\n\npart c: what can we conclude from multiple samples?\nSuppose we take 200 different samples of fish from the Lumber River. In how many of those samples would you expect to observe an estimated mean mercury concentration greater than 1.25ppm?\n\nWe don’t/can’t actually know! This depends on the true population parameter and the accuracy of our sampling distribution.\n\n\nWhat we can say is that if our sampling distribution model is accurate, then we should expect that about 10 out of 200 samples (5% of them) will produce confidence intervals that don’t contain the population parameter. We should expect that half of these–so 5 samples–are overestimates and the other half are underestimates.\n\n\n\npart d: intuition for constructing & interpreting test statistics\nSuppose previous environmental studies have found little evidence of mercury pollution in other rivers in the area, so perhaps our “default” assumption is that fish from the Lumber river should have an expected mercury concentration of 0ppm. How many standard errors is our sample estimate (1.078ppm) away from this expectation? What are three possible conclusions?\n\nIf we assume that 0ppm is the “true” mercury concentration, then our estimate of Beta_0 = 1.078ppm with a standard error of 0.08866 means that our estimate is (1.07808-0)/0.08866 = 12.16 standard errors away from what we should expect.\n\n\nPossible conclusions:\n\n\n\nOur working assumption that 0ppm should be the “true” mercury concentration in the Lumber river fish population was wrong! The confidence interval we constructed above suggests that a true value of 0ppm is extremely implausible.\n\n\n\n\nPerhaps ~0ppm is actually the true average mercury concentration in the population, we just got extremely, outrageously unlucky with our sample.\n\n\n\n\nPerhaps there was a measurement/data entry error, and the units are actually parts per billion, not million.\n\n\n\n\npart e: do individual observations contradict our conclusions?\nNow suppose we sample a single fish from the Lumber River and find it has a mercury concentration of 2.5ppm. Are you surprised by this result? Why or why not? (Hint: create a code chunk that calculates the mean, standard deviation, and maximum of the Concen variable in each river in our original sample)\n\nfish %&gt;% \n  group_by(River) %&gt;% \n  summarise(mean=mean(Concen), \n            sd=sd(Concen), \n            max=max(Concen))\n## # A tibble: 2 × 4\n##   River    mean    sd   max\n##   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 Lumber   1.08 0.649   3.5\n## 2 Wacamaw  1.28 0.829   3.6\n\n\nObserving a single fish with a mercury concentration of 2.5ppm is actually not that surprising! 2.5ppm is a little more than 2 standard deviations away from the mean mercury concentration in our sample of fish from the Lumber River (1.08+2*0.64=2.43), but there are certainly fish in the sample with even higher mercury concentrations (max=3.5ppm), so this isn’t outside the bounds of what we’d expect.",
    "crumbs": [
      "Hypothesis Testing- Discovery"
    ]
  },
  {
    "objectID": "activities/22-hypothesis-testing-discovery.html#exercise-2",
    "href": "activities/22-hypothesis-testing-discovery.html#exercise-2",
    "title": "Hypothesis Testing- Discovery",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet’s look at the model summary output again:\n\nsummary(mod_fish)\n## \n## Call:\n## lm(formula = Concen ~ River, data = fish)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1664 -0.5681 -0.1764  0.4219  2.4219 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   1.07808    0.08866  12.160   &lt;2e-16 ***\n## RiverWacamaw  0.19835    0.11712   1.694   0.0922 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.7575 on 169 degrees of freedom\n## Multiple R-squared:  0.01669,    Adjusted R-squared:  0.01087 \n## F-statistic: 2.868 on 1 and 169 DF,  p-value: 0.09218\n\n\npart a: interpret model coefficient\nNow, let’s interpret the RiverWacamaw coefficient. Based only on the coefficient (don’t think about the standard error yet), what can we say about the difference in mercury concentration among fish in the two rivers?\n\nThe RiverWacamaw coefficient is 0.19835, meaning that the mean mercury concentration among fish in the Wacamaw River is, on average, about 0.20ppm higher than that of fish in the Lumber River.\n\n\n\npart b: construct a CI\nUsing the 68-95-99.7 rule, construct an approximate 95% confidence interval for the RiverWacamaw coefficient, and provide an appropriate interpretation.\n\n0.20 +/- 2*0.11 –&gt; [-0.02, 0.42]\n\n\nPreferred interpretation: It is plausible that the true difference in mean mercury concentration among fish in the Wacamaw River compared to the Lumber River is between -0.02 ppm and 0.42ppm.\n\n\nNot as preferred interpretation: We are 95% confident that the mean mercury concentration among fish in the Wacamaw River somewhere between 0.02ppm less than that of fish in the Lumber River and 0.42ppm more than that of fish in the Lumber River.\n\n\n\npart c: interpreting the CI\nDo you believe it plausible that the mean mercury concentration of the fish population in the Wacamaw River is approximately the same as that of the fish population in the Lumber River? How would you confirm this? What assumptions are you making?\n\nAnswers may vary–this is certainly plausible, since our 95% CI contains 0 (i.e., there is no difference in means between the two rivers). However, we might also argue that there is SOME evidence of a difference, since most of the CI is &gt; 0.\n\n\n\npart d: effect of sample size on our conclusions\nSuppose we sample 10 times as many fish from the Wacamaw River, and get a similar coefficient estimate (0.2). Thinking back to the Central Limit Theorem, what should happen to the standard error of the RiverWacamaw coefficient? How small of a standard error would we need to more conclusively say that there is an actual difference in mean mercury concentrations of the Lumber River and Wacamaw River fish populations?\n\nA larger sample should result in a smaller standard error of the RiverWacamaw coefficient. If the standard error is smaller than 0.1 (say 0.098), then a 95% confidence interval would be [0.004, 0.396]. Since this interval doesn’t include 0, we could conclude that fish in the Wacamaw River, on average, have a higher mercury concentration than fish in the Lumber River. More importantly, the lower standard error of the coefficient allows us to say there is evidence that this difference should be observable across new samples.\n\n\n\npart e: reconciling parameter estimates and uncertainty\nSuppose the true population coefficient for the RiverWacamawparameter is 0.02 (i.e. the average mercury concentration is 0.02ppm higher for the Wacamaw River fish population compared to that of the Lumber River). Is this meaningful?\n\nThis will depend on context–a priori, this difference appears to be negligible, and we could potentially chalk it up to uncontrolled confounders (e.g., perhaps fish in one river tend to be older/bigger and therefore have slightly higher mercury concentrations, even if there is no underlying difference in mercury pollution). We also might consider: what is considered a “harmful” mercury concentration, and are fish in either river near that threshold? Has this changed over time, and by how much?\n\n\n\npart f: (CHALLENGE)\nUsing the model summary output, report the mean mercury concentration for our sample of fish from the Wacamaw River:\n\nsummary(mod_fish)\n## \n## Call:\n## lm(formula = Concen ~ River, data = fish)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1664 -0.5681 -0.1764  0.4219  2.4219 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)   1.07808    0.08866  12.160   &lt;2e-16 ***\n## RiverWacamaw  0.19835    0.11712   1.694   0.0922 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.7575 on 169 degrees of freedom\n## Multiple R-squared:  0.01669,    Adjusted R-squared:  0.01087 \n## F-statistic: 2.868 on 1 and 169 DF,  p-value: 0.09218\n\n\n1.07808 + 0.19835 = 1.27643ppm\n\nWhich of the following values do you think is the standard error of the sample mean for the Wacamaw River?\nTo answer this question, look at the code chunk below, which fits the same model, but uses the Wacamaw River as our reference category instead of the Lumber River:\n\nmod_fish2 &lt;- lm(Concen ~ River, data=fish %&gt;% mutate(River=ifelse(River == \"Wacamaw\", paste0(\"_\", River), River)))\nsummary(mod_fish2)\n## \n## Call:\n## lm(formula = Concen ~ River, data = fish %&gt;% mutate(River = ifelse(River == \n##     \"Wacamaw\", paste0(\"_\", River), River)))\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -1.1664 -0.5681 -0.1764  0.4219  2.4219 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  1.27643    0.07652  16.681   &lt;2e-16 ***\n## RiverLumber -0.19835    0.11712  -1.694   0.0922 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.7575 on 169 degrees of freedom\n## Multiple R-squared:  0.01669,    Adjusted R-squared:  0.01087 \n## F-statistic: 2.868 on 1 and 169 DF,  p-value: 0.09218\n\nCompare this to the output for mod_fish. What do you notice about the standard errors of the intercepts (i.e., the standard errors of the means for each river) compared to the standard errors of the RiverWacamaw and RiverLumber coefficients (i.e., the standard errors of the differences between the means)?\n\nSEs for the means are 0.08866 and 0.07652 for the Lumber and Wacamaw rivers, respectively. The SE for the difference is the same in both models (0.11712), which is greater than the SE of either mean. Because standard errors quantify uncertainty in a given parameter estimate, this tells us that the uncertainty of the estimated difference between two means is greater than the uncertainty in our estimate of either mean by itself.",
    "crumbs": [
      "Hypothesis Testing- Discovery"
    ]
  },
  {
    "objectID": "activities/24-hypothesis-testing-considerations.html",
    "href": "activities/24-hypothesis-testing-considerations.html",
    "title": "Hypothesis Testing- Consideration",
    "section": "",
    "text": "You can download the .qmd file for this activity here and open in R-studio. The rendered version is posted in the course website (Activities tab). I often experiment with the class activities (and see it in live!) and make updates, but I always post the final version before class starts. To be sure you have the most up-to-date copy, please download it once you’ve settled in before class begins.",
    "crumbs": [
      "Hypothesis Testing- Consideration"
    ]
  },
  {
    "objectID": "activities/24-hypothesis-testing-considerations.html#learning-goals",
    "href": "activities/24-hypothesis-testing-considerations.html#learning-goals",
    "title": "Hypothesis Testing- Consideration",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of this lesson, you should be able to:\n\nDifferentiate between more and less accurate interpretations of p-values\nExplain how different factors affect statistical power\nExplain the difference between practical and statistical significance\nExplain how multiple testing impacts the conduct and interpretation of statistical research",
    "crumbs": [
      "Hypothesis Testing- Consideration"
    ]
  },
  {
    "objectID": "activities/24-hypothesis-testing-considerations.html#readings-and-videos",
    "href": "activities/24-hypothesis-testing-considerations.html#readings-and-videos",
    "title": "Hypothesis Testing- Consideration",
    "section": "Readings and videos",
    "text": "Readings and videos\nNo new readings or videos for today.",
    "crumbs": [
      "Hypothesis Testing- Consideration"
    ]
  },
  {
    "objectID": "activities/24-hypothesis-testing-considerations.html#exercise-1-conceptual-understanding",
    "href": "activities/24-hypothesis-testing-considerations.html#exercise-1-conceptual-understanding",
    "title": "Hypothesis Testing- Consideration",
    "section": "Exercise 1: Conceptual understanding",
    "text": "Exercise 1: Conceptual understanding\n\nSuppose that you and a friend are in two different sections (each with the same number of students) of the same class. On your respective midterm exams, you each obtained 85%, and the class average in both of your classes was 80%. Could one of you or your friend still be considered further above the class average than the other? Briefly explain.\n\n\nYes! How well you do relative to the rest of the class depends on both the class average AND the variability of scores in each section. In this case, if your section was more variable, 85% wouldn’t be as far above the class average than in your friend’s section.\n\n\nNow suppose that your section’s test scores were more tightly packed around 80%: maybe the standard deviation of your section’s scores was 2.5, whereas the standard deviation of your friend’s section’s scores was 5. Which of you or your friend was further above the class average? Explain/justify your answer.\n\n\nIn this case, we could note that your score of 85 was 2 standard deviations above the class average (85-80)/2.5 = 2, whereas your friend’s score was only one standard deviation above the class average (85-80)/5 = 1. Here, your score would be considered more extreme, and therefore further above the class average than your friend’s score.\n\n\nBroadly speaking, does a p-value measure the chance of a hypothesis being true, or, the chance of the data having occurred? Why can’t a p-value measure the other quantity that you didn’t choose in the firs question?\n\n\nBroadly speaking, the latter is more correct. By definition, a p-value measures the probability that an observation (as or more extreme that what did observe) would occur over repeated sampling under the null hypothesis (if the null hypothesis were true). This does NOT measure the chance of a hypothesis being true, but rather, tries to make a statement about the null hypothesis through indirect means.\n\n\nExplain in words why, in calculating a p-value, we need to assume that the null hypothesis is true.\n\n\nIn order to make probabilistic statements about repeatedly sampled measures (as p-values do), we need to first be able to define a probability distribution. The null hypothesis tells us where this probability distribution is centered. As a concrete example, we can’t answer questions like “what is the chance we observed this data?” without making some assumption about what the underlying truth is, or sometimes, where the truth is unlikely to be. For example, if we observe a regression coefficient of 2.3, we don’t know if this is likely or not. We can say, however, how likely it is we would observe 2.3 if the true population coefficient were 2 compared to if the true population coefficient were 0.5.\n\n\nSuppose that a hypothesis test yields a p-value of 1e-6 (\\(1\\times 10^{-6}\\)). What can you tell about the magnitude of the effect or the uncertainty of the effect from this p-value? (i.e., What can you tell about the coefficient estimate or the standard error?)\n\n\nA p-value tells us nothing about the coefficient estimate or the standard error because it only tells us about the test statistic. A small p-value only indicates that the test statistic is large. A large test statistic could have come about from a large effect (large coefficient) or from a small coefficient with a very small standard error. This is why reporting a confidence interval is more informative than only reporting a p-value. We get a sense of both the magnitude of the estimate and the amount of uncertainty with a CI, and with just a p-value, we don’t know either.",
    "crumbs": [
      "Hypothesis Testing- Consideration"
    ]
  },
  {
    "objectID": "activities/24-hypothesis-testing-considerations.html#exercise-2-statistical-vs.-practical-significance",
    "href": "activities/24-hypothesis-testing-considerations.html#exercise-2-statistical-vs.-practical-significance",
    "title": "Hypothesis Testing- Consideration",
    "section": "Exercise 2: Statistical vs. practical significance",
    "text": "Exercise 2: Statistical vs. practical significance\nMusic researchers compiled information on 16,216 Spotify songs. They looked at the relationship between a song’s genre (latin vs. not latin) and song duration in seconds. Their modeling code and output is below:\nspotify_model &lt;- lm(duration ~ latin_genre, data = spotify)\ncoef(summary(spotify_model))\n##                   Estimate Std. Error   t value   Pr(&gt;|t|)\n## (Intercept)     212.673908  0.4165491 510.56143 0.00000000\n## latin_genreTRUE   1.555355  0.7435700   2.09174 0.03647731\n\nInterpret the latin_genreTRUE coefficient.\n\n\nResponse: On average, latin genre songs tend to be 1.56 seconds longer than non-latin songs.\n\n\nIn the context of song listening, is this a large or small effect size?\n\n\nResponse: Seems rather small—1.56 seconds in a song is really short.\n\n\nReport and interpret the p-value for the latin_genreTRUE coefficient.\n\n\nResponse: If there were truly no difference in the duration of latin vs non-latin songs (in the broader population of songs), there’s only a 3.6% chance that we would have obtained a sample in which the observed difference was so large relative to the amount of uncertainty in the estimate (the standard error).\n\n\nUse the p-value to make a yes/no decision about the evidence for a relationship between genre and song duration.\n\n\nResponse: We do have statistically significant evidence that latin genre songs tend to be longer than non-latin songs.\n\n\nThis exercise highlights the difference between statistical significance and practical significance—explain how. That is, when might we observe statistically significant results that aren’t practically significant?\n\n\nResponse: Our large sample size of over 16,000 songs is relevant. The more data we have, the smaller our standard errors. (Recall that standard errors are inversely proportionaly to the square root of sample size: std error = \\(c/\\sqrt{n}\\), where \\(c\\) is a constant from complicated formulas.) Larger sample sizes lead to narrower CIs, larger test staistics, and smaller p-values. With large sample sizes, it is easier to find statistically significant results even when the results aren’t practically significant.",
    "crumbs": [
      "Hypothesis Testing- Consideration"
    ]
  },
  {
    "objectID": "activities/24-hypothesis-testing-considerations.html#exercise-3-power",
    "href": "activities/24-hypothesis-testing-considerations.html#exercise-3-power",
    "title": "Hypothesis Testing- Consideration",
    "section": "Exercise 3: Power",
    "text": "Exercise 3: Power\nStatistical power is the probability of rejecting the null hypothesis when the alternative hypothesis is true. We are frequently testing hypotheses to investigate differences or relationships, so in this context, statistical power is the probability of detecting a relationship when there truly is a relationship.\nNavigate to this page to look at an interactive visualization of the factors that influence statistical power.\nUnder “Settings”, next to the “Solve for?” text, click “Power”. You will vary the 3 different parameters (significance level, sample size, and effect size) one at a time to understand how these factors affect power.\nSome context behind this interactive visualization:\n\nVisualization is based on a one sample Z-test:\nThis is a test for whether the true population mean equals a particular value. (e.g., true mean = 30)\nThe effect size slider is measured with a metric called Cohen’s d:\n\nCohen’s d = magnitude of effect/standard deviation of response variable\nHere: how far is the true mean from the null value in units of SD?\ne.g., If the null value is 30, true mean is 40, and the true population SD of the quantity is 5, the Cohen’s d effect size is (40-30)/5 = 2.\n\n\n\nWhat is your intuition about how changing the significance level will change power? Check your intuition with the visualization and explain why this happens.\n\n\nBecause power is the probability of correctly rejecting the null hypothesis (rejecting the null when the alternative hypothesis is true), parameter changes that increase the frequency of rejecting the null hypothesis will increase power. Keep this in mind as you work through. Visually, power corresponds to the light blue area under the distribution on the right. The distribution on the left is our familiar sampling distribution of the test statistic (here called \\(Z_{\\text{crit}}\\)) under the null. The distribution on the right is very closely related but is the sampling distribution of the test statistics under the alternative hypothesis (which is why the \\(H_A\\) label is above it). Power corresponds to the light blue area under the \\(H_A\\) distribution because this area actually corresponds to the test statistics for which we would reject the null. The area represents the probability that we would obtain such test statistics if indeed the alternative were true. That is, the area represents the percentage of samples that would generate test statistics large enough to reject the null (if the alternative hypothesis were true).\n\n\nIf the significance level increases, there is a “less stringent” threshold for rejecting the null (p-value only has to be less than some higher threshold). Increasing the significance level will result in more frequent rejections of the null, and thus higher power (but at the price of a higher type 1 error rate).\n\n\nRepeat Part a for the sample size.\n\n\nIf sample size increases, power should increase because higher sample size will decrease standard error, which will increase the test statistic, which more likely leads to rejecting the null.\n\n\nRepeat Part a for the effect size.\n\n\nIf the magnitude of the effect (numerator of Cohen’s d) increases, power should increase because it is easier (we are more likely) to detect large effects. It is harder (we are less likely) to detect very small effects.\n\n\nIf the variability of the response variable decreases (denominator of Cohen’s d), power should increase because any “signal” from our predictors being picked up by our coefficient estimates will rise far enough above the “noise” in the small variability of the response. The variability of the response variable also contributes to the standard error for the coefficient. With low variability of the response, we will have lower standard errors because there will be lower spread in the estimates across samples. And with lower standard errors, test statistics should increase, resulting in greater frequency of rejecting the null.\n\n\nA large effect magnitude and small variability in the response result in a large effect size, and increasing effect size results in higher power.",
    "crumbs": [
      "Hypothesis Testing- Consideration"
    ]
  },
  {
    "objectID": "activities/24-hypothesis-testing-considerations.html#exercise-4-ethical-considerations",
    "href": "activities/24-hypothesis-testing-considerations.html#exercise-4-ethical-considerations",
    "title": "Hypothesis Testing- Consideration",
    "section": "Exercise 4: Ethical considerations",
    "text": "Exercise 4: Ethical considerations\n\nVisit this page and look at both the comic at the top and the various ways in which researchers have described p-values that do not fall below the \\(\\alpha = 0.05\\) significance level threshold. What ethical consideration is arising here? (Just for fun: a related xkcd comic)\nTake a look at the xkcd comic here. What ethical consideration is arising here?",
    "crumbs": [
      "Hypothesis Testing- Consideration"
    ]
  },
  {
    "objectID": "activities/24-hypothesis-testing-considerations.html#exercise-4-ethical-considerations-1",
    "href": "activities/24-hypothesis-testing-considerations.html#exercise-4-ethical-considerations-1",
    "title": "Hypothesis Testing- Consideration",
    "section": "Exercise 4: Ethical considerations",
    "text": "Exercise 4: Ethical considerations\n\nVisit this page and look at both the comic at the top and the various ways in which researchers have described p-values that do not fall below the \\(\\alpha = 0.05\\) significance level threshold. What ethical consideration is arising here? (Just for fun: a related xkcd comic)\n\n\nResponse: The comic is an illustration of something called the file drawer phenomenon. There is a culture that has arisen when using hypothesis testing in statistical analysis to only appreciate rejections of the null hypothesis as meaningful results. Any results for which investigators were not able to reject the null were filed away, never to be reported. Investigators would keep trying until their p-values were lower than the significance level.\nThere are serious ethical considerations behind this phenomenon. Who ever said that rejecting the null hypothesis was the only way to get meaningful scientific results? There is immense benefit in knowing when there are no effects / no relationships. A very important public health example of this is the relationships between vaccination and autism risk in children. Time and time again, studies have not been able to detect any causal relationship. What would our society look like if those “null” results had been filed away, never to be reported?\n\n\nTake a look at the xkcd comic here. What ethical consideration is arising here?\n\n\nResponse: The idea here is that many, many hypothesis tests are being conducted, which is an idea called multiple testing. As more and more tests are being conducted, there is a higher and higher overall chance that the null hypothesis will be rejected - because we’re just trying so many times. If the null hypothesis is in fact true, testing more and more times is going to increase the probability of making at least one type 1 error.\nIn this comic, we would likely be inclined to believe that the null hypothesis is true (no true association between jelly bean eating and acne). But the sheer number of times that this hypothesis was tested (20 times) means that the scientists ended up finding an association just by chance. That is, the association found for green jelly beans was quite likely a type 1 error. And in fact one null hypothesis rejection among 20 tests is exactly a 5% error rate - in other words, exactly the number of null hypothesis rejections we would expect to make if indeed the null were true (exactly the expected number of type 1 errors).",
    "crumbs": [
      "Hypothesis Testing- Consideration"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Practice Problems",
    "section": "",
    "text": "Please review the Practice Problems Policies document before proceeding!\n…due at 11:59 pm Central on Moodle!\n\nPractice Problems 1, due 9/12\nPractice Problems 2, due 9/20\nPractice Problems 3, due 10/10\nPractice Problems 4, due 10/15\nPractice Problems 5, due 10/25\nPractice Problems 6, due 11/14\nPractice Problems 7, due 11/21\nPractice Problems 8, due 12/06"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project",
    "section": "",
    "text": "Please download and go through the Project Instructions in details.\nThe Rubrics for grading are here.\nParticipate in the STAT 155 Group Project Survey by Monday, October 27."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "",
    "text": "Section 03: M/W/F 09:40-10:40am, THTR 002\nSection 04: M/W/F 12:00-01:00pm, THTR 202\nWelcome to STAT 155! Whether in life or research, we’re often interested in relationships between 2+ variables. For example, how is one’s commute time to class related to their distance from campus and mode of transportation? Or, how is voter participation related to a person’s age and political affiliation? Statistical modeling is the art and science of turning data into information about such relationships of interest.\nBeing able to summarize, interpret, and communicate about data are crucial for navigating today’s information landscape, and these are precisely the skills that we’ll build in this class. Throughout the semester, we’ll study the fundamental methods that statisticians use to extract knowledge from data, emphasizing statistical literacy & intuition, real data applications, and modern computing over memorizing facts and formulas."
  },
  {
    "objectID": "syllabus.html#important-technical-concepts",
    "href": "syllabus.html#important-technical-concepts",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Important technical concepts",
    "text": "Important technical concepts\nUpon completion of this course, students should be able to:\n\nBuild, use, and interpret graphical and numerical summaries of data.\nGiven a research question: identify an appropriate model, use sample data to fit the model in RStudio (free!), evaluate the model’s quality, and quantify our uncertainty in the model’s coefficients and predictions.\nUse a sample model to make predictions & inferences about a population, using prediction/confidence intervals & hypothesis tests.\nInterpret & communicate an analysis in context & using appropriate notation, argumentation, & evidence.\nDescribe potential advantages, limitations, and ethical considerations of a data set and statistical analysis.\nIdentify common pitfalls in statistical analyses (e.g., spurious correlation vs. causal relationships, extrapolation, multicollinearity, multiple testing, practical vs. statistical significance).\nAccurately describe methods and results in a way that is scientifically sound and widely accessible.\nWork productively and effectively in a group setting."
  },
  {
    "objectID": "syllabus.html#important-statistical-skills",
    "href": "syllabus.html#important-statistical-skills",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Important statistical skills",
    "text": "Important statistical skills\nThe following skills are essential both within and beyond Statistics, and demonstrably improve your own learning and the learning of those around you:\n\nMove beyond a “homework only” study approach. Develop a deeper understanding of the material through continued review, reflection, and practice.\nThink creatively, and build confidence, applying course concepts in open-ended, novel settings.\nBe comfortable working through challenges and mistakes.\nContribute to a welcoming and engaged learning environment.\nWork effectively in a group setting."
  },
  {
    "objectID": "syllabus.html#about-your-professor",
    "href": "syllabus.html#about-your-professor",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "About your professor",
    "text": "About your professor\n\nMd Mutasim Billah, PhD\nPronunciation: listen here\nOffice: Olin-Rice 234\nEmail: mbillah@macalester.edu\n\n\n\n\n\n\nNotes from “your professor”\n\n\n\nGreetings! You can call me Mutasim, Bill or Professor Billah & I use he/him pronouns. Back when I was an undergrad student, I didn’t have the best experience in Intro Stat—those courses often emphasized formulas over real understanding. That experience has shaped my teaching—I concentrate on illustrating how statistical theories connect and can be applied in the real world. I’m excited to teach STAT 155 and to create a more meaningful experience—one that helps all students feel confident applying it beyond the classroom. My methodological research lies at the intersection of statistical genetics, biostatistics, and genomics. My current research interests include developing novel statistical methods and computationally efficient bioinformatics tools, leveraging modern machine- and deep-learning approaches analyze high-dimensional next-generation sequencing and multi-omics data to identify genes and regulatory mechanisms underlying complex diseases. Outside of my academic work, I enjoy spending time outdoors with family and friends or cooking variety of foods. If you can’t find me anywhere, I might be busy playing soccer or exploring new worlds on my PS5 Pro!\n\n\nDrop-in (office) hours:\n\nLocation: My office (OLRI 234)\nTimes: M/W/F: 11:10am - 11:40 am, W: 1:20pm - 2:50pm\nBy Appointment: I’m also happy to meet one-on-one if my normal drop-in hours don’t work for you. Shoot me an email and we can arrange it either in-person or over zoom, password: 123456.\nEmail Response Time: I do my best to reply to emails promptly during weekdays. Please note that messages sent after 4:00 pm or on weekends may take longer to receive a response."
  },
  {
    "objectID": "syllabus.html#about-your-preceptors",
    "href": "syllabus.html#about-your-preceptors",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "About your preceptors",
    "text": "About your preceptors\nWe have several wonderful STAT 155 preceptors this semester! Their role is to help students with content questions, assist in the navigation of available resources, advise on studying approaches, and assist with concepts, tools, and skills. Students are accountable for their own learning; as such, preceptors are not allowed to share answers to assignments (unless specifically directed by the instructor), they are not expected to immediately know the right approach to an exercise, and they do not provide assistance outside of office hours.\nIn hiring preceptors, we prioritize and emphasize kindness and respect. I expect the same of students in their interactions with the preceptors. Please utilize and respect their experience and commitment to supporting you in this course. Please check out some additional guidelines and expectations on how to interact with preceptors."
  },
  {
    "objectID": "syllabus.html#textbook-online-course-manual",
    "href": "syllabus.html#textbook-online-course-manual",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Textbook & Online Course Manual",
    "text": "Textbook & Online Course Manual\nThere is no required textbook for this course. Throughout the course, readings may be assigned from these notes, or other sources.\nThe online course manual includes all in-class activities (with solutions) and a daily Schedule. All links and materials needed will be provided on the schedule tab of this website."
  },
  {
    "objectID": "syllabus.html#moodle",
    "href": "syllabus.html#moodle",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Moodle",
    "text": "Moodle\nMoodle includes general resources, a broad course calendar, submission links, feedback, and a forum for student questions."
  },
  {
    "objectID": "syllabus.html#statistical-software",
    "href": "syllabus.html#statistical-software",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Statistical software",
    "text": "Statistical software\nWe will use the (completely free and open source) R programming language extensively throughout this course. RStudio (an interface for R) will facilitate our use of R. You may use RStudio in one of two ways:\n\nDesktop version: Download for Windows or Mac at https://posit.co/downloads/. Note: You first need to download and install R on your computer in order to use the desktop version of RStudio\nOnline: Go to https://rstudio.macalester.edu, and log in with your full Mac email address and your usual Mac password to get access. (Note that this is a shared resource across MSCS, and you may experience performance issues due to high traffic, server outages, etc.)\n\nMore detailed instructions on downloading, installing, and getting started with R, and RStudio are available on the R Resources tab."
  },
  {
    "objectID": "syllabus.html#office-hours-oh-and-r-support",
    "href": "syllabus.html#office-hours-oh-and-r-support",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Office hours (OH) and R Support",
    "text": "Office hours (OH) and R Support\nOH: Across the instructor and preceptors, there are several office hours each week. Names, times, and locations are on the Moodle course calendar. IMPORTANT: Always check the calendar before attending OH.\nData & R Support: In addition to the course preceptors, there is support on campus for working with data and R / RStudio. This is a great resource for R setup and troubleshooting throughout the semester. See https://www.macalester.edu/mscs/data-support for more information."
  },
  {
    "objectID": "syllabus.html#asking-questionscommunicating",
    "href": "syllabus.html#asking-questionscommunicating",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Asking questions/communicating",
    "text": "Asking questions/communicating\n\nOffice Hours\nOH are a great place to chat about the course, career planning, life,… Please visit us!!\n\nOH times & locations are on the Moodle course calendar.\nOH are oriented around group discussion. They are not first come, first served appointments.\nSince it’s not an effective way to deepen your learning, OH are not a place to sit and do assignments with me or preceptors. It’s an opportunity to discuss concepts & specific questions.\n\n\n\nMoodle Forum: STAT 155 Discussion Board\nThis forum is where we’ll communicate outside class. Students can post and answer comments / questions there. This is an informal way to converse, ask questions, share info, & connect. Do not rely on receiving responses outside weekdays between 9am & 5pm."
  },
  {
    "objectID": "syllabus.html#what-to-do-when-you-have-a-question-for-me",
    "href": "syllabus.html#what-to-do-when-you-have-a-question-for-me",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "What to do when you have a question for me?",
    "text": "What to do when you have a question for me?\n\nIf it’s non-private (e.g. about policies, homework (Practice Problems), class activities, etc), you must post it on STAT 155 Discussion Board in Moodle. Remember- collaboration is the KEY!\nIf it’s personal (e.g. about an absence), email me.\nIt’s good, professional practice to check whether your question is already answered in the provided resources. For example:\n\nInfo (what to do if you miss class): syllabus\nDue dates: course calendar at the top of Moodle + course schedule in the online manual\nQuiz dates: syllabus + course calendar at the top of Moodle + course schedule in the online manual\nHomework policies & grading: homework policies & grading doc\nFinals week: syllabus + course calendar at the top of Moodle + course schedule in the online manual"
  },
  {
    "objectID": "syllabus.html#thriving-in-stat-155",
    "href": "syllabus.html#thriving-in-stat-155",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Thriving in STAT 155",
    "text": "Thriving in STAT 155\n\n\n\n\n\n\n🗓️ Plan Ahead\n\n\n\nYou should plan to spend ~10-12 hours on any 4-credit course, including class time.1 Stay up-to-date on the course calendar and carve out time for studying & doing homework.\n\n\n\n\n\n\n\n\n✅ Do the Things\n\n\n\nAt minimum, thriving in this course requires the completion of some concrete tasks. Complete all assignments, regularly attend & engage in class, complete in-class activities (which might mean completing work outside of class), and check the activity solutions.\n\n\n\n\n\n\n\n\n🏗️ Build a Foundation\n\n\n\nIf your main focus is on checking off some boxes, you won’t get much out of this course (or college in general). Deeper, enduring learning requires more. Carve out time to rewrite, reflect upon, & review your notes. Summarize concepts in your own words.\n\n\n\n\n\n\n\n\n🎉 Engage, Ask Questions, Have Fun\n\n\n\nActively participate in the class & take ownership of your learning. PLEASE: Don’t be afraid to ask for help, make mistakes, and ask questions! These skills are critical to your well-being & learning. Finally, have some fun, be curious, and reflect upon what surprises you about the material and yourself"
  },
  {
    "objectID": "syllabus.html#flexibility",
    "href": "syllabus.html#flexibility",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Flexibility",
    "text": "Flexibility\nI provide transparent accommodations to all students. It helps reduce stress and the “hidden curriculum” (not everybody feels comfortable asking for flexibility).\n\nMissed Class: It’s okay to miss class in the case of an emergency. Please see the ‘Absences’ tab below for details.\nPractice Problems (PP): Limited extensions and limited mistakes without penalty. Please carefully go through all the sections from STAT 155 PP Policies & Grading doc.\nCheckpoints: Some class periods will have course videos and readings assigned ahead of time. For each class period where this is the case, a checkpoint quiz (on Moodle) must be completed by 09:00 am. Checkpoints may be attempted as many times as you’d like, but to earn completion credit for a given checkpoint you must score 100% by your final attempt. These short quizzes are designed to ensure that you stay on top of course material, since much of the content in this course builds on itself.\nQuizzes: Limited revisions. Additional flexibility will be provided in rare extenuating circumstances, upon discussion. Exceptions must be discussed with me (not assumed) early on (not after the fact).\n\n\n\n\n\n\n\n🤝 PLEASE REACH OUT WHEN YOU NEED HELP."
  },
  {
    "objectID": "syllabus.html#absences",
    "href": "syllabus.html#absences",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Absences",
    "text": "Absences\nIt’s okay to miss the occasional class. Except in rare extenuating circumstances (which must be discussed in advance): - 3 or fewer absences will not impact your grade - 4-6 absences will impact your grade (see Calculating Final Grades section) - you cannot pass the course if you accrue 7+ absences (more than 25% of class sessions)\n\n\n\n\n\n\nWhat to Do If You Miss Class\n\n\n\n\n📧 Send me a quick email. You do not need to share a reason for your absence, especially if it’s personal. It’s just a simple courtesy & keeps communication lines open.\n📅 Check the Course Schedule in the online manual for what is happening in class that day.\n📝 Complete the in-class activity on your own & check the solutions posted in the online manual.\n💬 Ask any follow-up questions on the Moodle forum or in office hours (OH)."
  },
  {
    "objectID": "syllabus.html#artificial-intelligence-ai",
    "href": "syllabus.html#artificial-intelligence-ai",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Artificial Intelligence (AI)",
    "text": "Artificial Intelligence (AI)\nUsing AI tools is an emerging skill. You may use AI (ChatGPT, Gemini, Grok, etc), with some caveats & limitations:\n\nAI is often wrong, thus is not a good resource on topics for which you don’t yet have expertise. Relatedly, though AI can be helpful with parts of a statistical analysis (eg: getting unstuck on code, checking grammar), you have to guide that process (eg: what questions are we trying to answer? what’s a reasonable approach?).\nWork on an exercise for at least 30 minutes before even thinking about AI. You will learn very little if you overly rely on AI, hence be unprepared for other interactions with the material (eg: in-class discussions, quizzes, future courses that build upon 155, etc). Learning comes from you doing the puzzling, not from you producing a correct answer.\nWhether or not you use AI, you must be able to defend/explain any code/discussion you hand in. You cannot simply use AI to bypass your own learning.\nYou may not use AI to generate entire arguments or discussions. Putting code and discussions into your own words is critical for your own deeper learning, independent thinking, and creativity. (For example, imagine how little you’d learn in a language course if you simply used AI to translate all text for you!!)\nAny use of AI must be cited, just like any other resource."
  },
  {
    "objectID": "syllabus.html#community-academic-integrity",
    "href": "syllabus.html#community-academic-integrity",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Community & Academic Integrity",
    "text": "Community & Academic Integrity\nMSCS strives to provide a learning environment that is equitable, inclusive, welcoming, mutually respectful, and free of discrimination. You’re expected to follow the MSCS Community Guidelines. You’re also required to be familiar with & follow the college’s academic integrity & other academic policies. In addition to the examples listed there, academic violations in this course include but are not limited to the following:\n\nUsing any materials from any past STAT 155 course, at Mac or elsewhere. Relatedly, you should not provide any materials to any future 155 students.\nGaining access to, using, or distributing solution sets.\nPassing off others’ work as your own. You must be able to defend / explain all work you hand in.\nUsing AI without citation, to generate entire discussions / code blocks, or without being able to defend the results. Policy violations will result in a score of 0 on the work & be reported to the Asst. Dean of Academic Programs & Advising."
  },
  {
    "objectID": "syllabus.html#engagement",
    "href": "syllabus.html#engagement",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "(1) Engagement",
    "text": "(1) Engagement\nEngagement is important to your own learning & to fostering a supportive learning community.\n\n\n\n\n\n\n📌 Expectations\n\n\n\n\nDo not miss more than 3 in-person class sessions. (4–6 absences will lower your grade. 7+ absences will result in a D/NC.)\nWhen attending class:\n\nBe on time & don’t leave early\n\nDo not use your phone (phones must be put away when you enter the course, even if class hasn’t started)\n\nDo not use your laptop for anything other than taking notes and in-class activities (e.g., no videos, no email, no messaging apps, etc.)\n\nBe actively present (e.g., don’t work alone, don’t work on other courses, etc.)\n\nOutside class:\n\nCheck your email for announcements (sent via Moodle) and stay updated on the Moodle forum\n\nWhen you have questions, or just want to chat, please stop by OH!"
  },
  {
    "objectID": "syllabus.html#collaboration",
    "href": "syllabus.html#collaboration",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "(2) Collaboration",
    "text": "(2) Collaboration\nCollaboration improves higher-level thinking, confidence, communication, community, & more. You will work in groups in and outside class. These groups may occasionally switch & may sometimes be assigned.\n\n\n\n\n\n\n🤝 Expectations\n\n\n\n\nFollow the MSCS Community Guidelines\nIn group settings, both in and outside class, you:\n\nUse your group members’ correct names and pronouns\n\nActively contribute to discussions\n\nActively include all other group members in discussion\n\nCreate a space where others feel comfortable making mistakes & sharing their ideas\n\nEffectively communicate with your group about meeting times, etc."
  },
  {
    "objectID": "syllabus.html#preparation-checkpoints",
    "href": "syllabus.html#preparation-checkpoints",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "(3) Preparation (Checkpoints)",
    "text": "(3) Preparation (Checkpoints)\nRoughly half of our class sessions will require some prep work. Before class you will watch videos which introduce new concepts, then take a low-stakes checkpoint quiz (CP). This will help us prepare for class, build a common foundation, & maximize our time together – just how readings & reading reflections might be used in another class!\n\n\n\n\n\n\n📊 Expectations\n\n\n\nComplete at least 13 (out of 17) CPs (≈80%) without affecting your final grades!\n\n\n\n\n\n\n\n\n📜 Policies\n\n\n\n\nCPs may be attempted as many times as you’d like, but to earn completion credit for a given checkpoint you must score 100% by your final attempt.\nIf you complete less than 13 (out of 17) CPs (≈80%) before the time they are due, your overall course grade will be lowered by 1/3 of a letter grade (i.e. B –&gt; B-, A- –&gt; B+, etc.).\n\nIf you complete less than 8 (out of 17) CPs (≈50%) before the time they are due, your overall course grade will be lowered by 1 of a letter grade (i.e. A –&gt; B, etc.).\n\nCPs are due 09:00am on the assigned date. There are no extensions for CPs, as they are important preparation for the relevant class session."
  },
  {
    "objectID": "syllabus.html#practice-practice-problem-sets",
    "href": "syllabus.html#practice-practice-problem-sets",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "(4) Practice (Practice Problem Sets)",
    "text": "(4) Practice (Practice Problem Sets)\nIn 8 practice problem sets (PP), you will practice and explore the course material in more depth. The following flexibility is built in to help reduce stress and to facilitate deeper learning. Detailed directions and policies are here.\n\nGrading: PP grades will be based on Exercises & Presentation. You can make some mistakes without it chipping away at your score (e.g. you will earn 5/5 points on an exercise if it’s at least 90% correct & complete).\nDropped score: IF you submit and demonstrate clear effort on all 8 PPs, your lowest PP score will be dropped from your final grade.\nExtensions: Limited extensions will be provided.\nSTAT 155 Discussion Board: Use the Moodle board as your first stop for Practice Problems (PP):\n\nAsk first: Post questions about PP on Moodle. Include the problem number, a brief summary of your approach, and where you’re stuck. For coding, share the entire code chunk for the related problem.\nHelp each other: If you know (or suspect) the answer, reply! Explaining your reasoning helps everyone learn.\nShare alternatives: Multiple correct methods are welcome—post yours with explanation.\nShow your work: To earn full collaboration credit, provide complete, well-explained solutions/steps. Partial or unexplained answers may lose points.\nBe professional: Be respectful, cite any resources you used, and write solutions in your own words.\nGoal: work together as a class so everyone can earn the PP points while learning deeply."
  },
  {
    "objectID": "syllabus.html#project-independence-application",
    "href": "syllabus.html#project-independence-application",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "(5) Project (Independence & Application)",
    "text": "(5) Project (Independence & Application)\nMore details will be provided later in the semester. Here are some basics:\n\nWe’ll start working on projects in ~week 6, with the majority of the work happening later in the semester.\nThe projects are collaborative. You will be working in groups. Though you will work in assigned groups at various points throughout the semester, you will pick your own group for the project. This is something to think about as you meet other students in class and learn about common interests.\nProject grades will be based upon a final group written report (no oral presentation), multiple group and individual checkpoints, and individual contributions to the project (thus it’s possible for different group members to earn different grades)."
  },
  {
    "objectID": "syllabus.html#quizzes-content-expertise",
    "href": "syllabus.html#quizzes-content-expertise",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "(6) Quizzes (Content Expertise)",
    "text": "(6) Quizzes (Content Expertise)\nYour course engagement, collaboration, preparation, practice, and application will support your deeper understanding of the course material. This will be assessed through three in-person quizzes. You must schedule all travel and other commitments around them — there will not be any alternative quiz times.\n\nQuiz 1: Wednesday, 09/24 in class (tentative)\nQuiz 2: Wednesday, 10/29 in class (tentative)\nQuiz 3: Finals week\n\nThe exam will be written to take ~1.25 hours, but you will have the full 2-hour period to complete it.\n\n09:40–10:40 am section 03: Monday 12/15- 08:00am-10:00am\n12:00–01:00 pm section 04: Saturday 12.13- 08:00am-10:00am\n\n\n\n\n\n\n\n\n\n📜 Quiz Policies\n\n\n\n\nAll quizzes will have the following format:\n\nTaken individually, using pen/pencil & paper\n\nYou will not need to write code or use a calculator, but you will need to read & interpret R output\n\nClosed notes, but you may use a 3x5 index card with writing on both sides. These can be handwritten or typed, but you may not include screenshots or share note cards. Making your own card is important to the review process- as you are required to submit the index card along with the answer paper.\n\nQuizzes 2 & 3 will be cumulative. This is unavoidable as the material builds upon itself.\nQuiz corrections:\nYou can earn up to 50% of missed points back on Quizzes 1 & 2 if you:\n\nWrite a reflection of how you prepared for the quiz and where you felt strongest or more uncertain in your understanding before taking the quiz; and\n\nSubmit your quiz corrections along with your reflection to the instructor, no later than one week after quizzes have been handed back.\nNote: Quiz 3 corrections are not allowed due to time constraints at the end of the semester."
  },
  {
    "objectID": "syllabus.html#grading-system",
    "href": "syllabus.html#grading-system",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Grading system",
    "text": "Grading system\nThe grading system in this course is designed to help you achieve the learning objectives while allowing space to make and learn from mistakes along the way. Your final course grade will consist of three, evenly-weighted components (Quizzes, Practice Problems, and the Project), modified by your progress toward the Engagement, Collaboration, and Preparation (Checkpoint) goals:\n\n\n\nCourse Percentage\n\\(\\tfrac{1}{3}\\) Practice Problems +\n\\(\\tfrac{1}{3}\\) Quiz total +\n\\(\\tfrac{1}{3}\\) Project\n\n\n\nGrade\nCourse percentage\n\n\n\n\nA\n&gt; 93%\n\n\nA-\n87–93%\n\n\nB+\n84–87%\n\n\nB\n81–84%\n\n\nB-\n78–81%\n\n\nC+\n75–78%\n\n\nC\n72–75%\n\n\nC-\n69–72%\n\n\nD/NC\n&lt; 69% or 7+ absences\n\n\n\n\n\n  \n\n\nGrade Modifiers\nEngagement (including attendance) +\nPreparation (checkpoints)\n\n\n\n\n\n\n\nModifier\nScenario\n\n\n\n\nnone (e.g. A → A)\nMeets expectations in all two areas (Engagement and Preparation)\n\n\n⅓ lower grade (e.g. A → A-)\nDemonstrates strong progress (e.g., 4 absences OR less than 13 (out of 17) CPs (≈80%))\n\n\n1 lower grade (e.g. A → B)\nDemonstrates moderate progress (e.g., 5–6 absences OR less than 8 (out of 17) CPs (≈50%))\n\n\n&gt;1 lower grade\nDemonstrates little progress toward expectations in two areas\n\n\nDrop to D/NC\nHas 7+ absences\n\n\n\nNOTE: The table presents general scenarios. Please reach out if you want to discuss progress in Engagement and/or Preparation.\n\n\n\n\n\n\n\n\n\n📊 Grading Caveats\n\n\n\n\nThe goal of sharing this specific information is to provide transparency around final grades, hence clear goals to work toward. That said, assigning grades is much more nuanced than any grading rubric / framework might suggest (for good reasons). What’s shared here is a worst case scenario – it represents the lowest a grade might be if you meet the corresponding goals.\nMoodle does NOT correctly weight your grades, thus should not be used alone to monitor your progress."
  },
  {
    "objectID": "syllabus.html#what-to-do-if-you-miss-class",
    "href": "syllabus.html#what-to-do-if-you-miss-class",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "What to Do If You Miss Class",
    "text": "What to Do If You Miss Class\nIf you do miss class, I expect you to complete any in-class activities on your own. Check the solutions in the online manual and come to office hours with any follow-up questions."
  },
  {
    "objectID": "syllabus.html#late-work-on-practice-problems-pps",
    "href": "syllabus.html#late-work-on-practice-problems-pps",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Late work on Practice Problems (PPs)",
    "text": "Late work on Practice Problems (PPs)\nThroughout the semester, you may use up to three, three-day extensions. These three extensions can be used on Practice Problems only, not quizzes. The purpose of deadlines (and extensions) are to keep you accountable for your own learning, to keep you on track with the pace of the course (which builds upon itself throughout the semester), and to provide preceptors and myself with the ability to provide you with timely feedback on assignments. Since the Problem Sets are due roughly every two weeks, you must begin working on them early if you want to succeed.\nExtensions can be used automatically, without letting me know in advance. The Moodle dropboxes for assignments will close exactly 3 days after the original deadline (i.e. Mondays at 11:59pm), and I will not accept work submitted after that point unless there are extenuating circumstances that you have communicated with me about ahead of the original deadline. If you email me a completed assignment after a 3-day extension is up, I may have the preceptors provide you with feedback, but you will not receive credit for the assignment (equivalent to “Needs Improvement” on every question of the relevant assignment).\nI expect you to keep track of how many extensions you’ve used. I will do my best to email you a reminder if you have used all three of your extensions and have none remaining.\nIf you have run out of extensions and/or an extenuating circumstance occurs that impacts your ability to submit assignments on time, please email me to discuss the situation. I am happy to be flexible as long as you communicate!"
  },
  {
    "objectID": "syllabus.html#religious-observance",
    "href": "syllabus.html#religious-observance",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Religious Observance",
    "text": "Religious Observance\nStudents may wish to take part in religious observances that occur during the semester. If you have a religious observance/practice that conflicts with your participation in the course, please contact me before the end of the second week of the semester to discuss appropriate accommodations.\nIn an effort to respect religious diversity, I request that students who plan to observe a religious holiday during scheduled class meetings/class requirements talk to me about reasonable consideration by the end of the second week of the course."
  },
  {
    "objectID": "syllabus.html#well-being",
    "href": "syllabus.html#well-being",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Well-being",
    "text": "Well-being\nI want you to succeed. Both here at Macalester and beyond. To help make this happen, I am committed to the following.\nRespect: Everyone comes from a different path through life, and it is our moral duty as human beings to listen to each other without judgment and to respect one another. I have no tolerance for discrimination of any kind, in and out of the classroom. If you are seeking campus resources regarding discrimination, the Department of Multicultural Life and the Center for Religious and Spiritual Life are wonderful resources. We will also respect the MSCS Community Guidelines.\nSensitive Topics: Applications in this course span issues in science, policy, and society. As such, we may sometimes address sensitive topics. I will try to announce in class if an assignment or activity involves a potentially sensitive topic. If you have reservations about a particular topic, please come talk to me to discuss possible options.\nAccommodations: If you need accommodations for any reason, please contact Disability Services to discuss your needs, and speak with me as soon as possible afterwards so that we can discuss your accommodation plan. If you already have official accommodations, please discuss these with me within the first week of class so that you get off to a great start. Contact me if you have other special circumstances. I will find resources for you.\nTitle IX: You deserve a community free from discrimination, sexual harassment, hostility, sexual assault, domestic violence, dating violence, and stalking. If you or anyone you know has experienced harassment or discrimination, know that you are not alone. Macalester provides staff and resources to help you find support. Please be aware that as a faculty member, it is my responsibility to report disclosure about sexual harassment, sexual misconduct, relationship violence, and stalking to the Title IX Office. The purpose of this report is to ensure that anyone experiencing harm receives the resources and support they need. I will keep this information private, and it will not be shared beyond this required report.\nYou may also contact Macalester’s Title IX Coordinator directly (phone: 651-696-6258; e-mail: titleixcordinator@macalester.edu); they will provide you with supportive measures, resources, and referrals. Additional information about how to file a report (including anonymously) is available on the Title IX website.\nGeneral Health and Well-being: I care that you prioritize your well-being in this semester and beyond. Investing time into taking care of yourself will have profound impacts on all aspects of your life. Remember that beyond being a student, you are a human being carrying your own experiences, thoughts, emotions, and identities. It is important to acknowledge any stressors you may be facing, which can be mental, emotional, physical, cultural, financial, etc., and how they can have an impact on you. I encourage you to remember that you have a body with needs. In the classroom, eat when you are hungry, drink water, use the restroom, and step out if you are upset and need some air. Please do what is necessary so long as it does not impede your or others’ ability to be mentally and emotionally present in the course. Outside of the classroom, sleeping well, moving your body, and connecting with others can be strategies can help nourish you. If you are having difficulties maintaining your well-being, please don’t hesitate to contact me and/or find support from physical and mental health resources here, here, and here."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "STAT 155: Introduction to Statistical Modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMacalester Academic Advising – High School Preparation↩︎"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html",
    "href": "template_qmds/02-foundations-univariate-notes.html",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe what a case (or unit of analysis) represents in a dataset.\nDescribe what a variable represents in a dataset.\nIdentify whether a variable is categorical or quantitative and what summarizations and visualizations are appropriate for that variable\nWrite R code to read in data and to summarize and visualize a single variable at a time.\nInterpret key features of barplots, boxplots, histograms, and density plots\nDescribe information about the distribution of a quantitative variable using the concepts of shape, center, spread, and outliers\nRelate summary statistics of data to the concepts of shape, center, spread, and outliers\n\n\n\n\nChoose either the reading or the videos to go through before class.\n\nReading: Sections 2.1-2.4, 2.6 in the STAT 155 Notes\nVideos:\n\nUnivariate summaries (slides)\n\nPart 1\nPart 2\n\nR Code for Categorical Visualization and Summarization\nR Code for Quantitative Visualization and Summarization\nQuarto docs\n\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#learning-goals",
    "href": "template_qmds/02-foundations-univariate-notes.html#learning-goals",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe what a case (or unit of analysis) represents in a dataset.\nDescribe what a variable represents in a dataset.\nIdentify whether a variable is categorical or quantitative and what summarizations and visualizations are appropriate for that variable\nWrite R code to read in data and to summarize and visualize a single variable at a time.\nInterpret key features of barplots, boxplots, histograms, and density plots\nDescribe information about the distribution of a quantitative variable using the concepts of shape, center, spread, and outliers\nRelate summary statistics of data to the concepts of shape, center, spread, and outliers"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#readings-and-videos",
    "href": "template_qmds/02-foundations-univariate-notes.html#readings-and-videos",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "",
    "text": "Choose either the reading or the videos to go through before class.\n\nReading: Sections 2.1-2.4, 2.6 in the STAT 155 Notes\nVideos:\n\nUnivariate summaries (slides)\n\nPart 1\nPart 2\n\nR Code for Categorical Visualization and Summarization\nR Code for Quantitative Visualization and Summarization\nQuarto docs\n\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-1-get-curious",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-1-get-curious",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 1: Get curious",
    "text": "Exercise 1: Get curious\n\nHypothesize with each other: what themes do you think might come up often in Dear Abby letters?\nAfter brainstorming, take a quick glance at the original article from The Pudding to see what themes they explored.\nGo to the very end of the Pudding article to the section titled “Data and Method”. In thinking about the who, what, when, where, why, and how of data context, what concerns/limitations surface with regards to using this data to learn about Americans’ concerns over the decades?"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-2-importing-and-getting-to-know-the-data",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-2-importing-and-getting-to-know-the-data",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 2: Importing and getting to know the data",
    "text": "Exercise 2: Importing and getting to know the data\nFirst, in the Console pane of RStudio, run the following command to install some necessary packages (you will need to do this any time you are installing a new package):\ninstall.packages(\"tidyverse\")\nNow, in the Quarto pane, run the following code chunk to load the package and load a dataset (you can either click the green arrow in the top right of the code chunk, put your cursor in the code chunk and hit Ctrl+Alt+C [on Windows/Linux] or Command+Option+C [on Mac]).\n\n# Load package\nlibrary(tidyverse)\n\n# Read in the Dear Abby data\nabby &lt;- read_csv(\"https://mac-stat.github.io/data/dear_abby.csv\")\n\nIf it runs successfully, you should see the following output appear in the Console pane:\n&gt; # Load package\n&gt; library(tidyverse)\n&gt; \n&gt; # Read in the course evaluation data\n&gt; abby &lt;- read_csv(\"https://mac-stat.github.io/data/dear_abby.csv\")\nRows: 20034 Columns: 11\n── Column specification ────────────────────────────\nDelimiter: \",\"\nchr (4): day, url, title, question_only\ndbl (7): year, month, letterId, afinn_overall, a...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nThroughout this activity, we’ll work only with the most recent year of data, from 2017. Run the following chunk:\n\n# Wrangle the Dear Abby data\n# Ignore this code for now!\nabby &lt;- abby %&gt;% \n  filter(year == 2017) %&gt;% \n  mutate(month = month(month, label = TRUE)) %&gt;%\n  mutate(\n    parents = str_detect(question_only, \"mother|mama|mom|father|papa|dad\"),\n    marriage = str_detect(question_only, \"marriage|marry|married\"),\n    money = str_detect(question_only, \"money|finance\")\n  ) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    themes = c(\n      if (parents) \"parents\",\n      if (marriage) \"marriage\",\n      if (money) \"money\"\n    ) %&gt;% paste(collapse = \", \"),\n    themes = ifelse(themes == \"\", \"other\", themes)\n  ) %&gt;%\n  ungroup() %&gt;% \n  select(year, month, day, question_only, bing_pos, afinn_overall, afinn_pos, afinn_neg, themes)\n\n\nClick on the Environment tab (generally in the upper right hand pane in RStudio). Then click the abby line. The abby data will pop up as a separate pane (like viewing a spreadsheet) – check it out.\nIn this tidy dataset, what is the unit of observation? That is, what is represented in each row of the dataset?\nWhat term do we use for the columns of the dataset?\nTry out each function below. Identify what each function tells you about the abby data and note this in the ???:\n\n\n# ??? [what do both numbers mean?]\ndim(abby)\n## [1] 514   9\n\n\n# ???\nnrow(abby)\n## [1] 514\n\n\n# ???\nncol(abby)\n## [1] 9\n\n\n# ???\nhead(abby)\n## # A tibble: 6 × 9\n##    year month day   question_only     bing_pos afinn_overall afinn_pos afinn_neg\n##   &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1  2017 Aug   30    \"i moved to the …    0.75             14        16         2\n## 2  2017 Aug   30    \"under what circ…   NA                NA        NA        NA\n## 3  2017 Aug   28    \"i'm not a dog p…    0.333             5         5         0\n## 4  2017 Aug   28    \"my 62-year-old …    0.143           -11         8        19\n## 5  2017 Aug   27    \"i have a friend…    0.222             0         7         7\n## 6  2017 Aug   27    \"i have been sel…    0.333            -5         2         7\n## # ℹ 1 more variable: themes &lt;chr&gt;\n\n\n# ???\nnames(abby)\n## [1] \"year\"          \"month\"         \"day\"           \"question_only\"\n## [5] \"bing_pos\"      \"afinn_overall\" \"afinn_pos\"     \"afinn_neg\"    \n## [9] \"themes\"\n\n\n[OPTIONAL] If you’re not sure how exactly to use a function, you can pull up a built-in help page with information about the arguments a function takes (i.e., what goes inside the parentheses), and the output it produces. To do this, click inside the Console pane, and enter ?function_name. For example, to pull up a help page for the dim() function, we can type ?dim and hit Enter. Try pulling up the help page for the read_csv() function we used to load the dataset."
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-3-preparing-to-summarize-and-visualize-the-data",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-3-preparing-to-summarize-and-visualize-the-data",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 3: Preparing to summarize and visualize the data",
    "text": "Exercise 3: Preparing to summarize and visualize the data\nIn the next exercises, we will be exploring themes in the Dear Abby questions and the overall “mood” or sentiment of the questions. Before continuing, read the codebook for this dataset for some context about sentiment analysis, which gives us a measure of the mood/sentiment of a text.\n\nWhat sentiment variables do we have in the dataset? Are they quantitative or categorical?\nCheck out the theme variable. Is this quantitative or categorical?\nWhat visualizations are appropriate for looking at the distribution of a single quantitative variable? What about a single categorical variable?"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-4-exploring-themes-in-the-letters",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-4-exploring-themes-in-the-letters",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 4: Exploring themes in the letters",
    "text": "Exercise 4: Exploring themes in the letters\n\nThe code below makes a barplot of the themes variable using the ggplot2 visualization package. Before making the plot, make note of what you expect the plot might look like. (This might be hard–just do your best!) Then compare to what you observe when you run the code chunk to make the plot. (Clearly defining your expectations first is good scientific practice to avoid confirmation bias.)\n\n\n# Load package\nlibrary(ggplot2)\n\n# barplot\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\nWe can follow up on the barplot with a simple numerical summary. Whereas the ggplot2 package is great for visualizations, dplyr is great for numerical summaries. The code below constructs a table of the number of questions with each theme. Make sure that these numerical summaries match up with what you saw in the barplot.\n\n\n# Construct a table of counts\nabby %&gt;% \n    count(themes)\n## # A tibble: 8 × 2\n##   themes                       n\n##   &lt;chr&gt;                    &lt;int&gt;\n## 1 marriage                    75\n## 2 marriage, money              5\n## 3 money                       21\n## 4 other                      234\n## 5 parents                    127\n## 6 parents, marriage           33\n## 7 parents, marriage, money     4\n## 8 parents, money              15\n\n\nBefore proceeding, let’s break down the plotting code above. Run each chunk to see how the two lines of code above build up the plot in “layers”. Add comments (on the lines starting with #) to document what you notice.\n\n\n# ???\nggplot(abby, aes(x = themes))\n\n\n\n\n\n\n\n\n\n# ???\nggplot(abby, aes(x = themes)) +\n    geom_bar()\n\n\n\n\n\n\n\n\n\n# ???\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n\n\n\n\n\n\n\n\n\n# ???\nggplot(abby, aes(x = themes)) +\n    geom_bar() +\n    theme_classic() +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-5-exploring-sentiment",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-5-exploring-sentiment",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 5: Exploring sentiment",
    "text": "Exercise 5: Exploring sentiment\nWe’ll look at the distribution of the bing_pos sentiment variable and associated summary statistics.\n\nThe code below creates a boxplot of this variable. In the comment, make note of how this code is similar to the code for the barplot above. As in the previous exercise, before running the code chunk to create the plot, make note of what you expect the boxplot to look like.\n\n\n# ???\nggplot(abby, aes(x = bing_pos)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\n\nChallenge: Using the code for the barplot and boxplot as a guide, try to make a histogram and a density plot of the overall average ratings.\n\nWhat information is given by the tallest bar of the histogram?\nHow would you describe the shape of the distribution?\n\n\n\n# Histogram\n\n# Density plot\n\n\nWe can compute summary statistics (numerical summaries) for a quantitative variable using the summary() function or with the summarize() function from the dplyr package. (1st Qu. and 3rd Qu. stand for first and third quartile.) After inspecting these summaries, look back to your boxplot, histogram, and density plot. Which plots show which summaries most clearly?\n\n\n# Summary statistics\n# Using summary() - convenient for computing many summaries in one command\n# Does not show the standard deviation\nabby %&gt;% \n    select(bing_pos) %&gt;% \n    summary()\n##     bing_pos     \n##  Min.   :0.0000  \n##  1st Qu.:0.1667  \n##  Median :0.3333  \n##  Mean   :0.3650  \n##  3rd Qu.:0.5000  \n##  Max.   :1.0000  \n##  NA's   :19\n\n# Using summarize() from dplyr\n# Note that we use %&gt;% to pipe the data into the summarize() function\n# We need to use na.rm = TRUE because there are missing values (NAs)\nabby %&gt;% \n    summarize(mean(bing_pos, na.rm = TRUE), median(bing_pos, na.rm = TRUE), sd(bing_pos, na.rm = TRUE))\n## # A tibble: 1 × 3\n##   `mean(bing_pos, na.rm = TRUE)` median(bing_pos, na.rm…¹ sd(bing_pos, na.rm =…²\n##                            &lt;dbl&gt;                    &lt;dbl&gt;                  &lt;dbl&gt;\n## 1                          0.365                    0.333                  0.279\n## # ℹ abbreviated names: ¹​`median(bing_pos, na.rm = TRUE)`,\n## #   ²​`sd(bing_pos, na.rm = TRUE)`\n\n\nWrite a good paragraph describing the information in the histogram (or density plot) by discussing shape, center, spread, and outliers. Incorporate the numerical summaries from part c."
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#pause-math-box",
    "href": "template_qmds/02-foundations-univariate-notes.html#pause-math-box",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Pause: Math box",
    "text": "Pause: Math box\nBelow is an example of a “math box” which summarizes the formulas for some of the numerical summaries above. You are not required to memorize, nor will you be assessed on, any formulas presented in this or any future math box. They serve 3 purposes:\n\nTo emphasize that there’s “math” / a formal structure behind what we’re doing.\nTo provide students that plan to continue studying Statistics a glimpse into the formal statistical theory they’ll explore in later courses.\nTo make happy the students that are simply interested in math!\n\n\n\n\n\n\n\n\n\n\nMATH BOX: Univariate numerical summaries\n\n\n\nLet \\((y_1, y_2, ..., y_n)\\) be a sample of \\(n\\) data points.\nmean: \\[\\overline{y} = \\frac{y_1 + y_2 + \\cdots + y_n}{n} = \\frac{\\sum_{i=1}^n y_i}{n}\\]\nvariance: \\[\\text{var}(y) = \\frac{(y_1 - \\overline{y})^2 + (y_2 - \\overline{y})^2 + \\cdots + (y_n - \\overline{y})^2}{n - 1} = \\frac{\\sum_{i=1}^n (y_i - \\overline{y})^2}{n - 1}\\]\nstandard deviation: \\[\\text{sd}(y) = \\sqrt{\\text{var}(y)}\\]"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-6-box-plots-vs.-histograms-vs.-density-plots",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-6-box-plots-vs.-histograms-vs.-density-plots",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 6: Box plots vs. histograms vs. density plots",
    "text": "Exercise 6: Box plots vs. histograms vs. density plots\nWe took 3 different approaches to plotting the quantitative average course variable above. They all have pros and cons.\n\nWhat is one pro about the boxplot in comparison to the histogram and density plot?\nWhat is one con about the boxplot in comparison to the histogram and density plots?\nIn this example, which plot do you prefer and why?"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-7-returning-to-our-context-looking-ahead",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-7-returning-to-our-context-looking-ahead",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 7: Returning to our context, looking ahead",
    "text": "Exercise 7: Returning to our context, looking ahead\nIn this activity, we explored data on Dear Abby question, with a focus on exploring a single variable at a time.\n\nIn big picture terms, what have we learned about Dear Abby questions?\nWhat further curiosities do you have about the data?"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-8-different-ways-to-think-about-data-visualization",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-8-different-ways-to-think-about-data-visualization",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 8: Different ways to think about data visualization",
    "text": "Exercise 8: Different ways to think about data visualization\nIn working with and visualizing data, it’s important to keep in mind what a data point represents. It can reflect the experience of a real person. It might reflect the sentiment in a piece of art. It might reflect history. We’ve taken one very narrow and technical approach to data visualization. Check out the following examples, and write some notes about anything you find interesting.\n\nDear Data\nW.E.B. DuBois\nDecolonizing Data Viz"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-9-rendering-your-work",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-9-rendering-your-work",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 9: Rendering your work",
    "text": "Exercise 9: Rendering your work\nSave this file, and then click the “Render” button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\n\nScroll through and inspect the document to see how your work was translated into this HTML format. Neat!\nClose the browser tab.\nGo to the “Background Jobs” pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your “Activities” subfolder within your “STAT155” folder and locate the HTML file. You can open it again in your browser to double check."
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#reflection",
    "href": "template_qmds/02-foundations-univariate-notes.html#reflection",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Reflection",
    "text": "Reflection\nGo to the top of this file and review the learning objectives for this lesson. Which objectives do you have a good handle on, are at least familiar with, or are struggling with? What feels challenging right now? What are some wins from the day?\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#advice-make-an-r-code-cheat-sheet",
    "href": "template_qmds/02-foundations-univariate-notes.html#advice-make-an-r-code-cheat-sheet",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Advice: make an R code “cheat sheet”!",
    "text": "Advice: make an R code “cheat sheet”!\nYou will continue to pick up new R code and ideas. You’re highly encouraged to start tracking this in a cheat sheet (eg: in a Google doc). The cheat sheet will be a handy reference for you, and the act of making it will help deepen your understanding and retention."
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-10-read-in-and-get-to-know-the-weather-data",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-10-read-in-and-get-to-know-the-weather-data",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 10: Read in and get to know the weather data",
    "text": "Exercise 10: Read in and get to know the weather data\nDaily weather data are available for 3 locations in Perth, Australia.\n\nView the codebook here.\nComplete the code below to read in the data.\n\n\n# Replace the ??? with your own name for the weather data\n# Replace the ___ with the correct function\n??? &lt;- ___(\"https://mac-stat.github.io/data/weather_3_locations.csv\")\n## Error in parse(text = input): &lt;text&gt;:3:5: unexpected assignment\n## 2: # Replace the ___ with the correct function\n## 3: ??? &lt;-\n##        ^"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-11-exploring-the-data-structure",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-11-exploring-the-data-structure",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 11: Exploring the data structure",
    "text": "Exercise 11: Exploring the data structure\nCheck out the basic features of the weather data.\n\n# Examine the first six cases\n\n# Find the dimensions of the data\n\nWhat does a case represent in this data?"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-12-exploring-rainfall",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-12-exploring-rainfall",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 12: Exploring rainfall",
    "text": "Exercise 12: Exploring rainfall\nThe raintoday variable contains information about rainfall.\n\nIs this variable quantitative or categorical?\nCreate an appropriate visualization, and compute appropriate numerical summaries.\nWhat do you learn about rainfall in Perth?\n\n\n# Visualization\n\n# Numerical summaries"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-13-exploring-temperature",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-13-exploring-temperature",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 13: Exploring temperature",
    "text": "Exercise 13: Exploring temperature\nThe maxtemp variable contains information on the daily high temperature.\n\nIs this variable quantitative or categorical?\nCreate an appropriate visualization, and compute appropriate numerical summaries.\nWhat do you learn about high temperatures in Perth?\n\n\n# Visualization\n\n# Numerical summaries"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-14-customizing-challenge",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-14-customizing-challenge",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 14: Customizing! (CHALLENGE)",
    "text": "Exercise 14: Customizing! (CHALLENGE)\nThough you will naturally absorb some RStudio code throughout the semester, being an effective statistical thinker and “programmer” does not require that we memorize all code. That would be impossible! In contrast, using the foundation you built today, do some digging online to learn how to customize your visualizations.\n\nFor the histogram below, add a title and more meaningful axis labels. Specifically, title the plot “Distribution of max temperatures in Perth”, change the x-axis label to “Maximum temperature” and y-axis label to “Number of days”. HINT: Do a Google search for something like “add axis labels ggplot”.\n\n\n# Add a title and axis labels\nggplot(weather, aes(x = maxtemp)) + \n    geom_histogram()\n## Error: object 'weather' not found\n\n\nAdjust the code below in order to color the bars green. NOTE: Color can be an effective tool, but here it is simply gratuitous.\n\n\n# Make the bars green\nggplot(weather, aes(x = raintoday)) + \n    geom_bar()\n## Error: object 'weather' not found\n\n\nCheck out the ggplot2 cheat sheet. Try making some of the other kinds of univariate plots outlined there.\nWhat else would you like to change about your plot? Try it!"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#exercise-15-optional-challenge",
    "href": "template_qmds/02-foundations-univariate-notes.html#exercise-15-optional-challenge",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Exercise 15: Optional challenge",
    "text": "Exercise 15: Optional challenge\nAt the top of this activity, we searched for words related to some topics of interest (parents, marriage, money) and combined them into a single theme variable. It looked something like this:\n\nabby_new &lt;- abby %&gt;% \n  mutate(\n    parents = str_detect(question_only, \"mother|mama|mom|father|papa|dad\"),\n    marriage = str_detect(question_only, \"marriage|marry|married\"),\n    money = str_detect(question_only, \"money|finance\")\n  ) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    themes = c(\n      if (parents) \"parents\",\n      if (marriage) \"marriage\",\n      if (money) \"money\"\n    ) %&gt;% paste(collapse = \", \"),\n    themes = ifelse(themes == \"\", \"other\", themes)\n  ) %&gt;%\n  ungroup()\n\nCheck it out:\n\nhead(abby_new)\n## # A tibble: 6 × 12\n##    year month day   question_only     bing_pos afinn_overall afinn_pos afinn_neg\n##   &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1  2017 Aug   30    \"i moved to the …    0.75             14        16         2\n## 2  2017 Aug   30    \"under what circ…   NA                NA        NA        NA\n## 3  2017 Aug   28    \"i'm not a dog p…    0.333             5         5         0\n## 4  2017 Aug   28    \"my 62-year-old …    0.143           -11         8        19\n## 5  2017 Aug   27    \"i have a friend…    0.222             0         7         7\n## 6  2017 Aug   27    \"i have been sel…    0.333            -5         2         7\n## # ℹ 4 more variables: themes &lt;chr&gt;, parents &lt;lgl&gt;, marriage &lt;lgl&gt;, money &lt;lgl&gt;\n\n\nUnderstand the code!\n\nInside mutate() the line parents = str_detect(question_only, \"mother|mama|mom|father|papa|dad\") created a new variable called parents. This variable takes on TRUE or FALSE. Explain what TRUE and FALSE mean here.\nThe themes variable combines the information from the parents, marriage, and money variables. Check out the themes for the first 3 rows / data points. Convince yourself that you understand how it corresponds to the parents, marriage, and money variables.\n\nBeyond parents, marriage, and money, what are some other topics that might pop up in the Dear Abby letters (and that you’re interested in exploring)? Modify the code below to explore those topics! Update the themes variable accordingly.\n\n\nabby_new &lt;- abby %&gt;% \n  mutate(\n    parents = str_detect(question_only, \"mother|mama|mom|father|papa|dad\"),\n    marriage = str_detect(question_only, \"marriage|marry|married\"),\n    money = str_detect(question_only, \"money|finance\")\n  ) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    themes = c(\n      if (parents) \"parents\",\n      if (marriage) \"marriage\",\n      if (money) \"money\"\n    ) %&gt;% paste(collapse = \", \"),\n    themes = ifelse(themes == \"\", \"other\", themes)\n  ) %&gt;%\n  ungroup()\n\n# Check out the raw data\nhead(abby_new)\n## # A tibble: 6 × 12\n##    year month day   question_only     bing_pos afinn_overall afinn_pos afinn_neg\n##   &lt;dbl&gt; &lt;ord&gt; &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n## 1  2017 Aug   30    \"i moved to the …    0.75             14        16         2\n## 2  2017 Aug   30    \"under what circ…   NA                NA        NA        NA\n## 3  2017 Aug   28    \"i'm not a dog p…    0.333             5         5         0\n## 4  2017 Aug   28    \"my 62-year-old …    0.143           -11         8        19\n## 5  2017 Aug   27    \"i have a friend…    0.222             0         7         7\n## 6  2017 Aug   27    \"i have been sel…    0.333            -5         2         7\n## # ℹ 4 more variables: themes &lt;chr&gt;, parents &lt;lgl&gt;, marriage &lt;lgl&gt;, money &lt;lgl&gt;\n\n# Check out the number of letters belonging to each theme\nabby_new %&gt;% \n  count(themes)\n## # A tibble: 8 × 2\n##   themes                       n\n##   &lt;chr&gt;                    &lt;int&gt;\n## 1 marriage                    75\n## 2 marriage, money              5\n## 3 money                       21\n## 4 other                      234\n## 5 parents                    127\n## 6 parents, marriage           33\n## 7 parents, marriage, money     4\n## 8 parents, money              15"
  },
  {
    "objectID": "template_qmds/02-foundations-univariate-notes.html#done",
    "href": "template_qmds/02-foundations-univariate-notes.html#done",
    "title": "Univariate visualization and summaries (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html",
    "href": "template_qmds/04-slr-formalization-notes.html",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDifferentiate between a response / outcome variable and a predictor / explanatory variable\nWrite a model formula for a simple linear regression model with a quantitative predictor\nWrite R code to fit a linear regression model\nInterpret the intercept and slope coefficients in a simple linear regression model with a quantitative predictor\nCompute expected / predicted / fitted values and residuals from a linear regression model formula\nInterpret predicted values and residuals in the context of the data\nExplain the connection between residuals and the least squares criterion\n\n\n\n\nChoose either the reading or the videos to go through before class.\n\nReading: Sections 2.8, 3.1-3.3, 3.6 in the STAT 155 Notes\nVideos:\n\nSimple linear regression Part 1: motivation & scatterplots\nSimple linear regression Part 2: correlation\nSimple linear regression Part 3: simple linear regression models\nR Code for Fitting a Linear Model (Time: 11:07)\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#learning-goals",
    "href": "template_qmds/04-slr-formalization-notes.html#learning-goals",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDifferentiate between a response / outcome variable and a predictor / explanatory variable\nWrite a model formula for a simple linear regression model with a quantitative predictor\nWrite R code to fit a linear regression model\nInterpret the intercept and slope coefficients in a simple linear regression model with a quantitative predictor\nCompute expected / predicted / fitted values and residuals from a linear regression model formula\nInterpret predicted values and residuals in the context of the data\nExplain the connection between residuals and the least squares criterion"
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#readings-and-videos",
    "href": "template_qmds/04-slr-formalization-notes.html#readings-and-videos",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "",
    "text": "Choose either the reading or the videos to go through before class.\n\nReading: Sections 2.8, 3.1-3.3, 3.6 in the STAT 155 Notes\nVideos:\n\nSimple linear regression Part 1: motivation & scatterplots\nSimple linear regression Part 2: correlation\nSimple linear regression Part 3: simple linear regression models\nR Code for Fitting a Linear Model (Time: 11:07)\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#exercise-1-get-to-know-the-data",
    "href": "template_qmds/04-slr-formalization-notes.html#exercise-1-get-to-know-the-data",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Exercise 1: Get to know the data",
    "text": "Exercise 1: Get to know the data\nCreate a new code chunk to look at the first few rows of the data and learn how much data (in terms of cases and variables) we have.\n\nWhat does a case represent?\nHow many and what kinds of variables do we have?\nThinking about the who, what, when, where, why, and how of this data, which of the 5W’s + H seem most relevant to our investigations? Explain your thoughts."
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#exercise-2-get-to-know-the-outcomeresponse-variable",
    "href": "template_qmds/04-slr-formalization-notes.html#exercise-2-get-to-know-the-outcomeresponse-variable",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Exercise 2: Get to know the outcome/response variable",
    "text": "Exercise 2: Get to know the outcome/response variable\nLet’s get acquainted with the riders_registered variable.\n\nConstruct an appropriate plot to visualize the distribution of this variable, and compute appropriate numerical summaries.\nWrite a good paragraph interpreting the plot and numerical summaries."
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#exercise-3-explore-the-relationship-between-ridership-and-temperature",
    "href": "template_qmds/04-slr-formalization-notes.html#exercise-3-explore-the-relationship-between-ridership-and-temperature",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Exercise 3: Explore the relationship between ridership and temperature",
    "text": "Exercise 3: Explore the relationship between ridership and temperature\nWe’d like to understand how daily ridership among registered users relates with the temperature that it feels like that day (temp_feel).\n\nWhat type of plot would be appropriate to visualize this relationship? Sketch and describe what you expect this plot to look like.\nCreate an appropriate plot using ggplot(). How does the plot compare to what you predicted?\nAdd the following two lines after your plot to add a linear (blue) and curved (red) smoothing line. What do you notice? Is a simple linear regression model appropriate for this data?\n\n\n# Add a red straight line of best fit and a blue curve of best fit\nYOUR_PLOT +\n    geom_smooth(method = \"lm\", color = \"red\", se = FALSE) +\n    geom_smooth(color = \"blue\", se = FALSE)"
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#exercise-4-filtering-our-data",
    "href": "template_qmds/04-slr-formalization-notes.html#exercise-4-filtering-our-data",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Exercise 4: Filtering our data",
    "text": "Exercise 4: Filtering our data\nThe relationship between registered riders and temperature looks linear below 80 degrees. We can use the filter() function from the dplyr package to subset our cases. (We’ll learn techniques soon for handling this nonlinear relationship.)\nIf we wanted to only keep cases where registered ridership was greater than 2000, we would use the following code:\n\n# The %&gt;% is called a \"pipe\" and feeds what comes before it\n# into what comes after (bikes data is \"fed into\" the filter() function)\nNEW_DATASET_NAME &lt;- bikes %&gt;% \n    filter(riders_registered &gt; 2000)\n\nAdapt the example above to create a new dataset called bikes_sub that only keeps cases where the felt temperature is less than 80 degrees."
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#exercise-5-model-fitting-and-coefficient-interpretation",
    "href": "template_qmds/04-slr-formalization-notes.html#exercise-5-model-fitting-and-coefficient-interpretation",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Exercise 5: Model fitting and coefficient interpretation",
    "text": "Exercise 5: Model fitting and coefficient interpretation\nLet’s fit a simple linear regression model and examine the results. Step through code chunk slowly, and make note of new code.\n\n# Construct and save the model as bike_mod\n# What's the purpose of \"riders_registered ~ temp_feel\"?\n# What's the purpose of \"data = bikes_sub\"?\nbike_mod &lt;- lm(riders_registered ~ temp_feel, data = bikes_sub)\n## Error in eval(mf, parent.frame()): object 'bikes_sub' not found\n\n\n# A long summary of the model stored in bike_mod\nsummary(bike_mod)\n## Error: object 'bike_mod' not found\n\n\n# A simplified model summary\ncoef(summary(bike_mod))\n## Error: object 'bike_mod' not found\n\n\nUsing the model summary output, complete the following model formula:\nE[riders_registered | temp_feel] = ___ + ___ * temp_feel\nInterpret the intercept in terms of the data context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases. Is the intercept meaningful in this situation?\nInterpret the slope in terms of the data context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases."
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#exercise-6-predictions-and-residuals",
    "href": "template_qmds/04-slr-formalization-notes.html#exercise-6-predictions-and-residuals",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Exercise 6: Predictions and residuals",
    "text": "Exercise 6: Predictions and residuals\nOn August 17, 2012, the temp_feel was 53.816 degrees and there were 5665 riders. We can get data for this day using the filter() and select() dplyr functions. Note, but don’t worry about the syntax – we haven’t learned this yet:\n\nbikes_sub %&gt;% \n    filter(date == \"2012-08-17\") %&gt;% \n    select(riders_registered, temp_feel) \n## Error: object 'bikes_sub' not found\n\n\nPeak back at the scatterplot. Identify which point corresponds to August 17, 2012. Is it close to the trend? Were there more riders than expected or fewer than expected?\nUse your model formula from the previous exercise to predict the ridership on August 17, 2012 from the temperature on that day. (That is, where do days with this temperature fall on the model trend line? How many registered riders would we expect on a 53.816 degree day?)\nCheck your part b calculation using the predict() function. Take careful note of the syntax – there’s a lot going on!\n\n\n# What is the purpose of newdata = ___???\npredict(bike_mod, newdata = data.frame(temp_feel = 53.816))\n## Error: object 'bike_mod' not found\n\n\nCalculate the residual or prediction error. How far does the observed ridership fall from the model prediction?\nresidual = observed y - predicted y = ???\nAre positive residuals above or below the trend line? When we have positive residuals, does the model over- or under-estimate ridership? Repeat these questions for negative residuals.\nFor an 85 degree day, how many registered riders would we expect? Do you think it’s a good idea to make this prediction? (Revisit the visualization and filtering we did in Exercises 3 and 4.)"
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#exercise-7-changing-temperature-units-challenge",
    "href": "template_qmds/04-slr-formalization-notes.html#exercise-7-changing-temperature-units-challenge",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Exercise 7: Changing temperature units (CHALLENGE)",
    "text": "Exercise 7: Changing temperature units (CHALLENGE)\nSuppose we had measured temperature in degrees Celsius rather than degrees Fahrenheit. How do you think our intercept and slope estimates, and their coefficient interpretations, would change?"
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#reflection",
    "href": "template_qmds/04-slr-formalization-notes.html#reflection",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Reflection",
    "text": "Reflection\nStatistics is a particular kind of language and collection of tools for channeling curiosity to improve our world.\nReview the learning objectives at the top of this file and the flow of today’s activity. How do the concepts we practiced today facilitate curiosity?\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#render-your-work",
    "href": "template_qmds/04-slr-formalization-notes.html#render-your-work",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Render your work",
    "text": "Render your work\n\nClick the “Render” button in the menu bar for this pane (blue arrow pointing right). This will create an HTML file containing all of the directions, code, and responses from this activity. A preview of the HTML will appear in the browser.\nScroll through and inspect the document to check that your work translated to the HTML format correctly.\nClose the browser tab.\nGo to the “Background Jobs” pane in RStudio and click the Stop button to end the rendering process.\nNavigate to your “Activities” subfolder within your “STAT155” folder and locate the HTML file. You can open it again in your browser to double check."
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#exercise-8-ridership-and-windspeed",
    "href": "template_qmds/04-slr-formalization-notes.html#exercise-8-ridership-and-windspeed",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Exercise 8: Ridership and windspeed",
    "text": "Exercise 8: Ridership and windspeed\nLet’s pull together everything that you’ve practiced in the preceding exercises to investigate the relationship between riders_registered and windspeed. Go back to using the bikes dataset (instead of bikes_sub) because we no longer need to only keep days less than 80 degrees.\n\n# Construct and interpret a visualization of this relationship\n# Include a representation of the relationship trend\n\n\n# Use lm to construct a model of riders_registered vs windspeed\n# Save this as bike_mod2\n\n\n# Get a short summary of this model\n\n\nSummarize your observations from the visualizations.\nWrite out a formula for the model trend.\nInterpret both the intercept and the windspeed coefficient. (Note: What does a negative slope indicate?)\nUse this model to predict the ridership on August 17, 2012 and calculate the corresponding residual. (Note: You’ll first need to find the windspeed on this date!)"
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#exercise-9-data-drills-filter-select-summarize",
    "href": "template_qmds/04-slr-formalization-notes.html#exercise-9-data-drills-filter-select-summarize",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Exercise 9: Data drills (filter, select, summarize)",
    "text": "Exercise 9: Data drills (filter, select, summarize)\nThis exercise is designed to help you keep building your dplyr skills. These skills are important to data cleaning and digging, which in turn is important to really making meaning of our data. We’ll work with a simpler set of 10 data points:\n\nnew_bikes &lt;- bikes %&gt;% \n    select(date, temp_feel, humidity, riders_registered, day_of_week) %&gt;% \n    head(10)\n\n\nVerb 1: summarize\nThus far, in the dplyr grammar you’ve seen 3 verbs or action words: summarize(), select(), filter(). Try out the following code and then summarize the point of the summarize() function:\n\nnew_bikes %&gt;% \n    summarize(mean(temp_feel), mean(humidity))\n## # A tibble: 1 × 2\n##   `mean(temp_feel)` `mean(humidity)`\n##               &lt;dbl&gt;            &lt;dbl&gt;\n## 1              52.0            0.544\n\n\n\nVerb 2: select\nTry out the following code and then summarize the point of the select() function:\n\nnew_bikes %&gt;%\n    select(date, temp_feel)\n## # A tibble: 10 × 2\n##    date       temp_feel\n##    &lt;date&gt;         &lt;dbl&gt;\n##  1 2011-01-01      64.7\n##  2 2011-01-02      63.8\n##  3 2011-01-03      49.0\n##  4 2011-01-04      51.1\n##  5 2011-01-05      52.6\n##  6 2011-01-06      53.0\n##  7 2011-01-07      50.8\n##  8 2011-01-08      46.6\n##  9 2011-01-09      42.5\n## 10 2011-01-10      45.6\n\n\nnew_bikes %&gt;% \n    select(-date, -temp_feel)\n## # A tibble: 10 × 3\n##    humidity riders_registered day_of_week\n##       &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n##  1    0.806               654 Sat        \n##  2    0.696               670 Sun        \n##  3    0.437              1229 Mon        \n##  4    0.590              1454 Tue        \n##  5    0.437              1518 Wed        \n##  6    0.518              1518 Thu        \n##  7    0.499              1362 Fri        \n##  8    0.536               891 Sat        \n##  9    0.434               768 Sun        \n## 10    0.483              1280 Mon\n\n\n\nVerb 3: filter\nTry out the following code and then summarize the point of the filter() function:\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850)\n## # A tibble: 7 × 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-03      49.0    0.437              1229 Mon        \n## 2 2011-01-04      51.1    0.590              1454 Tue        \n## 3 2011-01-05      52.6    0.437              1518 Wed        \n## 4 2011-01-06      53.0    0.518              1518 Thu        \n## 5 2011-01-07      50.8    0.499              1362 Fri        \n## 6 2011-01-08      46.6    0.536               891 Sat        \n## 7 2011-01-10      45.6    0.483              1280 Mon\n\n\nnew_bikes %&gt;% \n    filter(day_of_week == \"Sat\")\n## # A tibble: 2 × 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-01      64.7    0.806               654 Sat        \n## 2 2011-01-08      46.6    0.536               891 Sat\n\n\nnew_bikes %&gt;% \n    filter(riders_registered &gt; 850, day_of_week == \"Sat\")\n## # A tibble: 1 × 5\n##   date       temp_feel humidity riders_registered day_of_week\n##   &lt;date&gt;         &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;      \n## 1 2011-01-08      46.6    0.536               891 Sat"
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#exercise-10-your-turn",
    "href": "template_qmds/04-slr-formalization-notes.html#exercise-10-your-turn",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Exercise 10: Your turn",
    "text": "Exercise 10: Your turn\nUse dplyr verbs to complete each task below.\n\n# Keep only information about the humidity and day of week\n\n# Keep only information about the humidity and day of week using a different approach\n\n# Keep only information for Sundays\n\n# Keep only information for Sundays with temperatures below 50\n\n# Calculate the maximum and minimum temperatures"
  },
  {
    "objectID": "template_qmds/04-slr-formalization-notes.html#done",
    "href": "template_qmds/04-slr-formalization-notes.html#done",
    "title": "Simple linear regression: formalizing concepts (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/06-slr-transformations-notes.html",
    "href": "template_qmds/06-slr-transformations-notes.html",
    "title": "Simple linear regression: Transformations (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDistinguish between the different motivations for transformations of variables (interpretation, regression assumptions, etc.)\nDetermine when a particular transformation (center, scale, or log) may be appropriate\nInterpret regression coefficients after a transformation has taken place\n\n\n\n\nPlease watch the following video before class.\n\nVideo: Simple Linear Regression: Transformations\n\nThe following reading is optional.\n\nSection 3.8.4 in the STAT 155 Notes covers log transformations, and the “ladder of power,” which we will not cover in class.\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/06-slr-transformations-notes.html#learning-goals",
    "href": "template_qmds/06-slr-transformations-notes.html#learning-goals",
    "title": "Simple linear regression: Transformations (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDistinguish between the different motivations for transformations of variables (interpretation, regression assumptions, etc.)\nDetermine when a particular transformation (center, scale, or log) may be appropriate\nInterpret regression coefficients after a transformation has taken place"
  },
  {
    "objectID": "template_qmds/06-slr-transformations-notes.html#readings-and-videos",
    "href": "template_qmds/06-slr-transformations-notes.html#readings-and-videos",
    "title": "Simple linear regression: Transformations (Notes)",
    "section": "",
    "text": "Please watch the following video before class.\n\nVideo: Simple Linear Regression: Transformations\n\nThe following reading is optional.\n\nSection 3.8.4 in the STAT 155 Notes covers log transformations, and the “ladder of power,” which we will not cover in class.\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/06-slr-transformations-notes.html#exercise-1-location-transformations",
    "href": "template_qmds/06-slr-transformations-notes.html#exercise-1-location-transformations",
    "title": "Simple linear regression: Transformations (Notes)",
    "section": "Exercise 1: Location transformations",
    "text": "Exercise 1: Location transformations\nLocation transformations are ones that shift a predictor variable up or down by a fixed amount. Using a location transformation is sometimes also called centering a predictor.\nWe’ll use the homes data in this exercise.\n\nFit a linear regression model of Price as a function of Living.Area, and call this model home_mod.\n\n\n# Fit the model\n\n\n# Display model summary output\n\n\nInterpret the intercept and the coefficient for Living.Area. Is the interpretation of the intercept meaningful?\nWe can use a location transformation on Living.Area to “start” it at a more reasonable value. We can see from the summarize() code below that the smallest house is 616 quare feet, so let’s center this predictor at 600 square feet. There is no code to fill in here, but make note of the mutate() syntax.\n\n\nhomes %&gt;% \n    summarize(min(Living.Area))\n## # A tibble: 1 × 1\n##   `min(Living.Area)`\n##                &lt;dbl&gt;\n## 1                616\n\n# What is mutate() doing???\nhomes &lt;- homes %&gt;%\n    mutate(Living.Area.Shifted = Living.Area-600)\n\n\nWe can actually determine the coefficients of the Price ~ Living.Area.Shifted model by hand.\n\nFirst, write out in general terms (without specific numbers) how we would interpret the intercept and slope in this model.\nUse these general interpretations as well as the summary output of home_mod to determine what these new coefficients should be.\n\nNow check your answer to part d by fitting the model.\n\n\n# Fit a model of Price vs. Living.Area.Shifted\n\n\n# Display model summary output"
  },
  {
    "objectID": "template_qmds/06-slr-transformations-notes.html#exercise-2-scale-transformations",
    "href": "template_qmds/06-slr-transformations-notes.html#exercise-2-scale-transformations",
    "title": "Simple linear regression: Transformations (Notes)",
    "section": "Exercise 2: Scale transformations",
    "text": "Exercise 2: Scale transformations\nIn this exercise, we’ll explore the relationship between four-year graduation rate and admissions rate of colleges.\nIn the code chunk below, construct a visualization comparing graduation rate (our outcome variable) and admissions rate (our predictor of interest). Remember that your outcome variable should be on the y-axis, in general!\n\n# Scatterplot of graduation rate vs. admissions rate\n\n\nDescribe the relationship you observe between the two quantitative variables, in terms of correlation (weak/strong, positive/negative). Does the relationship appear to be roughly linear?\nWrite a linear regression model formula of the form E[Y | X] = … (filling in Y and X appropriately).\nFit this model in R, and report (don’t interpret yet!) the slope coefficient and intercept coefficient estimates.\n\n\n# Linear regression model with GradRate as the outcome, AdmisRate as predictor of interest\n\n\nIntercept Estimate: Your response here\n\n\nSlope Estimate: Your response here\n\n\nConsidering the units of AdmisRate, what does it mean for AdmisRate to change by one unit? What are the units for AdmisRate (and GradRate, for that matter!)?\nSuppose I want the interpretation of my slope coefficient for AdmisRate in my linear model to be in terms a “1% increase in admissions rate.” To achieve this, we could mutate our AdmisRate variable to range from 0 to 100. Let’s do that for GradRate too (just because!):\n\n\n# Mutate\ncollege &lt;- college %&gt;%\n  mutate(AdmisRate = AdmisRate * ___,\n         GradRate = ___ * ___)\n## Error in parse(text = input): &lt;text&gt;:3:35: unexpected input\n## 2: college &lt;- college %&gt;%\n## 3:   mutate(AdmisRate = AdmisRate * __\n##                                      ^\n\n\nFit a new linear regression model with the updated AdmisRate and GradRate variables as your predictor of interest and outcome, respectively. Again, report the intercept and slope estimate from your model.\n\n\n# Linear regression model with updated GradRate as the outcome, updated AdmisRate as predictor of interest\n\n\nIntercept Estimate: Your response here\n\n\nSlope Estimate: Your response here\n\nHow have your intercept and slope estimates changed from the previous model, if at all?\n\nInterpret the regression coefficient that corresponds to the estimated linear relationship between admissions and graduation rates, in the context of the problem. Make sure to use non-causal language, include units, and talk about averages rather than individual cases."
  },
  {
    "objectID": "template_qmds/06-slr-transformations-notes.html#exercise-3-log-transformations",
    "href": "template_qmds/06-slr-transformations-notes.html#exercise-3-log-transformations",
    "title": "Simple linear regression: Transformations (Notes)",
    "section": "Exercise 3: Log transformations",
    "text": "Exercise 3: Log transformations\nThe Big Mac Index has been published by The Economist since 1986 as a metric for comparing purchasing power between countries, giving rise to the phrase Burgernomics. It was developed (sort of jokingly) as a way to explain exchange rates in digestible terms.\nAs an example, suppose a Big Mac in Switzerland costs 6.70 Swiss franc, and in the U.S. a Big Mac costs 5.58 USD. Then the Big Mac Index is 6.70/5.58 = 1.20, and is the implied exchange rate between Swiss franc and USD.\nIf you’d like to read more about the Big Mac index, here’s an article in The Economist (this may be behind a pay-wall for you, you can read up to 5 free articles in the Economist per month).\nFor this exercise, we’ll explore the relationship between average teaching salary in a country and the amount of time someone needs to work to be able to afford a Big Mac. The variables we’ll consider are:\n\nbigmac_mins: average minutes to earn 1 Big Mac\ngross_annual_teacher_income: average gross teacher salary in 1 year (USD)\n\n\nCreate an appropriate visualization that displays the relationship between average minutes to earn a Big Mac and gross annual, average teaching salary, and describe what you observe.\n\n\n# Visualization: Big Mac minutes vs. gross annual teacher income\n\n\nExplain why correlation might not be an appropriate numerical summary for the relationship between the two variables you plotted above.\nFit a linear regression model with bigmac_mins as the outcome and gross_annual_teacher_income as the predictor of interest, and interpret the coefficient for gross_annual_teacher_income, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\n\n\n# Linear regression code\n\n\nPlot residuals vs. fitted values for the model you fit, and describe what you observe. Are there any noticeable patterns in the residuals? Describe them!\n\n\n# Residuals vs. fitted values plot\n\n\nFor which observations do the residuals from the linear regression model appear to be relatively large (i.e. for which observations would predictions fall farthest from observed outcomes)? What possible consequences would this have for people using this model to predict the amount of time it takes for them to earn enough money to afford a Big Mac?\n\nWe’ll now consider a log transformation of teaching salary. In the code chunk below, create a new variable called log_sal that contains the logged values of gross_annual_teacher_income.\n\n# Creating new variable log_sal\nbigmac &lt;- bigmac %&gt;%\n  mutate(log_sal = log(___))\n## Error in parse(text = input): &lt;text&gt;:3:25: unexpected input\n## 2: bigmac &lt;- bigmac %&gt;%\n## 3:   mutate(log_sal = log(__\n##                            ^\n\n\nCreate an appropriate visualization that displays the relationship between average minutes to earn a Big Mac and logged gross annual, average teaching salary, and describe what you observe. Does correlation seem like it may be an appropriate numerical summary for the relationship between these two variables? Explain why or why not.\nFit a linear regression model with bigmac_mins as the outcome and log_sal as the predictor of interest, and interpret the coefficient for log_sal, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\nPlot residuals vs. fitted values for the model you fit, and describe what you observe. Are there any noticeable patterns in the residuals? Describe them!\n\n\n# Residuals vs. fitted values plot"
  },
  {
    "objectID": "template_qmds/06-slr-transformations-notes.html#reflection",
    "href": "template_qmds/06-slr-transformations-notes.html#reflection",
    "title": "Simple linear regression: Transformations (Notes)",
    "section": "Reflection",
    "text": "Reflection\nTwo of the main motivations for transforming variables in our regression models is to (1) intentionally change the interpretation of regression coefficients, and (2) to better satisfy linear regression assumptions (e.g. remove “patterns” from our residual plots). The first is nearly always justified by the scientific context of the research questions you are trying to answer, while the second is a bit more muddy.\nThink about the pros and cons of transforming your variables to satisfy linear regression assumptions. Is there a limit to how much you would be willing to transform your variables? Would transforming too much leave you with un-interpretable regression coefficients?\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/06-slr-transformations-notes.html#done",
    "href": "template_qmds/06-slr-transformations-notes.html#done",
    "title": "Simple linear regression: Transformations (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html",
    "href": "template_qmds/08-mlr-intro-notes.html",
    "title": "Introduction to multiple regression (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be familiar with:\n\nsome limitations of simple linear regression\nthe general goals behind multiple linear regression\nstrategies for visualizing and interpreting multiple linear regression models of \\(Y\\) vs 2 predictors, 1 quantitative and 1 categorical\n\n\n\n\nToday is a day to discover ideas, so no readings or videos to go through before class.\n\n\n\nEXAMPLE 1\nLet’s explore some data on penguins. First, enter install.packages(\"palmerpenguins\") in the console (not Rmd). Then load the penguins data. You can find a codebook for these data by typing ?penguins in your console (not Rmd).\n\n# Load packages\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load data\nlibrary(palmerpenguins)\n## Error in library(palmerpenguins): there is no package called 'palmerpenguins'\ndata(penguins)\npenguins &lt;- penguins %&gt;% \n  filter(species != \"Adelie\", bill_length_mm &lt; 57)\n## Error in `filter()`:\n## ℹ In argument: `bill_length_mm &lt; 57`.\n## Caused by error:\n## ! object 'bill_length_mm' not found\n\n# Check it out\nhead(penguins)\n##   species    island bill_len bill_dep flipper_len body_mass    sex year\n## 1  Adelie Torgersen     39.1     18.7         181      3750   male 2007\n## 2  Adelie Torgersen     39.5     17.4         186      3800 female 2007\n## 3  Adelie Torgersen     40.3     18.0         195      3250 female 2007\n## 4  Adelie Torgersen       NA       NA          NA        NA   &lt;NA&gt; 2007\n## 5  Adelie Torgersen     36.7     19.3         193      3450 female 2007\n## 6  Adelie Torgersen     39.3     20.6         190      3650   male 2007\n\nOur goal is to build a model that we can use to get good predictions of penguins’ flipper (“arm”) lengths. Consider 2 simple linear regression models of flipper_length_mm by penguin sex and species:\n\nsummary(lm(flipper_length_mm ~ sex, penguins))$r.squared\n## Error in eval(predvars, data, env): object 'flipper_length_mm' not found\nsummary(lm(flipper_length_mm ~ species, penguins))$r.squared\n## Error in eval(predvars, data, env): object 'flipper_length_mm' not found\n\nHow might we improve our predictions of flipper_length_mm using only these 2 predictors? What do you think is a reasonable range of possible values for the new R-squqared?\nEXAMPLE 2\nConsider a simple linear regression model of flipper_length_mm by bill_length_mm:\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_length_mm, x = bill_length_mm)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error:\n## ! object 'bill_length_mm' not found\n\nThoughts? What’s going on here? How does this highlight the limitations of a simple linear regression model?\nEXAMPLE 3\nThe cps dataset contains employment information collected by the U.S. Current Population Survey (CPS) in 2018. We can use these data to explore wages among 18-34 year olds. The original codebook is here.\n\n# Import data\ncps &lt;- read_csv(\"https://mac-stat.github.io/data/cps_2018.csv\") %&gt;% \n  select(-education, -hours) %&gt;% \n  filter(age &gt;= 18, age &lt;= 34) %&gt;% \n  filter(wage &lt; 250000)\n\n\n# Check it out\nhead(cps)\n## # A tibble: 6 × 6\n##    wage   age marital industry   health    education_level\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;          \n## 1 75000    33 single  management fair      bachelors      \n## 2 33000    19 single  management very_good bachelors      \n## 3 43000    33 married management good      bachelors      \n## 4 50000    32 single  management excellent HS             \n## 5 14400    28 single  service    excellent HS             \n## 6 33000    31 married management very_good bachelors\n\nWe can use a simple linear regression model to summarize the relationship of wage with marital status:\n\n# Build the model\nwage_mod &lt;- lm(wage ~ marital, data = cps)\n\n# Summarize the model\ncoef(summary(wage_mod))\n##                Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)    46145.23    921.062  50.10002 0.000000e+00\n## maritalsingle -17052.37   1127.177 -15.12839 5.636068e-50\n\nWhat do you / don’t you conclude from this model? How does it highlight the limitations of a simple linear regression model?\nReflection: Why are multiple regression models so useful?\nWe can put more than 1 predictor into a regression model! Adding predictors to models…\n\nPredictive viewpoint: Helps us better predict the response\nDescriptive viewpoint: Helps us better understand the isolated (causal) effect of a variable by holding constant confounders\n\nMultiple linear regression model formula\nIn general, a multiple linear regression model of \\(Y\\) with multiple predictors \\((X_1, X_2, ..., X_p)\\) is represented by the following formula:\n\\[E[Y \\mid X_1, X_2, ..., X_p] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\]"
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#learning-goals",
    "href": "template_qmds/08-mlr-intro-notes.html#learning-goals",
    "title": "Introduction to multiple regression (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be familiar with:\n\nsome limitations of simple linear regression\nthe general goals behind multiple linear regression\nstrategies for visualizing and interpreting multiple linear regression models of \\(Y\\) vs 2 predictors, 1 quantitative and 1 categorical"
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#readings-and-videos",
    "href": "template_qmds/08-mlr-intro-notes.html#readings-and-videos",
    "title": "Introduction to multiple regression (Notes)",
    "section": "",
    "text": "Today is a day to discover ideas, so no readings or videos to go through before class."
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#motivation",
    "href": "template_qmds/08-mlr-intro-notes.html#motivation",
    "title": "Introduction to multiple regression (Notes)",
    "section": "",
    "text": "EXAMPLE 1\nLet’s explore some data on penguins. First, enter install.packages(\"palmerpenguins\") in the console (not Rmd). Then load the penguins data. You can find a codebook for these data by typing ?penguins in your console (not Rmd).\n\n# Load packages\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Load data\nlibrary(palmerpenguins)\n## Error in library(palmerpenguins): there is no package called 'palmerpenguins'\ndata(penguins)\npenguins &lt;- penguins %&gt;% \n  filter(species != \"Adelie\", bill_length_mm &lt; 57)\n## Error in `filter()`:\n## ℹ In argument: `bill_length_mm &lt; 57`.\n## Caused by error:\n## ! object 'bill_length_mm' not found\n\n# Check it out\nhead(penguins)\n##   species    island bill_len bill_dep flipper_len body_mass    sex year\n## 1  Adelie Torgersen     39.1     18.7         181      3750   male 2007\n## 2  Adelie Torgersen     39.5     17.4         186      3800 female 2007\n## 3  Adelie Torgersen     40.3     18.0         195      3250 female 2007\n## 4  Adelie Torgersen       NA       NA          NA        NA   &lt;NA&gt; 2007\n## 5  Adelie Torgersen     36.7     19.3         193      3450 female 2007\n## 6  Adelie Torgersen     39.3     20.6         190      3650   male 2007\n\nOur goal is to build a model that we can use to get good predictions of penguins’ flipper (“arm”) lengths. Consider 2 simple linear regression models of flipper_length_mm by penguin sex and species:\n\nsummary(lm(flipper_length_mm ~ sex, penguins))$r.squared\n## Error in eval(predvars, data, env): object 'flipper_length_mm' not found\nsummary(lm(flipper_length_mm ~ species, penguins))$r.squared\n## Error in eval(predvars, data, env): object 'flipper_length_mm' not found\n\nHow might we improve our predictions of flipper_length_mm using only these 2 predictors? What do you think is a reasonable range of possible values for the new R-squqared?\nEXAMPLE 2\nConsider a simple linear regression model of flipper_length_mm by bill_length_mm:\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_length_mm, x = bill_length_mm)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error:\n## ! object 'bill_length_mm' not found\n\nThoughts? What’s going on here? How does this highlight the limitations of a simple linear regression model?\nEXAMPLE 3\nThe cps dataset contains employment information collected by the U.S. Current Population Survey (CPS) in 2018. We can use these data to explore wages among 18-34 year olds. The original codebook is here.\n\n# Import data\ncps &lt;- read_csv(\"https://mac-stat.github.io/data/cps_2018.csv\") %&gt;% \n  select(-education, -hours) %&gt;% \n  filter(age &gt;= 18, age &lt;= 34) %&gt;% \n  filter(wage &lt; 250000)\n\n\n# Check it out\nhead(cps)\n## # A tibble: 6 × 6\n##    wage   age marital industry   health    education_level\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;          \n## 1 75000    33 single  management fair      bachelors      \n## 2 33000    19 single  management very_good bachelors      \n## 3 43000    33 married management good      bachelors      \n## 4 50000    32 single  management excellent HS             \n## 5 14400    28 single  service    excellent HS             \n## 6 33000    31 married management very_good bachelors\n\nWe can use a simple linear regression model to summarize the relationship of wage with marital status:\n\n# Build the model\nwage_mod &lt;- lm(wage ~ marital, data = cps)\n\n# Summarize the model\ncoef(summary(wage_mod))\n##                Estimate Std. Error   t value     Pr(&gt;|t|)\n## (Intercept)    46145.23    921.062  50.10002 0.000000e+00\n## maritalsingle -17052.37   1127.177 -15.12839 5.636068e-50\n\nWhat do you / don’t you conclude from this model? How does it highlight the limitations of a simple linear regression model?\nReflection: Why are multiple regression models so useful?\nWe can put more than 1 predictor into a regression model! Adding predictors to models…\n\nPredictive viewpoint: Helps us better predict the response\nDescriptive viewpoint: Helps us better understand the isolated (causal) effect of a variable by holding constant confounders\n\nMultiple linear regression model formula\nIn general, a multiple linear regression model of \\(Y\\) with multiple predictors \\((X_1, X_2, ..., X_p)\\) is represented by the following formula:\n\\[E[Y \\mid X_1, X_2, ..., X_p] = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\]"
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#exercise-1-visualizing-the-relationship",
    "href": "template_qmds/08-mlr-intro-notes.html#exercise-1-visualizing-the-relationship",
    "title": "Introduction to multiple regression (Notes)",
    "section": "Exercise 1: Visualizing the relationship",
    "text": "Exercise 1: Visualizing the relationship\nWe’ve learned how to visualize the relationship of flipper_length_mm by bill_length_mm alone:\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_length_mm, x = bill_length_mm)) + \n  geom_point()\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error:\n## ! object 'bill_length_mm' not found\n\n\nTHINK: How might we change the scatterplot points to also indicate information about penguin species? (There’s more than 1 approach!)\nTry out your idea by modifying the code below. If you get stuck, talk with the tables around you!\n\n\npenguins %&gt;%\n  ggplot(aes(y = flipper_length_mm, x = bill_length_mm, ___ = ___)) +\n  geom_point()\n## Error in parse(text = input): &lt;text&gt;:2:58: unexpected input\n## 1: penguins %&gt;%\n## 2:   ggplot(aes(y = flipper_length_mm, x = bill_length_mm, __\n##                                                             ^"
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#exercise-2-visualizing-the-model",
    "href": "template_qmds/08-mlr-intro-notes.html#exercise-2-visualizing-the-model",
    "title": "Introduction to multiple regression (Notes)",
    "section": "Exercise 2: Visualizing the model",
    "text": "Exercise 2: Visualizing the model\nWe’ve also learned that a simple linear regression model of flipper_length_mm by bill_length_mm alone can be represented by a line:\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_length_mm, x = bill_length_mm)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error:\n## ! object 'bill_length_mm' not found\n\n\nTHINK: Reflecting on your plot of flipper_length_mm by bill_length_mm and species in Exercise 1, how do you think a multiple regression model of flipper_length_mm using both of these predictors would be represented?\nCheck your intuition below by modifying the code below to include species in this plot, as you did in Exercise 1.\n\n\npenguins %&gt;%\n  ggplot(aes(y = flipper_length_mm, x = bill_length_mm, ___ = ___)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)\n## Error in parse(text = input): &lt;text&gt;:2:58: unexpected input\n## 1: penguins %&gt;%\n## 2:   ggplot(aes(y = flipper_length_mm, x = bill_length_mm, __\n##                                                             ^"
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#exercise-3-intuition",
    "href": "template_qmds/08-mlr-intro-notes.html#exercise-3-intuition",
    "title": "Introduction to multiple regression (Notes)",
    "section": "Exercise 3: Intuition",
    "text": "Exercise 3: Intuition\nYour plot in Exercise 2 demonstrated that the multiple linear regression model of flipper_length_mm by bill_length_mm and species is represented by 2 lines. Let’s interpret the punchlines!\nFor each question, provide an answer along with evidence from the model lines that supports your answer.\n\nWhat’s the relationship between flipper_length_mm and species, no matter a penguin’s bill_length_mm?\nWhat’s the relationship between flipper_length_mm and bill_length_mm, no matter a penguin’s species?\nDoes the rate of increase in flipper_length_mm with bill_length_mm differ between the two species?"
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#exercise-4-model-formula",
    "href": "template_qmds/08-mlr-intro-notes.html#exercise-4-model-formula",
    "title": "Introduction to multiple regression (Notes)",
    "section": "Exercise 4: Model formula",
    "text": "Exercise 4: Model formula\nOf course, there’s a formula behind the multiple regression model. We can obtain this using the usual lm() function.\n\n# Build the model\npenguin_mod &lt;- lm(flipper_length_mm ~ bill_length_mm + species, data = penguins)\n## Error in eval(predvars, data, env): object 'flipper_length_mm' not found\n\n# Summarize the model\ncoef(summary(penguin_mod))\n## Error: object 'penguin_mod' not found\n\n\nIn the lm() function, how did we communicate that we wanted to model flipper_length_mm by both bill_length_mm and species?\nComplete the following model formula:\nE[flipper_length_mm | bill_length_mm, speciesGentoo] = ___ + ___ * bill_length_mm + ___ * speciesGentoo"
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#exercise-5-sub-model-formulas",
    "href": "template_qmds/08-mlr-intro-notes.html#exercise-5-sub-model-formulas",
    "title": "Introduction to multiple regression (Notes)",
    "section": "Exercise 5: Sub-model formulas",
    "text": "Exercise 5: Sub-model formulas\nOk. We now have a single formula for the model.\nAnd we observed earlier that this formula is represented by two lines: one describing the relationship between flipper_length_mm and bill_length_mm for Chinstrap penguins and the other for Gentoo penguins.\nLet’s bring these ideas together.\nUtilize the model formula to obtain the equations of these two lines, i.e. to obtain the sub-model formulas for the 2 species. Hint: Plug speciesGentoo = 0 and speciesGentoo = 1.\nChinstrap: flipper_length_mm = ___ + ___ bill_length_mm\nGentoo: flipper_length_mm = ___ + ___ bill_length_mm"
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#exercise-6-coefficients-physical-interpretation",
    "href": "template_qmds/08-mlr-intro-notes.html#exercise-6-coefficients-physical-interpretation",
    "title": "Introduction to multiple regression (Notes)",
    "section": "Exercise 6: coefficients – physical interpretation",
    "text": "Exercise 6: coefficients – physical interpretation\nReflecting on Exercise 5, let’s interpret what the model coefficients tell us about the physical properties of the two 2 sub-model lines. Choose the correct option given in parentheses:\n\nThe intercept coefficient, 127.75, is the intercept of the line for (Chinstrap / Gentoo) penguins.\nThe bill_length_mm coefficient, 1.40, is the (intercept / slope) of both lines.\nThe speciesGentoo coefficient, 22.85, indicates that the (intercept / slope) of the line for Gentoo is 22.85mm higher than the (intercept / slope) of the line for Chinstrap. Similarly, since the lines are parallel, the line for Gentoo is 22.85mm higher than the line for Chinstrap at any bill_length_mm."
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#exercise-7-coefficients-contextual-interpretation",
    "href": "template_qmds/08-mlr-intro-notes.html#exercise-7-coefficients-contextual-interpretation",
    "title": "Introduction to multiple regression (Notes)",
    "section": "Exercise 7: coefficients – contextual interpretation",
    "text": "Exercise 7: coefficients – contextual interpretation\nNext, interpret each coefficient in a contextually meaningful way. What do they tell us about penguin flipper lengths?!\n\nInterpret 127.75 (intercept of the Chinstrap line).\nInterpret 1.40 (slope of both lines). For both Chinstrap and Gentoo penguins, we expect…\nInterpret 22.85. At any bill_length_mm, we expect…"
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#exercise-8-prediction",
    "href": "template_qmds/08-mlr-intro-notes.html#exercise-8-prediction",
    "title": "Introduction to multiple regression (Notes)",
    "section": "Exercise 8: Prediction",
    "text": "Exercise 8: Prediction\nNow that we better understand the model, let’s use it to predict flipper lengths! Recall the model summary and visualization:\n\ncoef(summary(penguin_mod))\n## Error: object 'penguin_mod' not found\n\npenguins %&gt;% \n  ggplot(aes(y = flipper_length_mm, x = bill_length_mm, color = species)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error:\n## ! object 'bill_length_mm' not found\n\n\nPredict the flipper length of a Chinstrap penguin with a 50mm long bill. Make sure your calculation is consistent with the plot.\n\n\n127.75 + 1.40*___ + 22.85*___\n## Error in parse(text = input): &lt;text&gt;:1:16: unexpected input\n## 1: 127.75 + 1.40*__\n##                    ^\n\n\nPredict the flipper length of a Gentoo penguin with a 50mm long bill. Make sure your calculation is consistent with the plot.\n\n\n127.75 + 1.40*___ + 22.85*___\n## Error in parse(text = input): &lt;text&gt;:1:16: unexpected input\n## 1: 127.75 + 1.40*__\n##                    ^\n\n\nUse the predict() function to confirm your predictions in parts a and b.\n\n\n# Confirm the calculation in part a\npredict(penguin_mod,\n        newdata = data.frame(bill_length_mm = ___, species = \"___\"))\n\n# Confirm the calculation in part b\npredict(penguin_mod,\n        newdata = data.frame(bill_length_mm = ___, species = \"___\"))\n## Error in parse(text = input): &lt;text&gt;:3:48: unexpected input\n## 2: predict(penguin_mod,\n## 3:         newdata = data.frame(bill_length_mm = __\n##                                                   ^"
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#exercise-9-r-squared",
    "href": "template_qmds/08-mlr-intro-notes.html#exercise-9-r-squared",
    "title": "Introduction to multiple regression (Notes)",
    "section": "Exercise 9: R-squared",
    "text": "Exercise 9: R-squared\nFinally, recall that improving our predictions was one motivation for multiple linear regression (using 2 predictors instead of 1). To this end, consider the R-squared values of the simple linear regression models that use just one predictor at a time:\n\nmod_bill &lt;- lm(flipper_length_mm ~ bill_length_mm, data = penguins)\n## Error in eval(predvars, data, env): object 'flipper_length_mm' not found\nsummary(mod_bill)\n## Error: object 'mod_bill' not found\n\nmod_species &lt;- lm(flipper_length_mm ~ species, data = penguins)\n## Error in eval(predvars, data, env): object 'flipper_length_mm' not found\nsummary(mod_species)\n## Error: object 'mod_species' not found\n\n\nIf you had to use only 1 of our 2 predictors, which would give the better predictions of flipper_length_mm?\nWhat do you guess is the R-squared of our multiple regression model that uses both of these predictors? Why?\nCheck your intuition. How does the R-squared of our multiple regression model compare to that of the 2 separate simple linear regression models?\n\n\nsummary(penguin_mod)\n## Error: object 'penguin_mod' not found"
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#reflection",
    "href": "template_qmds/08-mlr-intro-notes.html#reflection",
    "title": "Introduction to multiple regression (Notes)",
    "section": "Reflection",
    "text": "Reflection\nYou’ve now explored your first multiple regression model! Thus you likely have a lot of questions about what’s to come. What are they?\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/08-mlr-intro-notes.html#done",
    "href": "template_qmds/08-mlr-intro-notes.html#done",
    "title": "Introduction to multiple regression (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html",
    "href": "template_qmds/10-mlr-confounding-notes.html",
    "title": "Confounding variables (Notes)",
    "section": "",
    "text": "Slides with comments on Quiz 1\nYou can download a template file to work with here.\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder.\n\n\n\nBy the end of this lesson, you should be familiar with:\n\nconfounding variables\nhow to control for confounding variables in our models\nhow to represent the role of confounding variables using causal diagrams\n\n\n\n\nBefore class you should have read and watched:\n\nSections 3.9.2 in the STAT 155 Notes\nConfounding (and other causal diagrams)\n\nWatch from 0:00 - 6:54"
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#learning-goals",
    "href": "template_qmds/10-mlr-confounding-notes.html#learning-goals",
    "title": "Confounding variables (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be familiar with:\n\nconfounding variables\nhow to control for confounding variables in our models\nhow to represent the role of confounding variables using causal diagrams"
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#readings-and-videos",
    "href": "template_qmds/10-mlr-confounding-notes.html#readings-and-videos",
    "title": "Confounding variables (Notes)",
    "section": "",
    "text": "Before class you should have read and watched:\n\nSections 3.9.2 in the STAT 155 Notes\nConfounding (and other causal diagrams)\n\nWatch from 0:00 - 6:54"
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-1-review",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-1-review",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 1: Review",
    "text": "Exercise 1: Review\nThe peaks data includes information on hiking trails in the 46 “high peaks” in the Adirondack mountains of northern New York state:\n\n# Load useful packages and data\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\npeaks &lt;- read_csv(\"https://mac-stat.github.io/data/high_peaks.csv\") %&gt;%\n    mutate(ascent = ascent / 1000)\n\n# Check it out \nhead(peaks)\n## # A tibble: 6 × 7\n##   peak           elevation difficulty ascent length  time rating   \n##   &lt;chr&gt;              &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;    \n## 1 Mt. Marcy           5344          5   3.17   14.8  10   moderate \n## 2 Algonquin Peak      5114          5   2.94    9.6   9   moderate \n## 3 Mt. Haystack        4960          7   3.57   17.8  12   difficult\n## 4 Mt. Skylight        4926          7   4.26   17.9  15   difficult\n## 5 Whiteface Mtn.      4867          4   2.54   10.4   8.5 easy     \n## 6 Dix Mtn.            4857          5   2.8    13.2  10   moderate\n\nBelow is a model of the time (in hours) that it takes to complete a hike by the hike’s length (in miles), vertical ascent(in 1000s of feet), and rating (easy, moderate, or difficult):\n\npeaks_model &lt;- lm(time ~ length + ascent + rating, data = peaks)\ncoef(summary(peaks_model))\n\nInterpret the length and ratingeasy coefficients in the model formula below by using our strategy:\n\nStrategy: When interpreting a coefficient for a variable x, compare two units whose values of x differ by 1 but who are identical for all other variables.\n\nE[time | length, ascent, rating] = 6.511 + 0.459 length + 0.187 ascent - 3.169 ratingeasy - 2.477 ratingmoderate\n\nSynthesis:\n\nInterpreting the coefficient \\(\\beta_Q\\) for a quantitative variable Q:\n\nHolding all other variables constant, each unit increase in Q is associated with \\(\\beta_Q\\) change (note if it’s an increase or decrease) in Y on average.\n\nInterpreting the coefficient \\(\\beta_C\\) for an indicator variable:\n\nHolding all other variables constant, the average outcome for the group referenced by this indicator (group for whom indicator = 1), is \\(\\beta_C\\) higher/lower than that of the reference group."
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-2-confounders",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-2-confounders",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 2: Confounders",
    "text": "Exercise 2: Confounders\n\nResearch question: Is there a wage gap, hence possibly discrimination, by marital status among 18-34 year olds?\n\nTo explore, we can revisit the cps data with employment information collected by the U.S. Current Population Survey (CPS) in 2018. View the codebook here.\n\n# Import data\ncps &lt;- read_csv(\"https://mac-stat.github.io/data/cps_2018.csv\") %&gt;% \n    filter(age &gt;= 18, age &lt;= 34) %&gt;% \n    filter(wage &lt; 250000)\n\n# Check it out\nhead(cps)\n## # A tibble: 6 × 8\n##    wage   age education marital industry   health    hours education_level\n##   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;          \n## 1 75000    33        16 single  management fair         40 bachelors      \n## 2 33000    19        16 single  management very_good    40 bachelors      \n## 3 43000    33        16 married management good         40 bachelors      \n## 4 50000    32        12 single  management excellent    40 HS             \n## 5 14400    28        12 single  service    excellent    40 HS             \n## 6 33000    31        16 married management very_good    45 bachelors\n\nRecall that a simple linear regression model of wage by marital suggests that single workers make $17,052 less than married workers:\n\nwage_model_1 &lt;- lm(wage ~ marital, data = cps)\ncoef(summary(wage_model_1))\n\nThat’s a big gap!!\nBUT this model ignores important confounding variables that might help explain this gap.\nA confounding variable is a cause of both the predictor of interest (marital) and of the response variable (wage).\nWe can represent this idea with a causal diagram:\n\n\n\n\n\n\n\n\n\nAnother definition of a confounding variable is one that\n\nis a cause of the outcome (wage)\nis associated with the main variable of interest (marital status)\nNOT caused by the variable of interest\n\nWe can represent this on the causal diagram with a line from the confounder to the variable of interest (instead of an arrow):\n\n\n\n\n\n\n\n\n\nName at least 2 potential confounders."
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-2b-how-why-do-confounders-bias-results",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-2b-how-why-do-confounders-bias-results",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 2b: How & why do confounders bias results?",
    "text": "Exercise 2b: How & why do confounders bias results?\nUnaccounted-for confounders are often a source of bias in our models, meaning that when we ignore them, we often over- or under-estimate the true underlying relationship between a predictor and response variable. To explore why this is important, let’s first look at how our focal predictor marital is associated with our response variable, wage:\n\ncps %&gt;%\n  ggplot(aes(x=marital, y=wage))+\n  geom_boxplot()+\n  theme_classic()\n\n\n\n\n\n\n\n\nNow, let’s consider age as a potential confounder. The following plot shows how age is associated with marital status:\n\ncps %&gt;%\n  ggplot(aes(x=age, y=marital))+\n  geom_boxplot()+\n  theme_classic()\n\n\n\n\n\n\n\n\n…this should make sense, because the older a person is, the more likely they are to be married. Similarly, we can show how age is associated with wage:\n\ncps %&gt;%\n  ggplot(aes(x=age, y=wage))+\n  geom_point()+\n  geom_smooth(method=\"lm\", se=F)+\n  theme_classic()\n\n\n\n\n\n\n\n\nHere we see that there is a positive correlation between age and wages (which again makes sense, because people who have been in the workforce longer typically earn more).\nLet’s revisit our initial plot showing the relationship between marital status and wages:\n\ncps %&gt;%\n  ggplot(aes(x=marital, y=wage))+\n  geom_boxplot()+\n  theme_classic()\n\n\n\n\n\n\n\n\nSince we now know that age is associated with both being married and higher wages, this plot doesn’t tell the full story–people who are married could simply be earning higher wages because they tend to be older, not necessarily because they are married! Age is therefore a confounder in the relationship between marital status and wages."
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-3-controlling-for-confounders",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-3-controlling-for-confounders",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 3: Controlling for confounders",
    "text": "Exercise 3: Controlling for confounders\nThe exercise above illustrates that it is important to control or adjust for confounding variables when trying to understand the actual causal relationship between a predictor (e.g. marital) and response (e.g. wage).\n\nSometimes, we can control (adjust) for confounding variables through a carefully designed experiment. For example, in comparing the effectiveness (y) of 2 different cold remedies (x), we might want to control for the age, general health, and severity of symptoms among the participants. How might we do that?\nBUT we’re often working with observational, not experimental, data. Why? Well, explain what an experiment might look like if we wanted to explore the relationship between wage (y) and marital status (x) while controlling for age."
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-4-age",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-4-age",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 4: Age",
    "text": "Exercise 4: Age\nWe’re in luck.\nWe can control (adjust) for confounding variables by including them in our model!\nThat’s one of the superpowers of multiple linear regression.\nLet’s start simple, by controlling for age in our model of wages by marital status:\n\n# Construct the model\nwage_model_2 &lt;- lm(wage ~ marital + age, cps)\ncoef(summary(wage_model_2))\n\n\nVisualize this model by modifying the code below.\n\n(Note: The last line where we add a geom_line layer adds in trendlines similar to what we might obtain using geom_smooth, but it uses the exact fitted values from our model. geom_smooth, on the other hand, adds in trendlines based on fitting two separate models to the married and single subsets of the data. Tray adding geom_smooth(method=\"lm\", se=F, linetype=\"dashed\") to the plot to see how they compare).\n\nggplot(cps, aes(y = ___, x = ___, color = ___)) +\n    geom____(size = 0.1, alpha = 0.5) +\n    geom_line(aes(y = wage_model_2$fitted.values), linewidth = 0.5)\n\n\nSuppose 2 workers are the same age, but one is married and one is single. By how much do we expect the single worker’s wage to differ from the married worker’s wage? (How does this compare to the $17,052 marital gap among all workers?)\nHow can we interpret the maritalsingle coefficient?"
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-5-more-confounders",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-5-more-confounders",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 5: More confounders",
    "text": "Exercise 5: More confounders\nLet’s control for even more potential confounders!\nModel wages by marital status while controlling for age and years of education:\n\nwage_model_3 &lt;- lm(wage ~ marital + age + education, cps)\ncoef(summary(wage_model_3))\n\n\nWith so many variables, this is a tough model to visualize. If you had to draw it, how would the model trend appear: 1 point, 2 points, 2 lines, 1 plane, or 2 planes? Explain your rationale. Hint: pay attention to whether your predictors are quantitative or categorical.\nGiven our research question, which coefficient is of primary interest? Interpret this coefficient.\nInterpret the two other coefficients in this model."
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-6-even-more",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-6-even-more",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 6: Even more",
    "text": "Exercise 6: Even more\nLet’s control for another potential confounder, the job industry in which one works (categorical):\n\nwage_model_4 &lt;- lm(wage ~ marital + age + education + industry, cps)\ncoef(summary(wage_model_4))\n\nIf we had to draw it, this model would appear as 12 planes.\nThe original plane explains the relationship between wage and the 2 quantitative predictors, age and education.\nThen this plane is split into 12 (2*6) individual planes, 1 for each possible combination of marital status (2 possibilities) and industry (6 possibilities).\n\nInterpret the main coefficient of interest for our research question.\nWhen controlling for a worker’s age, marital status, and education level, which industry tends to have the highest wages? The lowest? Note: the following table shows the 6 industries:\n\n\ncps %&gt;% count(industry)"
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-7-biggest-model-yet",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-7-biggest-model-yet",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 7: Biggest model yet",
    "text": "Exercise 7: Biggest model yet\nBuild a model that helps us explore wage by marital status while controlling for: age, education, job industry, typical number of work hours, and health status.\nStore this model as wage_model_5."
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-8-reflection",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-8-reflection",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 8: Reflection",
    "text": "Exercise 8: Reflection\nTake two workers – one is married and the other is single.\nThe models above provided the following insights into the typical difference in wages for these two groups:\n\n\n\nModel\nAssume the two people have the same…\nWage difference\n\n\n\n\nwage_model_1\nNA\n-$17,052\n\n\nwage_model_2\nage\n-$7,500\n\n\nwage_model_3\nage, education\n-$6,478\n\n\nwage_model_4\nage, education, industry\n-$5,893\n\n\nwage_model_5\nage, education, industry, hours, health\n-$4,993\n\n\n\n\nThough not the case in every analysis, the marital coefficient got closer and closer to 0 as we controlled for more confounders. Explain the significance of this phenomenon, in context - what does it mean?\nDo you still find the wage gap for single vs married people to be meaningfully “large”? Can you think of any remaining factors that might explain part of this remaining gap? Or do you think we’ve found evidence of inequitable wage practices for single vs married workers?"
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-9-a-new-extreme-example",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-9-a-new-extreme-example",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 9: A new (extreme) example",
    "text": "Exercise 9: A new (extreme) example\nFor a more extreme example of why it’s important to control for confounding variables, let’s return to the diamonds data:\n\n# Import and wrangle the data\ndata(diamonds)\ndiamonds &lt;- diamonds %&gt;% \n    mutate(\n        cut = factor(cut, ordered = FALSE),\n        color = factor(color, ordered = FALSE),\n        clarity = factor(clarity, ordered = FALSE)\n    ) %&gt;% \n    select(price, clarity, cut, color, carat)\n\nOur goal is to explore how the price of a diamond depends upon its clarity (a measure of quality).\nClarity is classified as follows, in order from best to worst:\n\n\n\nclarity\ndescription\n\n\n\n\nIF\nflawless (no internal imperfections)\n\n\nVVS1\nvery very slightly imperfect\n\n\nVVS2\n” ”\n\n\nVS1\nvery slightly imperfect\n\n\nVS2\n” ”\n\n\nSI1\nslightly imperfect\n\n\nSI2\n” ”\n\n\nI1\nimperfect\n\n\n\n\nCheck out a model of price by clarity. What clarity has the highest average price? The lowest? (This is surprising!)\n\n\ndiamond_model_1 &lt;- lm(price ~ clarity, data = diamonds)\n\n# Get a model summary\ncoef(summary(diamond_model_1))\n\n\nWhat confounding variable might explain these results? What’s your rationale?"
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-10-size",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-10-size",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 10: Size",
    "text": "Exercise 10: Size\nIt turns out that carat, the size of a diamond, is an important confounding variable.\nLet’s explore what happens when we control for this in our model:\n\ndiamond_model_2 &lt;- lm(price ~ clarity + carat, data = diamonds)\n\n# Get a model summary\ncoef(summary(diamond_model_2))\n\n# Plot the model\ndiamonds %&gt;% \n    ggplot(aes(y = price, x = carat, color = clarity)) + \n    geom_line(aes(y = diamond_model_2$fitted.values))\n\nWhat do you think now?\nWhich clarity has the highest expected price?\nThe lowest?\nProvide numerical evidence from the model."
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-11-simpsons-paradox",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-11-simpsons-paradox",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 11: Simpson’s Paradox",
    "text": "Exercise 11: Simpson’s Paradox\nControlling for carat didn’t just change the clarity coefficients, hence our understanding of the relationship between price and clarity… It flipped the signs of many of these coefficients.\nThis extreme scenario has a name: Simpson’s paradox.\nCHALLENGE: Explain why this happened and support your argument with graphical evidence.\nHINTS: Think about the causal diagram below. How do you think carat influences clarity? How do you think carat influences price? Make 2 ggplot() that support your answers."
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#exercise-12-final-conclusion",
    "href": "template_qmds/10-mlr-confounding-notes.html#exercise-12-final-conclusion",
    "title": "Confounding variables (Notes)",
    "section": "Exercise 12: Final conclusion",
    "text": "Exercise 12: Final conclusion\nWhat’s your final conclusion about diamond prices?\nWhich diamonds are more expensive: flawed ones or flawless ones?"
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#reflection",
    "href": "template_qmds/10-mlr-confounding-notes.html#reflection",
    "title": "Confounding variables (Notes)",
    "section": "Reflection",
    "text": "Reflection\nWrite a one-sentence warning label for what might happen if we do not control for confounding variables in our model.\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/10-mlr-confounding-notes.html#done",
    "href": "template_qmds/10-mlr-confounding-notes.html#done",
    "title": "Confounding variables (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/12-mlr-interaction-practice-notes.html",
    "href": "template_qmds/12-mlr-interaction-practice-notes.html",
    "title": "Multiple linear regression: interaction terms practice (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nVisualize interactions between categorical and quantitative predictors using scatterplots and side-by-side or boxplots\nCritically think through whether an interaction term makes sense, or should be included in a multiple linear regression model\nWrite a model formula for a multiple linear regression model with an interaction term between two quantitative predictors, two categorical predictors, or one quantitative and one categorical predictor\nInterpret the intercept and slope coefficients in a multiple linear regression model with an interaction term\n\n\n\n\nChoose either the reading or the videos to go through before class.\n\nReading: Section 3.9.3 in the STAT 155 Notes\nVideo:\n\nInteraction variables\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/12-mlr-interaction-practice-notes.html#learning-goals",
    "href": "template_qmds/12-mlr-interaction-practice-notes.html#learning-goals",
    "title": "Multiple linear regression: interaction terms practice (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nVisualize interactions between categorical and quantitative predictors using scatterplots and side-by-side or boxplots\nCritically think through whether an interaction term makes sense, or should be included in a multiple linear regression model\nWrite a model formula for a multiple linear regression model with an interaction term between two quantitative predictors, two categorical predictors, or one quantitative and one categorical predictor\nInterpret the intercept and slope coefficients in a multiple linear regression model with an interaction term"
  },
  {
    "objectID": "template_qmds/12-mlr-interaction-practice-notes.html#readings-and-videos",
    "href": "template_qmds/12-mlr-interaction-practice-notes.html#readings-and-videos",
    "title": "Multiple linear regression: interaction terms practice (Notes)",
    "section": "",
    "text": "Choose either the reading or the videos to go through before class.\n\nReading: Section 3.9.3 in the STAT 155 Notes\nVideo:\n\nInteraction variables\n\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/12-mlr-interaction-practice-notes.html#exercise-1-translating-scientific-questions-into-statistical-questions",
    "href": "template_qmds/12-mlr-interaction-practice-notes.html#exercise-1-translating-scientific-questions-into-statistical-questions",
    "title": "Multiple linear regression: interaction terms practice (Notes)",
    "section": "Exercise 1: Translating scientific questions into statistical questions",
    "text": "Exercise 1: Translating scientific questions into statistical questions\n\nLook at the variables we have access to in the cleaned version of the data we read into R, and consider our first research question. How might we translate this question into a statistical one, that we could answer using the data we have available?\n\nThere is no one right answer to this! Brainstorm with your group.\n\nhead(campaigns)\n## # A tibble: 6 × 5\n##   wholename         district             votes incumbent spending\n##   &lt;chr&gt;             &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;\n## 1 Aengus O Snodaigh Dublin South Central  5591 No          28.9  \n## 2 Aidan McMahon     Louth                  294 No           0.557\n## 3 Aidan Ryan        Limerick East           19 No           2.24 \n## 4 Aine Ni Chonaill  Dublin South Central   926 No           4.08 \n## 5 Alan Dukes        Kildare South         4967 Yes         12.1  \n## 6 Alan Shatter      Dublin South          5363 Yes         11.9\n\n\nQuestion 2 (a) is a bit more specific than Question 1. Translate this question into a statistical one that can be answered using a simple linear regression model. Write out the model statement in \\(E[Y | X] = ...\\) notation that would answer this question, and note which regression coefficient you would interpret to provide you with an answer.\n\n\\[\nE[___ | ___] = ...\n\\]\n\nQuestion 2 (b) is also specific, and builds on Question 2 (a). Translate this question into a statistical one that can be answered using a multiple linear regression model. Write out the model statement in \\(E[Y | X] = ...\\) notation that would answer this question, and note which regression coefficient you would interpret to provide you with an answer.\n\n\\[\nE[___ | ___] = ...\n\\]"
  },
  {
    "objectID": "template_qmds/12-mlr-interaction-practice-notes.html#exercise-2-visualizing-interaction",
    "href": "template_qmds/12-mlr-interaction-practice-notes.html#exercise-2-visualizing-interaction",
    "title": "Multiple linear regression: interaction terms practice (Notes)",
    "section": "Exercise 2: Visualizing Interaction",
    "text": "Exercise 2: Visualizing Interaction\n\nWrite R code to visualize the relationship between campaign spending and number of votes a candidate received. Include an aesthetic to distinguish this relationship between incumbents and challengers. Do not include lines of best fit from any statistical model on your plot at this point!\n\n\n# Visualization\n\n\nBased on your visualization from part (a), what are your answers to research questions 2 (a) and 2 (b)? Write your answer in 2-3 sentences, describing general trends you notice, suitable for a general audience.\nAdd lines of best fit from a statistical model that includes an interaction term between incumbent status and spending to your plot from part (a), using geom_smooth. Based on your updated plot, do you think including an interaction between incumbent status and spending in a multiple linear regression model would be meaningful in this context? Why or why not?\n\n\n# Visualization with lines of best fit"
  },
  {
    "objectID": "template_qmds/12-mlr-interaction-practice-notes.html#exercise-3-fitting-and-interpreting-models-with-interaction-terms",
    "href": "template_qmds/12-mlr-interaction-practice-notes.html#exercise-3-fitting-and-interpreting-models-with-interaction-terms",
    "title": "Multiple linear regression: interaction terms practice (Notes)",
    "section": "Exercise 3: Fitting and interpreting models with interaction terms",
    "text": "Exercise 3: Fitting and interpreting models with interaction terms\n\nFit the regression model you wrote out in Exercise 1 (c). Report (do not interpret yet!) the regression coefficients below.\n\n\n# Model with interaction term\n\n\n(Intercept):\n\n\nincumbentYes:\n\n\nspending:\n\n\nincumbentYes:spending:\n\n\nUsing the coefficient estimates from part (a), write out two separate model statements, one for incumbents and one for challengers. Combine terms (using algebra) when you can! Hint: remember the indicator variables video!\n\n\nFor incumbents:\n\n\\[\nE[votes | spending] =\n\\]\n\nFor challengers:\n\n\\[\nE[votes | spending] =\n\\]\n\nInterpret the coefficient for incumbent in your interaction model, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases. Is this coefficient scientifically meaningful?\nWhen interpreting an interaction coefficient where one of the variables interacting is quantitative and one is categorical, it is often convenient to do so in separate sentences: interpret the slope for each category separately!\n\nInterpret the coefficient for the interaction term in your model, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases.\n\nBased on your interpretation in part (d), and the visualization you made including lines of best fit, do you think that including an interaction term for incumbent status and spending is meaningful, when predicting number of votes? Explain why or why not."
  },
  {
    "objectID": "template_qmds/12-mlr-interaction-practice-notes.html#exercise-4-interactions-between-two-categorical-variables",
    "href": "template_qmds/12-mlr-interaction-practice-notes.html#exercise-4-interactions-between-two-categorical-variables",
    "title": "Multiple linear regression: interaction terms practice (Notes)",
    "section": "Exercise 4: Interactions between two categorical variables",
    "text": "Exercise 4: Interactions between two categorical variables\nLet’s return to our data on bike ridership. Suppose we are interested in the relationship between daily ridership (our response variable) and whether a user is a casual or registered rider and whether the day falls on a weekend. First, we need to create a binary variable indicating whether a user is a casual or registered rider.\n\n# Creating user variable, don't worry about syntax!\nnew_bikes &lt;- bikes %&gt;%\n  dplyr::select(riders_casual, riders_registered, weekend, temp_actual) %&gt;%\n  pivot_longer(cols = riders_casual:riders_registered, names_to = \"user\",\n               names_prefix = \"riders_\", values_to = \"rides\") %&gt;%\n  mutate(weekend = factor(weekend))\n\n\nFor each of our three relevant variables, weekend, user, and rides, classify them as quantitative or categorical.\n\n\nweekend:\n\n\nuser:\n\n\nrides:\n\n\nMake an appropriate visualization to explore the relationship between these three variables.\n\n\n# Visualization\n\n\nIs the relationship between ridership and weekend status the same for both registered and casual users? Explain why or why not, referencing the visualization you made in part (b).\nTo reflect what you observed in your visualization, fit a multiple linear regression model with an interaction term between weekend and user in our model of ridership.\n\n\n# Multiple linear regression model\n\n\nInterpret the interaction term from your model, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases. Just as in Exercise 3, you may find it useful to first write out multiple model statements for different categories defined by one of your categorical variables, and proceed from there!"
  },
  {
    "objectID": "template_qmds/12-mlr-interaction-practice-notes.html#exercise-5-interactions-between-two-quantitative-variables",
    "href": "template_qmds/12-mlr-interaction-practice-notes.html#exercise-5-interactions-between-two-quantitative-variables",
    "title": "Multiple linear regression: interaction terms practice (Notes)",
    "section": "Exercise 5: Interactions between two quantitative variables",
    "text": "Exercise 5: Interactions between two quantitative variables\nHere we’ll explore the relationship between price, milage, and age of a used car. Below is a scatterplot of mileage vs. price, colored by age:\n\ncars %&gt;% \n  ggplot(aes(x = milage, y = price, col = age)) +\n  geom_point(alpha = 0.5) + # make the points less opaque\n  scale_color_viridis_c(option = \"H\") + # a fun, colorblind-friendly palette!\n  theme_classic() # removes the gray background and grid\n\n\n\n\n\n\n\n\nIt’s a little difficult to tell what exactly is going on here. In particular, does the relationship between mileage and price vary with age of a used car? Let’s try adding some fitted lines for cars of different ages.\n\n# Ignore where the numbers in geom_abline() came from for now... we'll get there\ncars %&gt;% \n  ggplot(aes(x = milage, y = price, col = age)) +\n  geom_point(alpha = 0.5) + \n  scale_color_viridis_c(option = \"H\") + \n  theme_classic() +\n  geom_abline(slope = -6.558e-01 + 2.431e-02, intercept = 9.096e+04 -2.665e+03, col = \"black\") +\n  geom_abline(slope = -6.558e-01 + 10 * 2.431e-02, intercept = 9.096e+04 - 10 * 2.665e+03, col = \"blue\") +\n  geom_abline(slope = -6.558e-01 + 30 * 2.431e-02, intercept = 9.096e+04 - 30 * 2.665e+03, col = \"green\") +\n  ggtitle(\"Black: Age = 1yr, Blue: Age = 10yr, Green: Age = 30yr\")\n\n\n\n\n\n\n\n\n\nChallenge question: Based on the fitted lines in the plot above, anticipate what the signs (positive or negative) of the coefficients in the following interaction model will be:\n\n\\[\nE[price | age, milage] = \\beta_0 + \\beta_1 milage + \\beta_2 age + \\beta_3 milage:age\n\\] * \\(\\beta_0\\): Put your response here…\n\n\\(\\beta_1\\): Put your response here…\n\\(\\beta_2\\): Put your response here…\n\\(\\beta_3\\): Put your response here…\n\n\nFit a multiple linear regression model with an interaction term between milage and age in our model of used car price.\n\n\n# Multiple linear regression model\n\n\n\n# ... now do you see where the numbers in geom_abline() came from?\n\nAs before, we could choose distinct ages, and interpret the relationship between mileage and price for each of those groups separately. However, since age is quantitative and not categorical, this doesn’t quite give us the whole picture. Instead, we want to know how the relationship between mileage and price changes for each additional year old a car is. This is what the interaction coefficient estimates, when the interaction term is between two quantitative variables!\n\nInterpret the interaction term, in context. Make sure to use non-causal language, include units, and talk about averages rather than individual cases."
  },
  {
    "objectID": "template_qmds/12-mlr-interaction-practice-notes.html#reflection",
    "href": "template_qmds/12-mlr-interaction-practice-notes.html#reflection",
    "title": "Multiple linear regression: interaction terms practice (Notes)",
    "section": "Reflection",
    "text": "Reflection\nThrough the exercises above, you practiced visualizing, fitting, and interpreting multiple linear regression models with interaction terms between combinations of categorical and quantitative variables. Think about how the fitted lines looked in situations where you think there was a meaningful interaction taking place. How do you think the fitted lines would look if there was no meaningful interaction present? Explain your reasoning.\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/12-mlr-interaction-practice-notes.html#done",
    "href": "template_qmds/12-mlr-interaction-practice-notes.html#done",
    "title": "Multiple linear regression: interaction terms practice (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html",
    "href": "template_qmds/14-mlr-model-building-2-notes.html",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain when variables are redundant or multicollinear.\nRelate redundancy and multicollinearity to coefficient estimates and \\(R^2\\).\nExplain why adjusted \\(R^2\\) is preferable to multiple \\(R^2\\) when comparing models with different numbers of predictors.\n\n\n\n\nToday is a day to discover ideas, so no readings or videos to go through before class, but if you want to see today’s ideas presented in a different way, you can take a look at the following after class:\n\nReading: Section 3.9.5 in the STAT 155 Notes\nVideo: Redundancy and Multicollinearity\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html#learning-goals",
    "href": "template_qmds/14-mlr-model-building-2-notes.html#learning-goals",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain when variables are redundant or multicollinear.\nRelate redundancy and multicollinearity to coefficient estimates and \\(R^2\\).\nExplain why adjusted \\(R^2\\) is preferable to multiple \\(R^2\\) when comparing models with different numbers of predictors."
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html#readings-and-videos",
    "href": "template_qmds/14-mlr-model-building-2-notes.html#readings-and-videos",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "",
    "text": "Today is a day to discover ideas, so no readings or videos to go through before class, but if you want to see today’s ideas presented in a different way, you can take a look at the following after class:\n\nReading: Section 3.9.5 in the STAT 155 Notes\nVideo: Redundancy and Multicollinearity\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html#exercise-1-modeling-bill-length-by-flipper-length",
    "href": "template_qmds/14-mlr-model-building-2-notes.html#exercise-1-modeling-bill-length-by-flipper-length",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "Exercise 1: Modeling bill length by flipper length",
    "text": "Exercise 1: Modeling bill length by flipper length\nWhat can a penguin’s flipper (arm) length tell us about their bill length? To answer this question, we’ll consider 3 of our models:\n\n\n\nmodel\npredictors\n\n\n\n\npenguin_model_1\nflipper_length_mm\n\n\npenguin_model_2\nflipper_length_cm\n\n\npenguin_model_3\nflipper_length_mm + flipper_length_cm\n\n\n\nPlots of the first two models are below:\n\nggplot(penguins, aes(y = bill_length_mm, x = flipper_length_mm)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error:\n## ! object 'flipper_length_mm' not found\n\nggplot(penguins, aes(y = bill_length_mm, x = flipper_length_cm)) + \n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE)\n## Error in `geom_point()`:\n## ! Problem while computing aesthetics.\n## ℹ Error occurred in the 1st layer.\n## Caused by error:\n## ! object 'flipper_length_cm' not found\n\n\nBefore examining the model summaries, check your intuition. Do you think the penguin_model_2 R-squared will be less than, equal to, or more than that of penguin_model_1? Similarly, how do you think the penguin_model_3 R-squared will compare to that of penguin_model_1?\nCheck your intuition: Examine the R-squared values for the three penguin models and summarize how these compare.\n\n\nsummary(penguin_model_1)$r.squared\n## Error: object 'penguin_model_1' not found\nsummary(penguin_model_2)$r.squared\n## Error: object 'penguin_model_2' not found\nsummary(penguin_model_3)$r.squared\n## Error: object 'penguin_model_3' not found\n\n\nExplain why your observation in part b makes sense. Support your reasoning with a plot of just the 2 predictors: flipper_length_mm vs flipper_length_cm.\n\n\nOPTIONAL challenge: In summary(penguin_model_3), the flipper_length_cm coefficient is NA. Explain why this makes sense. HINT: Thinking about what you learned about controlling for covariates, why wouldn’t it make sense to interpret this coefficient? BONUS: For those of you that have taken MATH 236, this has to do with matrices that are not of full rank!"
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html#exercise-2-incorporating-body_mass_g",
    "href": "template_qmds/14-mlr-model-building-2-notes.html#exercise-2-incorporating-body_mass_g",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "Exercise 2: Incorporating body_mass_g",
    "text": "Exercise 2: Incorporating body_mass_g\nIn this exercise you’ll consider 3 models of bill_length_mm:\n\n\n\nmodel\npredictors\n\n\n\n\npenguin_model_1\nflipper_length_mm\n\n\npenguin_model_4\nbody_mass_g\n\n\npenguin_model_5\nflipper_length_mm + body_mass_g\n\n\n\n\nWhich is the better predictor of bill_length_mm: flipper_length_mm or body_mass_g? Provide some numerical evidence.\npenguin_model_5 incorporates both flipper_length_mm and body_mass_g as predictors. Before examining a model summary, ask your gut: Will the penguin_model_5 R-squared be close to 0.35, close to 0.43, or greater than 0.6?\nCheck your intuition. Report the penguin_model_5 R-squared and summarize how this compares to that of penguin_model_1 and penguin_model_4.\nExplain why your observation in part c makes sense. Support your reasoning with a plot of the 2 predictors: flipper_length_mm vs body_mass_g."
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html#exercise-3-redundancy-and-multicollinearity",
    "href": "template_qmds/14-mlr-model-building-2-notes.html#exercise-3-redundancy-and-multicollinearity",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "Exercise 3: Redundancy and Multicollinearity",
    "text": "Exercise 3: Redundancy and Multicollinearity\nThe exercises above have illustrated special phenomena in multivariate modeling:\n\ntwo predictors are redundant if they contain the same exact information\ntwo predictors are multicollinear if they are strongly associated (they contain very similar information) but are not completely redundant.\n\nRecall that we examined 5 models:\n\n\n\nmodel\npredictors\n\n\n\n\npenguin_model_1\nflipper_length_mm\n\n\npenguin_model_2\nflipper_length_cm\n\n\npenguin_model_3\nflipper_length_mm + flipper_length_cm\n\n\npenguin_model_4\nbody_mass_g\n\n\npenguin_model_5\nflipper_length_mm + body_mass_g\n\n\n\n\nWhich model had redundant predictors and which predictors were these?\nWhich model had multicollinear predictors and which predictors were these?\nIn general, what happens to the R-squared value if we add a redundant predictor to a model: will it decrease, stay the same, increase by a small amount, or increase by a significant amount?\nSimilarly, what happens to the R-squared value if we add a multicollinear predictor to a model: will it decrease, stay the same, increase by a small amount, or increase by a significant amount?"
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html#exercise-4-considerations-for-strong-models",
    "href": "template_qmds/14-mlr-model-building-2-notes.html#exercise-4-considerations-for-strong-models",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "Exercise 4: Considerations for strong models",
    "text": "Exercise 4: Considerations for strong models\nLet’s dive deeper into important considerations when building a strong model. We’ll use a subset of the penguins data for exploring these ideas.\n\n# For illustration purposes only, take a sample of 10 penguins.\n# We'll discuss this code later in the course!\nset.seed(155)\npenguins_small &lt;- sample_n(penguins, size = 10) %&gt;%\n  mutate(flipper_length_mm = jitter(flipper_length_mm))\n## Error in `mutate()`:\n## ℹ In argument: `flipper_length_mm = jitter(flipper_length_mm)`.\n## Caused by error:\n## ! object 'flipper_length_mm' not found\n\nConsider 3 models of bill length:\n\n# A model with one predictor (flipper_length_mm)\npoly_mod_1 &lt;- lm(bill_length_mm ~ flipper_length_mm, penguins_small)\n## Error in eval(mf, parent.frame()): object 'penguins_small' not found\n\n# A model with two predictors (flipper_length_mm and flipper_length_mm^2)\npoly_mod_2 &lt;- lm(bill_length_mm ~ poly(flipper_length_mm, 2), penguins_small)\n## Error in eval(mf, parent.frame()): object 'penguins_small' not found\n\n# A model with nine predictors (flipper_length_mm, flipper_length_mm^2, ... on up to flipper_length_mm^9)\npoly_mod_9 &lt;- lm(bill_length_mm ~ poly(flipper_length_mm, 9), penguins_small)\n## Error in eval(mf, parent.frame()): object 'penguins_small' not found\n\n\nBefore doing any analysis, which of the three models do you think will be best?\nCalculate the R-squared values of these 3 models. Which model do you think is best?\n\n\nsummary(poly_mod_1)$r.squared\n## Error: object 'poly_mod_1' not found\nsummary(poly_mod_2)$r.squared\n## Error: object 'poly_mod_2' not found\nsummary(poly_mod_9)$r.squared\n## Error: object 'poly_mod_9' not found\n\n\nCheck out plots depicting the relationship estimated by these 3 models. Which model do you think is best?\n\n\n# A plot of model 1\nggplot(penguins_small, aes(y = bill_length_mm, x = flipper_length_mm)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", se = FALSE)\n## Error: object 'penguins_small' not found\n\n\n# A plot of model 2\nggplot(penguins_small, aes(y = bill_length_mm, x = flipper_length_mm)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = FALSE)\n## Error: object 'penguins_small' not found\n\n\n# A plot of model 9\nggplot(penguins_small, aes(y = bill_length_mm, x = flipper_length_mm)) + \n    geom_point() + \n    geom_smooth(method = \"lm\", formula = y ~ poly(x, 9), se = FALSE)\n## Error: object 'penguins_small' not found"
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html#exercise-5-reflecting-on-these-investigations",
    "href": "template_qmds/14-mlr-model-building-2-notes.html#exercise-5-reflecting-on-these-investigations",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "Exercise 5: Reflecting on these investigations",
    "text": "Exercise 5: Reflecting on these investigations\n\nList 3 of your favorite foods. Now imagine making a dish that combines all of these foods. Do you think it would taste good?\nToo many good things doesn’t make necessarily make a better thing. Model 9 demonstrates that it’s always possible to get a perfect R-squared of 1, but there are drawbacks to putting more and more predictors into our model. Answer the following about model 9:\n\nHow easy would it be to interpret this model?\nWould you say that this model captures the general trend of the relationship between bill_length_mm and flipper_length_mm?\nHow well do you think this model would generalize to penguins that were not included in the penguins_small sample? For example, would you expect these new penguins to fall on the wiggly model 9 curve?"
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html#exercise-6-overfitting",
    "href": "template_qmds/14-mlr-model-building-2-notes.html#exercise-6-overfitting",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "Exercise 6: Overfitting",
    "text": "Exercise 6: Overfitting\nModel 9 provides an example of a model that is overfit to our sample data. That is, it picks up the tiny details of our data at the cost of losing the more general trends of the relationship of interest. Check out the following xkcd comic. Which plot pokes fun at overfitting?\n\nSome other goodies:"
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html#exercise-7-questioning-r-squared",
    "href": "template_qmds/14-mlr-model-building-2-notes.html#exercise-7-questioning-r-squared",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "Exercise 7: Questioning R-squared",
    "text": "Exercise 7: Questioning R-squared\nZooming out, explain some limitations of relying on R-squared to measure the strength / usefulness of a model."
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html#exercise-8-adjusted-r-squared",
    "href": "template_qmds/14-mlr-model-building-2-notes.html#exercise-8-adjusted-r-squared",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "Exercise 8: Adjusted R-squared",
    "text": "Exercise 8: Adjusted R-squared\nWe’ve seen that, unless a predictor is redundant with another, R-squared will increase. Even if that predictor is strongly multicollinear with another. Even if that predictor isn’t a good predictor! Thus if we only look at R-squared we might get overly greedy. We can check our greedy impulses a few ways. We take a more in depth approach in STAT 253, but one quick alternative is reported right in our model summary() tables. Adjusted R-squared includes a penalty for incorporating more and more predictors. Mathematically (where \\(n\\) is the sample size and \\(p\\) is the number of non-intercept coefficients):\n\\[\n\\text{Adjusted } R^2 = 1 - (1 - R^2) \\left( \\frac{n-1}{n-p-1} \\right)\n\\]\nThus unlike R-squared, Adjusted R-squared can decrease when the information that a predictor contributes to a model isn’t enough to offset the complexity it adds to that model. Consider two models:\n\nexample_1 &lt;- lm(bill_length_mm ~ species, penguins)\n## Error in eval(predvars, data, env): object 'bill_length_mm' not found\nexample_2 &lt;- lm(bill_length_mm ~ species + island, penguins)\n## Error in eval(predvars, data, env): object 'bill_length_mm' not found\n\n\nCheck out the summaries for the 2 example models. In general, how does a model’s Adjusted R-squared compare to the R-squared? Is it greater, less than, or equal to the R-squared?\nHow did the R-squared change from example model 1 to model 2? How did the Adjusted R-squared change?\nExplain what it is about island that resulted in a decreased Adjusted R-squared. Note: it’s not necessarily the case that island is a bad predictor on its own!"
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html#reflection",
    "href": "template_qmds/14-mlr-model-building-2-notes.html#reflection",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "Reflection",
    "text": "Reflection\nToday we looked at some cautions surrounding indiscriminately adding variables to a model. Summarize key takeaways.\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/14-mlr-model-building-2-notes.html#done",
    "href": "template_qmds/14-mlr-model-building-2-notes.html#done",
    "title": "Multiple linear regression: model building (part 2) (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/16-logistic-univariate-notes.html",
    "href": "template_qmds/16-logistic-univariate-notes.html",
    "title": "Simple logistic regression (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain the differences between linear regression and logistic regression for modeling binary outcomes\nConstruct simple logistic regression models in R\nInterpret coefficients in simple logistic regression models\nUse simple logistic regression models to make predictions\nDescribe the form (shape) of relationships on the log odds, odds, and probability scales\n\n\n\n\nChoose either the reading or the videos to go through before class.\n\nReading: Sections 4.1-4.3 in the STAT 155 Notes\nVideo: Logistic regression (slides)\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/16-logistic-univariate-notes.html#learning-goals",
    "href": "template_qmds/16-logistic-univariate-notes.html#learning-goals",
    "title": "Simple logistic regression (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain the differences between linear regression and logistic regression for modeling binary outcomes\nConstruct simple logistic regression models in R\nInterpret coefficients in simple logistic regression models\nUse simple logistic regression models to make predictions\nDescribe the form (shape) of relationships on the log odds, odds, and probability scales"
  },
  {
    "objectID": "template_qmds/16-logistic-univariate-notes.html#readings-and-videos",
    "href": "template_qmds/16-logistic-univariate-notes.html#readings-and-videos",
    "title": "Simple logistic regression (Notes)",
    "section": "",
    "text": "Choose either the reading or the videos to go through before class.\n\nReading: Sections 4.1-4.3 in the STAT 155 Notes\nVideo: Logistic regression (slides)\n\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder."
  },
  {
    "objectID": "template_qmds/16-logistic-univariate-notes.html#exercise-1-exploring-age",
    "href": "template_qmds/16-logistic-univariate-notes.html#exercise-1-exploring-age",
    "title": "Simple logistic regression (Notes)",
    "section": "Exercise 1: Exploring age",
    "text": "Exercise 1: Exploring age\nDid younger passengers tend to have higher survival rates than older passengers?\nVisualizing the relationship between a binary response and a quantitative predictor can be tricky. We will take a few approaches here.\n\nCreate a boxplot where one box corresponds to the age distribution of survivors and the second to that of non-survivors.\nCreate density plots with separate colors for the survivors and non-survivors.\nThe remainder of the code below creates a plot of the fraction who survived at each age. (Since we have a large data set and multiple (though sometimes not many) observations at most ages, we can manually calculate the survival fraction.\n\nAfter inspecting the plots, summarize what you learn.\n\n# Create a boxplot\n# Note that you'll need to force R to view Survived as a binary categorical variable by using x = factor(Survived) instead of just x = Survived in the aes() part of your plot\n\n\n# Create a density plot (you'll need to use factor(Survived) again)\n\n\n# Use the code below to create a plot of the fraction who survived at each age\ntitanic_summ &lt;- titanic %&gt;% \n    group_by(Age) %&gt;%\n    summarize(frac_survived = mean(Survived))\n\nggplot(titanic_summ, aes(x = Age, y = frac_survived)) +\n    geom_point() +\n    geom_smooth(se = FALSE)"
  },
  {
    "objectID": "template_qmds/16-logistic-univariate-notes.html#exercise-2-exploring-sex-and-ticket-class",
    "href": "template_qmds/16-logistic-univariate-notes.html#exercise-2-exploring-sex-and-ticket-class",
    "title": "Simple logistic regression (Notes)",
    "section": "Exercise 2: Exploring sex and ticket class",
    "text": "Exercise 2: Exploring sex and ticket class\nWere males or females more likely to survive? Did 1st class passengers tend to survive more than 2nd and 3rd class passengers?\nThe code below creates plots that allow us to explore how Sex and PClass relate to survival. The first two plots are standard bar plots that use color to indicate what fraction of each group survived. The last two plots are mosaic plots that are much like the standard bar plots, but the width of the bars reflects the distribution of the x-axis variable. (The widest bar is the most prevalent category.)\nSummarize what you learn about the relationship between sex, ticket class, and survival.\n\n# Standard bar plots\nggplot(titanic, aes(x = Sex, fill = factor(Survived))) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\nggplot(titanic, aes(x = PClass, fill = factor(Survived))) +\n    geom_bar(position = \"fill\")\n\n\n\n\n\n\n\n\n# Mosaic plots\nggplot(data = titanic %&gt;% mutate(Survived = as.factor(Survived))) +\n    geom_mosaic(aes(x = product(Sex), fill = Survived))\n\n\n\n\n\n\n\n\nggplot(data = titanic %&gt;% mutate(Survived = as.factor(Survived))) +\n    geom_mosaic(aes(x = product(PClass), fill = Survived))"
  },
  {
    "objectID": "template_qmds/16-logistic-univariate-notes.html#exercise-3-linear-regression-model",
    "href": "template_qmds/16-logistic-univariate-notes.html#exercise-3-linear-regression-model",
    "title": "Simple logistic regression (Notes)",
    "section": "Exercise 3: Linear regression model",
    "text": "Exercise 3: Linear regression model\nFor now we will focus on exploring the relationship between (ticket) class and survival.\nLet’s tabulate survival across classes. We can tabulate across two variables by providing both variables to count():\n\ntitanic %&gt;% \n    count(PClass, Survived)\n## # A tibble: 7 × 3\n##   PClass Survived     n\n##   &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt;\n## 1 1st           0   129\n## 2 1st           1   193\n## 3 2nd           0   160\n## 4 2nd           1   119\n## 5 3rd           0   573\n## 6 3rd           1   138\n## 7 &lt;NA&gt;          0     1\n\n\nUse the count() output to fill in the following contingency table:\n\n\n\n\nClass\nDied\nSurvived\nTotal\n\n\n\n\n1st Class\n___\n___\n___\n\n\n2nd Class\n___\n___\n___\n\n\n3rd Class\n___\n___\n___\n\n\nTotal\n___\n___\n___\n\n\n\n\nUsing your table, estimate the following:\n\nthe probability of surviving among 1st class passengers\nthe probability of surviving among 2nd class passengers\nthe probability of surviving among 3rd class passengers\nthe difference in the probability of surviving, comparing 2nd class passengers to 1st class passengers (i.e., how much lower is the probability of 2nd class passengers as compared to 1st class passengers?)\nthe difference in the probability of surviving, comparing 3rd class passengers to 1st class passengers (i.e., how much lower is the probability of 3rd class passengers as compared to 1st class passengers?)\n\nAfter fitting the linear regression model below, write out the model formula using correct notation. Explain carefully what it means to talk about the expected/average value of a binary variable.\n\n\nlin_mod &lt;- lm(Survived ~ PClass, data = titanic)\nsummary(lin_mod)\n## \n## Call:\n## lm(formula = Survived ~ PClass, data = titanic)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.5994 -0.1941 -0.1941  0.4006  0.8059 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.59938    0.02468  24.284  &lt; 2e-16 ***\n## PClass2nd   -0.17286    0.03623  -4.772 2.03e-06 ***\n## PClass3rd   -0.40529    0.02975 -13.623  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.4429 on 1309 degrees of freedom\n##   (1 observation deleted due to missingness)\n## Multiple R-squared:  0.1315, Adjusted R-squared:  0.1302 \n## F-statistic: 99.09 on 2 and 1309 DF,  p-value: &lt; 2.2e-16\n\n\nWrite an interpretation of each of the coefficients in your linear regression model. How do your coefficient estimates compare to your answers in part b?"
  },
  {
    "objectID": "template_qmds/16-logistic-univariate-notes.html#exercise-4-logistic-regression-model-categorical-predictor",
    "href": "template_qmds/16-logistic-univariate-notes.html#exercise-4-logistic-regression-model-categorical-predictor",
    "title": "Simple logistic regression (Notes)",
    "section": "Exercise 4: Logistic regression model (categorical predictor)",
    "text": "Exercise 4: Logistic regression model (categorical predictor)\n\nRefer back to your contingency table from Exercise 3a. Using your table, estimate the following:\n\nthe odds of surviving among 1st class passengers\nthe odds of surviving among 2nd class passengers\nthe odds of surviving among 3rd class passengers\nthe ratio of the odds of surviving, comparing 2nd class passengers to 1st class passengers (i.e., how many times higher/lower is the odds of survival among 2nd class passengers as compared to 1st class passengers?)\nthe ratio of the odds of surviving, comparing 3rd class passengers to 1st class passengers\n\nAfter fitting the logistic regression model below, write out the model formula using correct notation.\n\n\nlog_mod &lt;- glm(Survived ~ PClass, data = titanic, family = \"binomial\")\ncoef(summary(log_mod))\n##               Estimate Std. Error    z value     Pr(&gt;|z|)\n## (Intercept)  0.4028778  0.1137246   3.542574 3.962427e-04\n## PClass2nd   -0.6989281  0.1660923  -4.208071 2.575600e-05\n## PClass3rd   -1.8265098  0.1480705 -12.335410 5.839072e-35\n\n\nWrite an interpretation of each of the exponentiated coefficients in your logistic regression model. Think carefully about what we are modeling when we fit a logistic regression model. How do these exponentiated coefficient estimates compare to your answers in part a?"
  },
  {
    "objectID": "template_qmds/16-logistic-univariate-notes.html#exercise-5-logistic-regression-model-quantitative-predictor",
    "href": "template_qmds/16-logistic-univariate-notes.html#exercise-5-logistic-regression-model-quantitative-predictor",
    "title": "Simple logistic regression (Notes)",
    "section": "Exercise 5: Logistic regression model (quantitative predictor)",
    "text": "Exercise 5: Logistic regression model (quantitative predictor)\nNow we will explore how to interpret a quantitative predictor in a logistic regression model.\n\nAfter fitting the logistic regression model below, write out the model formula using correct notation.\n\n\nlog_mod &lt;- glm(Survived ~ Age, data = titanic, family = \"binomial\")\ncoef(summary(log_mod))\n##                Estimate Std. Error    z value   Pr(&gt;|z|)\n## (Intercept) -0.08142783 0.17386170 -0.4683483 0.63953556\n## Age         -0.00879462 0.00523158 -1.6810637 0.09275054\n\n\nWrite an interpretation of each of the exponentiated coefficients in this logistic regression model."
  },
  {
    "objectID": "template_qmds/16-logistic-univariate-notes.html#exercise-6-linear-vs.-logistic-modeling",
    "href": "template_qmds/16-logistic-univariate-notes.html#exercise-6-linear-vs.-logistic-modeling",
    "title": "Simple logistic regression (Notes)",
    "section": "Exercise 6: Linear vs. logistic modeling",
    "text": "Exercise 6: Linear vs. logistic modeling\nTo highlight a key difference between linear vs. logistic modeling, consider the following linear and logistic regression models of survival with sex and age as predictors in addition to ticket class.\n\nlin_mod2 &lt;- lm(Survived ~ PClass + Sex + Age, data = titanic)\ncoef(summary(lin_mod2))\n##                 Estimate  Std. Error    t value     Pr(&gt;|t|)\n## (Intercept)  1.130522829 0.051940872  21.765573 8.158449e-82\n## PClass2nd   -0.207433817 0.039239825  -5.286308 1.637737e-07\n## PClass3rd   -0.393344488 0.037709874 -10.430809 7.001373e-24\n## Sexmale     -0.501325667 0.029419802 -17.040416 2.697807e-55\n## Age         -0.006004789 0.001105949  -5.429536 7.633977e-08\n\nlog_mod2 &lt;- glm(Survived ~ PClass + Sex + Age, data = titanic, family = \"binomial\")\ncoef(summary(log_mod2))\n##                Estimate  Std. Error    z value     Pr(&gt;|z|)\n## (Intercept)  3.75966210 0.397567324   9.456668 3.179129e-21\n## PClass2nd   -1.29196240 0.260075781  -4.967638 6.777324e-07\n## PClass3rd   -2.52141915 0.276656805  -9.113888 7.948131e-20\n## Sexmale     -2.63135683 0.201505379 -13.058494 5.684093e-39\n## Age         -0.03917681 0.007616218  -5.143868 2.691392e-07\n\n\nUse the linear regression model to predict the probability of survival for Rose (a 17 year old female in 1st class) and Jack (a 20 year old male in 3rd class). Show your work.\nNow use the logistic regression model to predict the survival probability for Rose and Jack. Show your work. (Hint: use the logistic regression model to obtain the predicted log odds, exponentiate to get the odds, and then convert to probability.)\nComment on differences that you notice in the predictions from parts a and b."
  },
  {
    "objectID": "template_qmds/16-logistic-univariate-notes.html#reflection",
    "href": "template_qmds/16-logistic-univariate-notes.html#reflection",
    "title": "Simple logistic regression (Notes)",
    "section": "Reflection",
    "text": "Reflection\nWhat binary outcomes might be relevant in your project? What predictor(s) could be relevant in a logistic regression model for that outcome?\n\nResponse: Put your response here."
  },
  {
    "objectID": "template_qmds/16-logistic-univariate-notes.html#done",
    "href": "template_qmds/16-logistic-univariate-notes.html#done",
    "title": "Simple logistic regression (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html",
    "href": "template_qmds/18-sampling-normal-notes.html",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "",
    "text": "Recognize the difference between a population parameter and a sample estimate.\nReview the Normal probability model, a tool we’ll need to turn information in our sample data into inferences about the broader population.\nExplore the ideas of randomness, sampling distributions, and standard error through a class experiment. (We’ll define these more formally in the next class.)\n\n\n\n\nPlease do the following videos and reading before class.\n\nReading: Section 6 Introduction, and Section 6.6 in the STAT 155 Notes\nVideo 1: exploration vs inference\nVideo 2: Normal probability model"
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html#learning-goals",
    "href": "template_qmds/18-sampling-normal-notes.html#learning-goals",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "",
    "text": "Recognize the difference between a population parameter and a sample estimate.\nReview the Normal probability model, a tool we’ll need to turn information in our sample data into inferences about the broader population.\nExplore the ideas of randomness, sampling distributions, and standard error through a class experiment. (We’ll define these more formally in the next class.)"
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html#readings-and-videos",
    "href": "template_qmds/18-sampling-normal-notes.html#readings-and-videos",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "",
    "text": "Please do the following videos and reading before class.\n\nReading: Section 6 Introduction, and Section 6.6 in the STAT 155 Notes\nVideo 1: exploration vs inference\nVideo 2: Normal probability model"
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html#exercise-1-using-the-normal-model",
    "href": "template_qmds/18-sampling-normal-notes.html#exercise-1-using-the-normal-model",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "Exercise 1: Using the Normal model",
    "text": "Exercise 1: Using the Normal model\nSuppose that the speeds of cars on a highway, in miles per hour, can be reasonably represented by the Normal model with a mean of 55mph and a standard deviation of 5mph from car to car:\n\\[\nX \\sim N(55, 5^2)\n\\]\n\nshaded_normal(mean = 55, sd = 5)\n\n\n\n\n\n\n\n\n\nProvide the (approximate) range of the middle 68% of speeds, and shade in the corresponding region on your Normal curve. NOTE: a is the lower end of the range and b is the upper end.\n\n\nshaded_normal(mean = 55, sd = 5, a = ___, b = ___)\n## Error in parse(text = input): &lt;text&gt;:1:39: unexpected input\n## 1: shaded_normal(mean = 55, sd = 5, a = __\n##                                           ^\n\n\nUse the 68-95-99.7 rule to estimate the probability that a car’s speed exceeds 60mph.\n\n\nYour response here\n\n\n# Visualize\nshaded_normal(mean = 55, sd = 5, a = 60)\n\n\n\n\n\n\n\n\n\nWhich of the following is the correct range for the probability that a car’s speed exceeds 67mph? Explain your reasoning.\n\n\nless than 0.0015\nbetween 0.0015 and 0.025\nbetween 0.025 and 0.16\ngreater than 0.16\n\n\nExplain your reasoning here\n\n\n# Visualize\nshaded_normal(mean = 55, sd = 5, a = 67)"
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html#exercise-2-z-scores",
    "href": "template_qmds/18-sampling-normal-notes.html#exercise-2-z-scores",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "Exercise 2: Z-scores",
    "text": "Exercise 2: Z-scores\nInherently important to all of our calculations above is how many standard deviations a value “X” is from the mean.\nThis distance is called a Z-score and can be calculated as follows:\n\\[\n\\text{Z-score} = \\frac{X - \\text{mean}}{\\text{sd}}\n\\]\nFor example (from Exercise 1), if I’m traveling 40 miles an hour, my Z-score is -3. That is, my speed is 3 standard deviations below the average speed:\n\n(40 - 55) / 5\n## [1] -3\n\n\nConsider 2 other drivers. Both drivers are speeding. Who do you think is speeding more, relative to the distributions of speed in their area?\n\nDriver A is traveling at 60mph on the highway where speeds are N(55, 5^2) and the speed limit is 55mph.\nDriver B is traveling at 36mph on a residential road where speeds are N(30, 3^2) and the speed limit is 30mph.\n\n\n\nPut your best guess (hypothesis) here\n\n\nCalculate the Z-scores for Drivers A and B.\n\n\n# Driver A\n\n\n# Driver B\n\n\nNow, based on the Z-scores, who is speeding more? NOTE: The below plots might provide some insights.\n\n\n# Driver A\nshaded_normal(mean = 55, sd = 5) + \n  geom_vline(xintercept = 60)\n\n\n\n\n\n\n\n\n# Driver B\nshaded_normal(mean = 30, sd = 3) + \n  geom_vline(xintercept = 36)  \n\n\n\n\n\n\n\n\n\nYour response here"
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html#exercise-3-parameter-vs-estimate",
    "href": "template_qmds/18-sampling-normal-notes.html#exercise-3-parameter-vs-estimate",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "Exercise 3: Parameter vs estimate",
    "text": "Exercise 3: Parameter vs estimate\nIt’s important to note that this dataset is by no means a comprehensive collection of films and their review scores–it does not contain every film that was released from 2014-2015, nor films released outside of that date range. The review scores are also frozen in time–all of these films have almost certainly accumulated additional reviews since the data were first collected.\nHowever, our stated goal is to make inferences about the overarching relationship between critic reviews and user reviews for all films (relatedly, we may want to use our model to make predictions about how user reviews are affected by critic reviews for films that may not even exist yet!). Can we actually make these inferences/predictions about a potentially infinite collection of films when all we have is a fairly limited subset of these?\n\nPopulations and Samples\nThis question points to two of the most important concepts in the field of statistical inference: populations and samples. Statisticians have many different ways of defining a population (depending on the questions they are asking), but for the purposes of this exercise, we can think of the population as the set of all possible films and all possible review scores that have been or could be catalogued on RottenTomatoes.\nOur dataset of 146 films is considered a sample of this population. A sample is simply a subset of observations taken from that population.\n\n\n\n\n\n\nSampling Criteria\n\n\n\n\n\nWhen we take a sample of data from a population, there is always some set of criteria used to determine how a sample is taken. This could be as simple as “we randomly selected 1% of all films catalogued on RottenTomatoes as of 4/1/2025”, or a more complex set of specific criteria (for this dataset, the sample was taken by selecting all films that had tickets for sale on Fandango on 8/24/2015, then further filtering to include films that have a Rotten Tomatoes rating, a RT User rating, a Metacritic score, a Metacritic User score, an IMDb score, and at least 30 fan reviews on Fandango.)\n\n\n\n\n\n“True” parameters versus estimates\nIn order to conduct statistical inference using linear regression, we must assume that there is some true, underlying, fixed intercept and slope \\(\\beta_0\\) and \\(\\beta_1\\), that describe the true linear relationship in the overall population that we’re interested in.\nIf we are modeling the relationship between UserScore and CriticScore on RottenTomatoes, The “true” underlying model we assume is thus:\n\\[\nUserScore_i = \\beta_0 + \\beta_1 CriticScore_i + e_i\n\\]\nHowever, the “true” values of \\(\\beta_0\\) and \\(\\beta_1\\) are typically impossible to know, because knowing them requires access to our entire population of interest (in this case, the review scores for every film that has been or will be released). When we fit a regression model using the sample that we do have, we are actually obtaining estimates of those true population parameters (note the notation change of putting a \\(\\hat{ }\\) on top of the Betas, to indicate that this is an estimate):\n\\[\nE[UserScore \\mid CriticScore] = \\hat{\\beta}_0 + \\hat{\\beta}_1 CriticScore\n\\]\nwhere our estimates are given by our model as \\(\\hat{\\beta}_0 = 32.3%\\), \\(\\hat{\\beta}_1 = 0.52%\\)\nFor the sake of this activity, let’s assume that these estimates are identical to the true population parameters.\n🚩🚩🚩 HOWEVER, be very careful not to make this assumption in other models you encounter.For this dataset, recall the specific sampling criteria that were used, which means these 146 films likely aren’t representative of the full population of films we’re interested in. This means that the estimates we obtained probably don’t match the true population parameters–they may or may not be close, but we don’t know for certain! 🚩🚩🚩\nBelow, we’ll simulate how parameter estimates are impacted by taking different samples. You’ll each take a random sample of 10 films in the dataset, and we’ll see if we can recover the presumed population parameters (i.e., the coefficient estimates we obtained from our model using all 146 films that were initially sampled).\nFirst, fill in your intuition below:\n\nDo you think every student will get the same set of 10 films?\n\n\nYour response here\n\n\nDo you think that your coefficient estimates will be the same as your neighbors’?\n\n\nYour responses here"
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html#exercise-4-random-sampling",
    "href": "template_qmds/18-sampling-normal-notes.html#exercise-4-random-sampling",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "Exercise 4: Random sampling",
    "text": "Exercise 4: Random sampling\n\nUse the sample_n() function to take a random sample of 2 films\n\n\n# Try running the following chunk A FEW TIMES\nsample_n(fandango, size = 2, replace = FALSE)\n## Error in sample_n(fandango, size = 2, replace = FALSE): could not find function \"sample_n\"\n\nReflect:\n\nHow do your results compare to your neighbors’?\n\n\nYour response here\n\n\nWhat is the role of size = 2? HINT: Remember you can look at function documentation by running ?sample_n in the console!\n\n\nYour response here\n\n\nWhat is the role of replace = FALSE? HINT: Remember you can look at function documentation by running ?sample_n in the console!\n\n\nYour response here\n\n\nNow, “set the seed” to 155 and re-try your sampling.\n\n\n# Try running the following FULL chunk A FEW TIMES\nset.seed(155)\nsample_n(fandango, size = 2, replace = FALSE)\n## Error in sample_n(fandango, size = 2, replace = FALSE): could not find function \"sample_n\"\n\nWhat changed?\n\nYour response here"
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html#exercise-5-take-your-own-sample",
    "href": "template_qmds/18-sampling-normal-notes.html#exercise-5-take-your-own-sample",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "Exercise 5: Take your own sample",
    "text": "Exercise 5: Take your own sample\nThe underlying random number generator plays a role in the random sample we happen to get. If we set.seed(some positive integer) before taking a random sample, we’ll get the same results.\nThis reproducibility is important:\n\nwe get the same results every time we render our qmd\nwe can share our work with others & ensure they get our same answers\nit wouldn’t be great if you submitted your work to, say, a journal and weren’t able to back up / confirm / reproduce your results!\n\nFollow the chunks below to obtain and use your own unique sample.\n\n# DON'T SKIP THIS STEP! \n# Set the random number seed to the digits of your own phone number (just the numbers)\nset.seed()\n## Error in set.seed(): argument \"seed\" is missing, with no default\n\n# Take a sample of 10 films\nmy_sample &lt;- sample_n(fandango, size = 10, replace = FALSE)\n## Error in sample_n(fandango, size = 10, replace = FALSE): could not find function \"sample_n\"\nmy_sample                       \n## Error: object 'my_sample' not found\n\n\n# Plot the relationship of UserScore with CriticScore among your sample\nmy_sample %&gt;% \n  ggplot(aes(y = userscore_rt, x = criticscore_rt)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n## Error in my_sample %&gt;% ggplot(aes(y = userscore_rt, x = criticscore_rt)): could not find function \"%&gt;%\"\n\n\n# Model the relationship among your sample\nmy_model &lt;- lm(userscore_rt ~ criticscore_rt, data = my_sample)\n## Error in eval(mf, parent.frame()): object 'my_sample' not found\ncoef(summary(my_model))[,1]\n## Error: object 'my_model' not found\n\nREPORT YOUR WORK\nLog your intercept and slope sample estimates in this survey."
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html#exercise-6-sampling-variation",
    "href": "template_qmds/18-sampling-normal-notes.html#exercise-6-sampling-variation",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "Exercise 6: Sampling variation",
    "text": "Exercise 6: Sampling variation\nRecall that we are assuming the population parameters are equal to the estimates we obtained from the model we fit using the initial sample of 146 films:\n\\[\nE[UserScore \\mid CriticScore] = 32.3 + 0.52 CriticScore\n\\]\nLet’s explore how our sample estimates of these parameters varied from student to student:\n\n# Import the experiment results\nlibrary(gsheet)\nresults &lt;- gsheet2tbl('https://docs.google.com/spreadsheets/d/11OT1VnLTTJasp5BHSKulgJiCbSLiutv8mKDOfvvXZSo/edit?usp=sharing')\n\nPlot each student’s sample estimate of the model line (gray). How do these compare to the assumed population model (red)?\n\nfandango %&gt;% \n  ggplot(aes(y = userscore_rt, x = criticscore_rt)) +\n  geom_abline(data = results, aes(intercept = sample_intercept, slope = sample_slope, linetype=section), color = \"gray\") + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n## Error in fandango %&gt;% ggplot(aes(y = userscore_rt, x = criticscore_rt)): could not find function \"%&gt;%\""
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html#exercise-7-sample-intercepts",
    "href": "template_qmds/18-sampling-normal-notes.html#exercise-7-sample-intercepts",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "Exercise 7: Sample intercepts",
    "text": "Exercise 7: Sample intercepts\nLet’s focus on just the sample estimates of the intercept parameter:\n\nresults %&gt;% \n  ggplot(aes(x = sample_intercept)) + \n  geom_density() + \n  geom_vline(xintercept = 32.3, color = \"red\")\n## Error in results %&gt;% ggplot(aes(x = sample_intercept)): could not find function \"%&gt;%\"\n\nComment on the shape, center, and spread of these sample estimates and how they relate to the (assumed) population intercept (red line).\n\nYour response here"
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html#exercise-8-slopes",
    "href": "template_qmds/18-sampling-normal-notes.html#exercise-8-slopes",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "Exercise 8: Slopes",
    "text": "Exercise 8: Slopes\nSuppose we were to construct a density plot of the sample estimates of the criticscore_rt coefficient (i.e. the slopes).\n\nIntuitively, what shape do you think this plot will have?\n\n\nYour response here\n\n\nIntuitively, around what value do you think this plot will be centered?\n\n\nYour response here\n\n\nCheck your intuition:\n\n\nresults %&gt;% \n  ggplot(aes(x = sample_slope)) + \n  geom_density() + \n  geom_vline(xintercept = 0.52, color = \"red\")\n## Error in results %&gt;% ggplot(aes(x = sample_slope)): could not find function \"%&gt;%\"\n\n\nThinking back to the 68-95-99.7 rule, visually approximate the standard deviation among the sample slopes.\n\n\nYour response here"
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html#exercise-9-standard-error",
    "href": "template_qmds/18-sampling-normal-notes.html#exercise-9-standard-error",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "Exercise 9: Standard error",
    "text": "Exercise 9: Standard error\nYou’ve likely observed that the typical or mean slope estimate is roughly equal to the (assumed) population slope parameter of 0.52:\n\nresults %&gt;% \n  summarize(mean(sample_slope))\n## Error in results %&gt;% summarize(mean(sample_slope)): could not find function \"%&gt;%\"\n\nThus the standard deviation of the slope estimates measures how far we might expect an estimate to fall from the (assumed) population slope parameter.\nThat is, it measures the typical or standard error in our sample estimates:\n\nresults %&gt;% \n  summarize(sd(sample_slope))\n## Error in results %&gt;% summarize(sd(sample_slope)): could not find function \"%&gt;%\"\n\n\nRecall your sample estimate of the slope. How far is it from the population slope, 0.52?\n\n\nHow many standard errors does your estimate fall from the population slope? That is, what’s your Z-score?\n\n\nReflecting upon your Z-score, do you think your sample estimate was one of the “lucky” ones, or one of the “unlucky” ones?\n\n\nYour response here"
  },
  {
    "objectID": "template_qmds/18-sampling-normal-notes.html#done",
    "href": "template_qmds/18-sampling-normal-notes.html#done",
    "title": "The Normal model & sampling variation (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html",
    "href": "template_qmds/19-sampling-dist-clt-notes.html",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "",
    "text": "Let \\(\\beta\\) be some population parameter and \\(\\hat{\\beta}\\) be a sample estimate of \\(\\beta\\). Our goals for the day are to:\n\nuse simulation to solidify our understanding of sampling distributions and standard errors\nexplore the appropriateness of the Central Limit Theorem in approximating a sampling distribution\nexplore the impact of sample size on sampling distributions and standard errors\n\n\n\n\nPlease watch the following videos and readings before class:\n\nReading: Section 6.7 in the STAT 155 Notes\nVideo 1: sampling distributions\nVideo 2: Central Limit Theorem"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#learning-goals",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#learning-goals",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "",
    "text": "Let \\(\\beta\\) be some population parameter and \\(\\hat{\\beta}\\) be a sample estimate of \\(\\beta\\). Our goals for the day are to:\n\nuse simulation to solidify our understanding of sampling distributions and standard errors\nexplore the appropriateness of the Central Limit Theorem in approximating a sampling distribution\nexplore the impact of sample size on sampling distributions and standard errors"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#readings-and-videos",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#readings-and-videos",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "",
    "text": "Please watch the following videos and readings before class:\n\nReading: Section 6.7 in the STAT 155 Notes\nVideo 1: sampling distributions\nVideo 2: Central Limit Theorem"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#exercise-1-500-samples-of-size-10",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#exercise-1-500-samples-of-size-10",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "Exercise 1: 500 samples of size 10",
    "text": "Exercise 1: 500 samples of size 10\nRecall that we can sample 10 counties using sample_n():\n\n# Run this chunk a few times to explore the different samples you get\ncounty_clean %&gt;% \n  sample_n(size = 10, replace = FALSE)\n## Error: object 'county_clean' not found\n\nWe can also take a sample and then use the data to estimate the model:\n\n# Run this chunk a few times to explore the different sample models you get\ncounty_clean %&gt;% \n  sample_n(size = 10, replace = FALSE) %&gt;% \n  with(lm(pci_2019 ~ pci_2017))\n## Error: object 'county_clean' not found\n\nWe can also take multiple unique samples and build a sample model from each.\nThe code below obtains 500 separate samples of 10 counties, and stores the model estimates from each:\n\n# Set the seed so that we all get the same results\nset.seed(155)\n\n# Store the sample models\nsample_models_10 &lt;- mosaic::do(500)*(\n  county_clean %&gt;% \n    sample_n(size = 10, replace = FALSE) %&gt;% \n    with(lm(pci_2019 ~ pci_2017))\n)\n## Error: object 'county_clean' not found\n\n# Check it out\nhead(sample_models_10)\n## Error in h(simpleError(msg, call)): error in evaluating the argument 'x' in selecting a method for function 'head': object 'sample_models_10' not found\ndim(sample_models_10)\n## Error: object 'sample_models_10' not found\n\nReflect\n\nWhat’s the point of the do() function?!? If you’ve taken any COMP classes, what process do you think do() is a shortcut for?\nWhat is stored in the Intercept, pci_2017, and r.squared columns of the results?"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#exercise-2-sampling-distribution",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#exercise-2-sampling-distribution",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "Exercise 2: Sampling distribution",
    "text": "Exercise 2: Sampling distribution\nCheck out the resulting 500 sample models:\n\ncounty_clean %&gt;% \n  ggplot(aes(x = pci_2017, y = pci_2019)) + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  geom_abline(data = sample_models_10, \n              aes(intercept = Intercept, slope = pci_2017), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n## Error: object 'county_clean' not found\n\nLet’s focus on the slopes of these 500 sample models. A plot of the 500 slopes approximates the sampling distribution of the sample slopes.\n\nsample_models_10 %&gt;% \n  ggplot(aes(x = pci_2017)) + \n  geom_density() + \n  geom_vline(xintercept = 1.027, color = \"red\") + \n  xlim(0.3, 1.7)\n## Error: object 'sample_models_10' not found\n\nReflect: Describe the sampling distribution. What’s its general shape? Where is it centered? Roughly what’s its spread / i.e. what’s the range of estimates you observed?"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#exercise-3-standard-error",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#exercise-3-standard-error",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "Exercise 3: Standard error",
    "text": "Exercise 3: Standard error\nFor a more rigorous assessment of the spread among the sample slopes, let’s calculate their standard deviation:\n\nsample_models_10 %&gt;% \n  summarize(sd(pci_2017))\n## Error: object 'sample_models_10' not found\n\nRecall: The standard deviation of sample estimates is called a “standard error”.\nIt measures the typical distance of a sample estimate from the actual population value."
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#exercise-4-central-limit-theorem-clt",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#exercise-4-central-limit-theorem-clt",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "Exercise 4: Central Limit Theorem (CLT)",
    "text": "Exercise 4: Central Limit Theorem (CLT)\nRecall that the CLT assumes that, so long as our sample size is “big enough”, the sampling distribution of the sample slope will be Normal.\nSpecifically, all possible sample slopes will vary Normally around the population slope.\n\nDo your simulation results support this assumption?\nWant more intuition into the CLT? Watch this video explanation using bunnies and dragons: https://www.youtube.com/watch?v=jvoxEYmQHNM"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#exercise-5-using-the-clt",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#exercise-5-using-the-clt",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "Exercise 5: Using the CLT",
    "text": "Exercise 5: Using the CLT\nLet \\(\\hat{\\beta}_1\\) be an estimate of the population slope parameter \\(\\beta_1\\) calculated from a sample of 10 counties.\nIn exercise 3, you approximated that \\(\\hat{\\beta}_1\\) has a standard error of roughly 0.16.\nThus, by the CLT, the sampling distribution of \\(\\hat{\\beta}_1\\) is:\n\\[\\hat{\\beta}_1 \\sim N(\\beta_1, 0.16^2)\\]\nUse this result with the 68-95-99.7 property of the Normal model to understand the potential error in a slope estimate.\n\nThere are many possible samples of 10 counties. What percent of these will produce an estimate \\(\\hat{\\beta}_1\\) that’s within 0.32, i.e. 2 standard errors, of the actual population slope \\(\\beta_1\\)?\nMore than 2 standard errors from \\(\\beta_1\\)?\nMore than 0.48, i.e. 3 standard errors, above \\(\\beta_1\\)?"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#exercise-6-clt-and-the-68-95-99.7-rule",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#exercise-6-clt-and-the-68-95-99.7-rule",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "Exercise 6: CLT and the 68-95-99.7 Rule",
    "text": "Exercise 6: CLT and the 68-95-99.7 Rule\nFill in the blanks below to complete some general properties assumed by the CLT:\n\n___% of samples will produce \\(\\hat{\\beta}_1\\) estimates within 1 st. err. of \\(\\beta_1\\)\n___% of samples will produce \\(\\hat{\\beta}_1\\) estimates within 2 st. err. of \\(\\beta_1\\)\n___% of samples will produce \\(\\hat{\\beta}_1\\) estimates within 3 st. err. of \\(\\beta_1\\)"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#exercise-7-increasing-sample-size",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#exercise-7-increasing-sample-size",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "Exercise 7: Increasing sample size",
    "text": "Exercise 7: Increasing sample size\nNow that we have a sense of the potential variability and error in sample estimates, let’s consider the impact of sample size. Suppose we were to increase our sample size from n = 10 to n = 50 or n = 200 counties. What impact do you anticipate this having on our sample estimates of the population parameters:\n\nDo you expect there to be more or less variability among the sample model lines?\nAround what value would you expect the sampling distribution of sample slopes to be centered?\nWhat general shape would you expect that sampling distribution to have?\nIn comparison to estimates based on the samples of size 10, do you think the estimates based on samples of size 50 will be closer to or farther from the true slope (on average)?"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#exercise-8-500-samples-of-size-n",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#exercise-8-500-samples-of-size-n",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "Exercise 8: 500 samples of size n",
    "text": "Exercise 8: 500 samples of size n\nLet’s increase the sample size in our simulation. Fill in the blanks to take 500 samples of size 50, and build a sample model from each. Once you complete the code, remove eval = FALSE.\n\nset.seed(155)\nsample_models_50 &lt;- mosaic::do(___)*(\n  county_clean %&gt;% \n    ___(size = ___, replace = FALSE) %&gt;% \n    ___(___(pci_2019 ~ pci_2017))\n)\n\n# Check it out\nhead(sample_models_50)\n\nSimilarly, take 500 samples of size 200, and build a sample model from each. Once you complete the code, remove eval = FALSE.\n\nset.seed(155)\nsample_models_200 &lt;- mosaic::do(___)*(\n  county_clean %&gt;% \n    ___(size = ___, replace = FALSE) %&gt;% \n    ___(___(pci_2019 ~ pci_2017))\n)\n\n# Check it out\nhead(sample_models_200)"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#exercise-9-impact-of-sample-size-part-i",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#exercise-9-impact-of-sample-size-part-i",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "Exercise 9: Impact of sample size (part I)",
    "text": "Exercise 9: Impact of sample size (part I)\nCompare and contrast the 500 sets of sample models when using samples of size 10, 50, and 200.\n\n# Remove eval = FALSE\n# 500 sample models using samples of size 10\ncounty_clean %&gt;% \n  ggplot(aes(x = pci_2017, y = pci_2019)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_abline(data = sample_models_10, \n              aes(intercept = Intercept, slope = pci_2017), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n# Remove eval = FALSE\n# 500 sample models using samples of size 50\ncounty_clean %&gt;% \n  ggplot(aes(x = pci_2017, y = pci_2019)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_abline(data = sample_models_50, \n              aes(intercept = Intercept, slope = pci_2017), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\n\n# Remove eval = FALSE\n# 500 sample models using samples of size 200\ncounty_clean %&gt;% \n  ggplot(aes(x = pci_2017, y = pci_2019)) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  geom_abline(data = sample_models_200, \n              aes(intercept = Intercept, slope = pci_2017), \n              color = \"gray\", size = 0.25) + \n  geom_smooth(method = \"lm\", color = \"red\", se = FALSE)\n\nReflect: What happens to our sample models as sample size increases? Was this what you expected?"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#exercise-10-impact-of-sample-size-part-ii",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#exercise-10-impact-of-sample-size-part-ii",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "Exercise 10: Impact of sample size (part II)",
    "text": "Exercise 10: Impact of sample size (part II)\nLet’s focus on just the sampling distributions of our 500 slope estimates \\(\\hat{\\beta}_1\\). For easy comparison, plot the estimates based on samples of size 10, 50, and 200 on the same frame:\n\n# Remove eval = FALSE\n\n# Don't think too hard about this code!\n# Combine the estimates & sample size into a new data set\n# Then plot it\ndata.frame(estimates = c(sample_models_10$pci_2017, sample_models_50$pci_2017, sample_models_200$pci_2017),\n           sample_size = rep(c(\"10\",\"50\",\"200\"), each = 500)) %&gt;% \n  mutate(sample_size = fct_relevel(sample_size, c(\"10\", \"50\", \"200\"))) %&gt;% \n  ggplot(aes(x = estimates, color = sample_size)) + \n  geom_density() + \n  geom_vline(xintercept = 1.027, color = \"red\", linetype = \"dashed\") + \n  labs(title = \"Sampling distributions of the sample slope\")\n\nReflect: How do the shapes, centers, and spreads of these sampling distributions compare? Was this what you expected?"
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#exercise-11-properties-of-sampling-distributions",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#exercise-11-properties-of-sampling-distributions",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "Exercise 11: Properties of sampling distributions",
    "text": "Exercise 11: Properties of sampling distributions\nIn light of your observations, complete the following statements about the sampling distribution of the sample slope.\n\nFor all sample sizes, the shape of the sampling distribution is roughly ___ and the sampling distribution is roughly centered around ___, the true population slope.\nAs sample size increases:\nThe average sample slope estimate INCREASES / DECREASES / IS FAIRLY STABLE.\nThe standard error of the sample slopes INCREASES / DECREASES / IS FAIRLY STABLE.\nThus, as sample size increases, our sample slopes become MORE RELIABLE / LESS RELIABLE."
  },
  {
    "objectID": "template_qmds/19-sampling-dist-clt-notes.html#done",
    "href": "template_qmds/19-sampling-dist-clt-notes.html#done",
    "title": "Sampling distributions & the CLT (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/21-confidence-intervals-notes.html",
    "href": "template_qmds/21-confidence-intervals-notes.html",
    "title": "Confidence intervals (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nConstruct (approximate) confidence intervals by hand using the 68-95-99.7 rule\nConstruct exact confidence intervals in R\nInterpret confidence intervals in context by referring to the coefficient of interest\nUse confidence intervals to make statements about whether there appear to be true population relationships, changes, and differences\n\n\n\n\nPlease complete the following reading before class.\n\nReading: Section 7 Introduction, Section 7.1, Section 7.2 (stop when you get to 7.2.4.3 Confidence Intervals for Prediction) in the STAT 155 Notes\n\nOptionally you can use the following videos as a companion to the reading (not in place of the reading):\n\nVideo 1: Introduction to Confidence Intervals\nVideo 2: Confidence Intervals: Construction and Interpretation"
  },
  {
    "objectID": "template_qmds/21-confidence-intervals-notes.html#learning-goals",
    "href": "template_qmds/21-confidence-intervals-notes.html#learning-goals",
    "title": "Confidence intervals (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nConstruct (approximate) confidence intervals by hand using the 68-95-99.7 rule\nConstruct exact confidence intervals in R\nInterpret confidence intervals in context by referring to the coefficient of interest\nUse confidence intervals to make statements about whether there appear to be true population relationships, changes, and differences"
  },
  {
    "objectID": "template_qmds/21-confidence-intervals-notes.html#readings-and-videos",
    "href": "template_qmds/21-confidence-intervals-notes.html#readings-and-videos",
    "title": "Confidence intervals (Notes)",
    "section": "",
    "text": "Please complete the following reading before class.\n\nReading: Section 7 Introduction, Section 7.1, Section 7.2 (stop when you get to 7.2.4.3 Confidence Intervals for Prediction) in the STAT 155 Notes\n\nOptionally you can use the following videos as a companion to the reading (not in place of the reading):\n\nVideo 1: Introduction to Confidence Intervals\nVideo 2: Confidence Intervals: Construction and Interpretation"
  },
  {
    "objectID": "template_qmds/21-confidence-intervals-notes.html#exercise-1",
    "href": "template_qmds/21-confidence-intervals-notes.html#exercise-1",
    "title": "Confidence intervals (Notes)",
    "section": "Exercise 1",
    "text": "Exercise 1\nResearch question: Is the relationship between wind speed (windspeed) (in miles per hour) and number of riders (riders_total) different across weekdays and weekends?\n\nPart a\nConstruct and interpret a visualization that would address this question.\n\n\nPart b\nFit a regression model that would address our research question. (Should it be a linear or a logistic regression model?) Interpret only the coefficient of interest.\n\nmod_bikes &lt;- ___\n## Error in parse(text = input): &lt;text&gt;:1:15: unexpected input\n## 1: mod_bikes &lt;- __\n##                   ^\n\n\n\nPart c\n\nConstruct an approximate 95% confidence interval (CI) for the coefficient of interest by hand using the 68-95-99.7 rule.\nCompare your confidence interval to the one given by confint() which gives an exact confidence interval. (The columns give the lower and upper ends of the CI for each coefficient.)\nInterpret the exact confidence interval in context.\nIs zero in the interval? Do we have evidence for a real difference in the windspeed-riders relationship across weekends and weekdays?\n\n\n# By hand (you fill in)\n\n\n# Using confint()\nconfint(mod_bikes, level = 0.95)\n## Error: object 'mod_bikes' not found\n\n\n\nPart d\nLet’s see if these results agree when looking at adjusted R-squared.\nFit another regression model that does not have the coefficient of interest from your Part b model. Compare the adjusted R-squared values between this model and the Part b model. Explain your findings."
  },
  {
    "objectID": "template_qmds/21-confidence-intervals-notes.html#exercise-2",
    "href": "template_qmds/21-confidence-intervals-notes.html#exercise-2",
    "title": "Confidence intervals (Notes)",
    "section": "Exercise 2",
    "text": "Exercise 2\nResearch question: How different is holiday ridership from non-holidays, after accounting for confounding factors?\n\nPart a\nWe believe that weather category (weather_cat), temperature (temp_actual), and wind speed (windspeed) confound the relationship of interest.\n\nDraw a causal graph that shows the 5 variables of interest. Based on your graph do you believe that the 3 potential confounders are indeed confounders (and not mediators or colliders)?\nConstruct visualizations that allow you how each potential confounder relates to riders_total and to holiday.\n\n\n\nPart b\nBased on your Part a explorations, fit an appropriate regression model to answer our research question. Interpret only the coefficient of interest.\nA note about scientific notation in R: Sometimes you may see numbers with the letter e in the middle. This is R’s way of expressing scientific notation. Whenever you see e, replace that with 10 to the power of .... So:\n\n1.234e+02 is 1.234 x 10^2 = 123.4\n1.234e-02 is 1.234 x 10^(-2) = 0.01234\n\n\n\nPart c\n\nUse confint() to construct a 95% confidence interval for the coefficient of interest.\nInterpret this confidence interval in context.\nIs zero in the interval? Do we have evidence for a real holiday effect on ridership?"
  },
  {
    "objectID": "template_qmds/21-confidence-intervals-notes.html#exercise-3",
    "href": "template_qmds/21-confidence-intervals-notes.html#exercise-3",
    "title": "Confidence intervals (Notes)",
    "section": "Exercise 3",
    "text": "Exercise 3\nThe Western Collaborative Group Study (WCGS) was designed in order to investigate a possible link between Type A behavior and coronary heart disease (CHD), and to develop a framework to select patients for intervention in order to decrease risk of CHD. The study contained 3154 cis men between the ages of 39 and 59 in California who had no history of CHD. They were enrolled in the study in 1960 and 1961, underwent a medical examination and covered their medical history, and they were re-examined annually for interim cardiovascular history.\nA full codebook is available here. We will focus on the following variables:\n\nchd: Presence (1) or absence (0) of CHD over followup (outcome)\ntabp: Presence (1) or absence (0) of Type A behavior (main variable of interest)\nage: Age at time of enrollment in the study (years)\nsbp: Systolic blood pressure\ndbp: Diastolic blood pressure\nchol: Cholesterol (mg/dL)\nncigs: Number of cigarettes smoked per day\narcus: Presence (1) or absence (0) of arcus senilis (a colored ring around the cornea made up of lipids like cholesterol and believed to be a risk factor for CHD)\nbmi: BMI = weight * 703 / height^2\n\nResearch question: Is there a causal effect of Type A/B personality on developing coronary heart disease?\n\nwcgs &lt;- read_csv(\"https://mac-stat.github.io/data/wcgs.csv\")\n\n\nPart a\nWe believe that the following variables are confounders of the relationship between Type A/B personality tabp and coronary heart disease (CHD): age + sbp + dbp + chol + ncigs + arcus + bmi.\nFit a regression model that would address our research question. (Should it be a linear or a logistic regression model?) Interpret only the coefficient of interest.\n\ntypea_mod &lt;- ___\n## Error in parse(text = input): &lt;text&gt;:1:15: unexpected input\n## 1: typea_mod &lt;- __\n##                   ^\n\n\n\nPart b\n\nConstruct a 95% confidence interval for the odds ratio of interest using the following code.\nInterpret the confidence interval in context.\nIs 1 contained in the interval? Why is 1 a relevant value to look for here?\n\n\n\nPart c\n(On your own time)\nThe data context in this exercise has a fraught history with the smoking industry. Read this article for some context about how the Type A personality came to be defined and studied. (One big takeaway: The smoking industry had a large incentive to find something to blame health problems on other than smoking!)"
  },
  {
    "objectID": "template_qmds/21-confidence-intervals-notes.html#exercise-4",
    "href": "template_qmds/21-confidence-intervals-notes.html#exercise-4",
    "title": "Confidence intervals (Notes)",
    "section": "Exercise 4",
    "text": "Exercise 4\nFor each of the following MISINTERPRETATIONS of a 95% confidence interval (a,b), explain why the statement is a misinterpretation.\n\nMisinterpretation 1: “There is a 95% probability that the population parameter is within (a,b).”\n\nResponse: The population parameter is not random. It is either in the interval or not, so the probability is 1 or 0. The 95% means that 95% of random samples (that are representative of the population of interest) are expected to contain the true population parameter—“95% confidence” is describing confidence in the interval construction process.\n\nMisinterpretation 2: “There is a 5% probability that the population parameter is not within (a,b).”\n\nResponse: This is incorrect for the same reason as the first misinterpretation.\n\nMisinterpretation 3: “There is a 95% chance that the sample estimate in (a,b).”\n\nResponse: The sample estimate is always in the interval by construction."
  },
  {
    "objectID": "template_qmds/21-confidence-intervals-notes.html#reflection",
    "href": "template_qmds/21-confidence-intervals-notes.html#reflection",
    "title": "Confidence intervals (Notes)",
    "section": "Reflection",
    "text": "Reflection\nHow are you feeling about your ability to translate research questions into appropriate statistical investigations and addressing those questions using output from those investigations? What has gotten easier? What remains challenging?\n\nResponse:"
  },
  {
    "objectID": "template_qmds/21-confidence-intervals-notes.html#done",
    "href": "template_qmds/21-confidence-intervals-notes.html#done",
    "title": "Confidence intervals (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/23-hypothesis-testing-details-notes.html",
    "href": "template_qmds/23-hypothesis-testing-details-notes.html",
    "title": "Hypothesis testing details and practice (Notes)",
    "section": "",
    "text": "You can download a template file to work with here.\nFile organization: Save this file in the “Activities” subfolder of your “STAT155” folder.\n\n\n\nBy the end of this lesson, you should be able to:\n\nApply the procedure for a formal hypothesis test\nArticulate how we can formalize a research question as a testable, statistical hypothesis\n\n\n\n\nPlease complete the following reading or videos before class:\n\nReading: Section 7.3 (stop when you get to Section 7.3.4) in the STAT 155 Notes\nVideo 1: Introduction to Statistical Inference\nVideo 2: Hypothesis Testing Framework\nVideo 3: Hypothesis Testing Procedure"
  },
  {
    "objectID": "template_qmds/23-hypothesis-testing-details-notes.html#learning-goals",
    "href": "template_qmds/23-hypothesis-testing-details-notes.html#learning-goals",
    "title": "Hypothesis testing details and practice (Notes)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nApply the procedure for a formal hypothesis test\nArticulate how we can formalize a research question as a testable, statistical hypothesis"
  },
  {
    "objectID": "template_qmds/23-hypothesis-testing-details-notes.html#readings-and-videos",
    "href": "template_qmds/23-hypothesis-testing-details-notes.html#readings-and-videos",
    "title": "Hypothesis testing details and practice (Notes)",
    "section": "",
    "text": "Please complete the following reading or videos before class:\n\nReading: Section 7.3 (stop when you get to Section 7.3.4) in the STAT 155 Notes\nVideo 1: Introduction to Statistical Inference\nVideo 2: Hypothesis Testing Framework\nVideo 3: Hypothesis Testing Procedure"
  },
  {
    "objectID": "template_qmds/23-hypothesis-testing-details-notes.html#exercise-1",
    "href": "template_qmds/23-hypothesis-testing-details-notes.html#exercise-1",
    "title": "Hypothesis testing details and practice (Notes)",
    "section": "Exercise 1",
    "text": "Exercise 1\nResearch Question: Can we predict whether or not a mushroom is poisonous based on the shape of its cap?\nFor this exercise, we will look at data from various species of gilled mushrooms in the Agaricus and Lepiota Family. We have information on whether a mushroom is poisonous (TRUE if it is, FALSE if it’s edible), the shape of its cap (cap_shape, a categorical variable with 6 categories), the texture of its cap surface (cap_surface, a categorical variable with 4 categories), and the size of its gills (gill_size, a categorical variable with two categories)\n\n# Load the data & packages\nlibrary(tidyverse)\nlibrary(readr)\nlibrary(broom)\n\nmushrooms &lt;- read_csv(\"https://Mac-STAT.github.io/data/mushrooms.csv\")\n\nmushrooms &lt;- mushrooms %&gt;%\n  mutate(cap_shape = relevel(as.factor(cap_shape), ref=\"flat\")) %&gt;%\n  dplyr::select(poisonous, cap_shape)\n\nhead(mushrooms)\n## # A tibble: 6 × 2\n##   poisonous cap_shape\n##   &lt;lgl&gt;     &lt;fct&gt;    \n## 1 TRUE      convex   \n## 2 FALSE     convex   \n## 3 FALSE     bell     \n## 4 TRUE      convex   \n## 5 FALSE     convex   \n## 6 FALSE     convex\n\n\nPart a\nOne of the most poisonous species of mushrooms is the Amanita phalloides or “Death Cap” mushroom, which typically has a flat cap shape when mature. Based on this anecdote, we hypothesize that species of mushrooms with flat caps in general may be more likely to be poisonous than edible.\nFirst, let’s translate this question to an appropriate null and alternative hypothesis that we can compare with a formal hypothesis test. Remember that poisonous is a binary outcome, so we need to frame our null and alternative hypotheses in terms of odds (i.e., Odds(poisionous | flat cap) = P(poisonous|flat cap)/P(edible | flat cap)).\n\nYour answer\n\n\n\nPart b\n\nFit a logistic regression model to investigate whether cap_shape is associated with a mushroom being poisonous. (Note that in the setup code above, we have forced the reference category for the cap_shape predictor to be flat; without this, the reference category by default would be set as bell, which is the first category when sorted alphabetically).\n\n\nmushroom_mod1 &lt;- ()\n\ncoef(mushroom_mod1)\n## Error in parse(text = input): &lt;text&gt;:1:19: unexpected ')'\n## 1: mushroom_mod1 &lt;- ()\n##                       ^\n\n\n\nPart c\nProvide an appropriate interpretation of the intercept coefficient on the odds scale. Based on this interpretation, do you believe mushrooms with flat caps are more likely to be poisonous, or more likely to be edible?\n\nYour answer here\n\n\n\nPart d\nLet’s look at the full model summary:\n\nsummary(mushroom_mod1)\n## Error: object 'mushroom_mod1' not found\n\nReport and interpret the test statistic for the intercept term (our coefficient of interest):\n\nYour answer\n\n\n\nPart e\n\nReport and interpret the p-value for the intercept term.\nBased on this p-value and a significance level of 0.05, do we have evidence that mushrooms with flat caps are more likely to be poisonous than edible?\n\n\nYour answer\n\n\n\nPart f\nNow suppose we are interested in whether the odds of being poisonous are different for mushrooms with other cap shapes.\nBy hand, calculate the odds of being poisonous for mushrooms with knobbed caps, conical caps, and sunken caps (remember that the non-exponentiated coefficients represent a difference in log-odds compared to the reference category):\n\nodds(poisonous | knobbed cap) =\n\n\nodds(poisonous | conical cap) =\n\n\nodds(poisonous | sunken cap) =\n\n\n\nPart g\nBased on these odds, which of the 4 mushroom cap shapes we’ve investigated (flat, knobbed, conical, and sunken) do you believe is the best indicator that it’s edible? Which cap shape do you expect is most likely to be poisonous?\n\nYour answer\n\n\n\nPart h\nLet’s get the full model summary again:\n\ntidy(mushroom_mod1) %&gt;% \n    mutate(exp_estimate = exp(estimate)) %&gt;% \n    select(term, estimate, exp_estimate, everything())\n## Error: object 'mushroom_mod1' not found\n\nNow report and interpret the p-values for the coefficients corresponding to cap_shapeknobbed, cap_shapeconical, and cap_shapesunken:\n\nYour answer\n\n\n\nPart i\nBased on the model summary output in part h above, if you were given a plate of mushrooms with different cap shapes and had to pick one to eat, which one would you choose? Which cap shape would you absolutely avoid at all costs? Are your decisions guided by the coefficient estimates, the p-values, or both?\n\nYour answer\n\n\n\nPart j\nLet’s look at the data a slightly different way, using a 6x2 table of counts:\n\nmushrooms %&gt;% \n  mutate(cap_shape=as.factor(cap_shape),\n         poisonous=as.factor(poisonous)) %&gt;%\n  dplyr::count(cap_shape, poisonous, .drop=FALSE) %&gt;% \n  pivot_wider(names_from=poisonous, values_from=n, names_prefix=\"Poisonous = \")\n## # A tibble: 6 × 3\n##   cap_shape `Poisonous = FALSE` `Poisonous = TRUE`\n##   &lt;fct&gt;                   &lt;int&gt;              &lt;int&gt;\n## 1 flat                     1596               1556\n## 2 bell                      404                 48\n## 3 conical                     0                  4\n## 4 convex                   1948               1708\n## 5 knobbed                   228                600\n## 6 sunken                     32                  0\n\nNow, if you were given a plate of mushrooms with different cap shapes and had to pick one shape to eat and one to absolutely avoid, would you choose the same shapes? Why or why not?\n\nYour answer"
  },
  {
    "objectID": "template_qmds/23-hypothesis-testing-details-notes.html#exercise-2",
    "href": "template_qmds/23-hypothesis-testing-details-notes.html#exercise-2",
    "title": "Hypothesis testing details and practice (Notes)",
    "section": "Exercise 2",
    "text": "Exercise 2\nFor this exercise, let’s return to the fish dataset from a previous activity.\n\nfish &lt;- read_csv(\"https://Mac-STAT.github.io/data/Mercury.csv\")\n\nhead(fish)\n## # A tibble: 6 × 5\n##   River  Station Length Weight Concen\n##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1 Lumber       0   47     1616   1.6 \n## 2 Lumber       0   48.7   1862   1.5 \n## 3 Lumber       0   55.7   2855   1.7 \n## 4 Lumber       0   45.2   1199   0.73\n## 5 Lumber       0   44.7   1320   0.56\n## 6 Lumber       0   43.8   1225   0.51\n\nResearch question: We believe the length of a fish (measured in centimeters) is causally associated with its mercury concentration (measured in parts per million [ppm]). We suspect that the river a fish is sampled from may be a confounder, since differences in the river environment may causally influence both the average length of fish (e.g. due to differences in water temperature or food availability) as well as mercury concentration (e.g. due to differences between the two rivers in mercury pollution levels).\n\nPart a\nFit a linear regression model that can be used to answer our research question.\n\nmod_fish1 &lt;- ___\nsummary(mod_fish1)\n## Error in parse(text = input): &lt;text&gt;:1:15: unexpected input\n## 1: mod_fish1 &lt;- __\n##                   ^\n\n\n\nPart b\nInterpret the coefficient estimate, test statistic, and p-value for the RiverWacamaw coefficient. Assume we have specified a significance level of 0.05.\n\nResponse\n\n\n\nPart c\nSuppose we now want to determine if the causal effect of fish length on mercury concentration differs according to the river a fish was sampled from.\nFirst, modify the code chunk below to visualize the 3-way relationship between the Concen, Length, and River variables.\n\nfish %&gt;% \n  ggplot(aes(x = ___, y = ___, colour = ___)) + \n  # [ADDITIONAL GGPLOT LAYER(S)]\n## Error in parse(text = input): &lt;text&gt;:2:19: unexpected input\n## 1: fish %&gt;% \n## 2:   ggplot(aes(x = __\n##                      ^\n\nNext, fit an appropriate linear regression model with an interaction term to investigate this question.\n\nmod_fish2 &lt;- ___\nsummary(mod_fish2)\n## Error in parse(text = input): &lt;text&gt;:1:15: unexpected input\n## 1: mod_fish2 &lt;- __\n##                   ^\n\n\n\nPart d\nInterpret the coefficient estimate, test statistic, and p-value for the RiverWacamaw:Length interaction term in this revised model (mod_fish2). Assume we’ve set a significance level of 0.05.\n\nResponse\n\n\n\nPart e\nInterpret the coefficient estimate, test statistic, and p-value for the RiverWacamaw coefficient in this revised model (mod_fish2). (again, you can assume we’ve set a significance level of 0.05).\n\n\nPart f (CHALLENGE)\nSuppose another researcher runs the same model we fit in part c above (mod_fish2), but they claim that a more appropriate alternative hypothesis should be Beta_1 &lt; 0, (and not Beta_1 ≠ 0, as is assumed by default when running a regression model). Because of this, they reported a smaller p-value for the coefficient, and claim that the Wacamaw River has a lower baseline mercury concentration (i.e., when Length = 0cm).\nWhat is the p-value they would have reported for the RiverWacamaw coefficient in mod_fish2?\n\nResponse\n\nWhat is a potential ethical problem with the other researcher’s claim that the alternative hypothesis should be Beta_1 &lt; 0?\n\nResponse\n\n\n\nPart g (CHALLENGE)\nYou point out to the other researcher that the intercept and RiverWacamaw coefficients are both negative, so whatever difference in mercury concentration between the two rivers your model predicts “at baseline” is not useful or meaningful–you cannot have a fish that is 0cm long, nor a mercury concentration &lt;0ppm.\nYou propose that a more appropriate model should transform the Length variable in some way to make the intercept more interpretable. Create a new variable named Length_adj with this transformation and use it to re-fit the model:\n\nmod_fish3 &lt;- lm(Concen ~ Length_adj*River, data=fish)\n## Error in eval(predvars, data, env): object 'Length_adj' not found\nsummary(mod_fish3)\n## Error: object 'mod_fish3' not found\n\nCompare the output of this model to that of mod_fish2. What happened to the estimate, test statistic, and p-value for the RiverWacamaw coefficient? How does this affect your conclusion? How about the other researcher’s conclusion?\n\nResponse"
  },
  {
    "objectID": "template_qmds/23-hypothesis-testing-details-notes.html#done",
    "href": "template_qmds/23-hypothesis-testing-details-notes.html#done",
    "title": "Hypothesis testing details and practice (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  },
  {
    "objectID": "template_qmds/25-f-tests-notes.html",
    "href": "template_qmds/25-f-tests-notes.html",
    "title": "Nested Models & F-Tests (Notes)",
    "section": "",
    "text": "“Nested models” are models where the covariates of one model are a subset of another model. As an example, consider the following models for estimating the association between forced expiratory volume (FEV) and smoking status:\nModel 1:\n\\[\nE[FEV \\mid smoke] = \\beta_0 + \\beta_1 smoke\n\\]\nModel 2:\n\\[\nE[FEV \\mid smoke, age] = \\beta_0 + \\beta_1 smoke + \\beta_2 age\n\\] Here, Model 1 is “nested” inside Model 2, since the covariates included in Model 1 (only smoke) are a subset of those in Model 2 (both smoke and age).\nAn example of non-nested models are…\nModel 3:\n\\[\nE[FEV \\mid smoke, height] = \\beta_0 + \\beta_1 smoke + \\beta_2 height\n\\]\nModel 4:\n\\[\nE[FEV \\mid smoke, sex] = \\beta_0 + \\beta_1 smoke + \\beta_2 sex\n\\]\nHere, even though Model 3 and Model 4 both contain smoke as explanatory variables, neither is nested in the other, since sex is not a part of Model 3, and height is not a part of Model 4.\n\n\n\nBy the end of this activity, you should be able to:\n\nDetermine if one model is nested within another\nDetermine which null and alternative hypotheses require an f-test\nDetermine which f-tests require the use of the anova function in R vs. the overall f-test given in regular regression output\nInterpret the results of an f-test in context\n\n\n\n\nPlease read the following notes and watch the following video before class:\n\nReading: Section 7.3.4 in the STAT 155 Notes\nVideo: F-Tests (script)"
  },
  {
    "objectID": "template_qmds/25-f-tests-notes.html#nested-models",
    "href": "template_qmds/25-f-tests-notes.html#nested-models",
    "title": "Nested Models & F-Tests (Notes)",
    "section": "",
    "text": "“Nested models” are models where the covariates of one model are a subset of another model. As an example, consider the following models for estimating the association between forced expiratory volume (FEV) and smoking status:\nModel 1:\n\\[\nE[FEV \\mid smoke] = \\beta_0 + \\beta_1 smoke\n\\]\nModel 2:\n\\[\nE[FEV \\mid smoke, age] = \\beta_0 + \\beta_1 smoke + \\beta_2 age\n\\] Here, Model 1 is “nested” inside Model 2, since the covariates included in Model 1 (only smoke) are a subset of those in Model 2 (both smoke and age).\nAn example of non-nested models are…\nModel 3:\n\\[\nE[FEV \\mid smoke, height] = \\beta_0 + \\beta_1 smoke + \\beta_2 height\n\\]\nModel 4:\n\\[\nE[FEV \\mid smoke, sex] = \\beta_0 + \\beta_1 smoke + \\beta_2 sex\n\\]\nHere, even though Model 3 and Model 4 both contain smoke as explanatory variables, neither is nested in the other, since sex is not a part of Model 3, and height is not a part of Model 4."
  },
  {
    "objectID": "template_qmds/25-f-tests-notes.html#learning-goals",
    "href": "template_qmds/25-f-tests-notes.html#learning-goals",
    "title": "Nested Models & F-Tests (Notes)",
    "section": "",
    "text": "By the end of this activity, you should be able to:\n\nDetermine if one model is nested within another\nDetermine which null and alternative hypotheses require an f-test\nDetermine which f-tests require the use of the anova function in R vs. the overall f-test given in regular regression output\nInterpret the results of an f-test in context"
  },
  {
    "objectID": "template_qmds/25-f-tests-notes.html#readings-and-videos",
    "href": "template_qmds/25-f-tests-notes.html#readings-and-videos",
    "title": "Nested Models & F-Tests (Notes)",
    "section": "",
    "text": "Please read the following notes and watch the following video before class:\n\nReading: Section 7.3.4 in the STAT 155 Notes\nVideo: F-Tests (script)"
  },
  {
    "objectID": "template_qmds/25-f-tests-notes.html#exercise-1-nested-models",
    "href": "template_qmds/25-f-tests-notes.html#exercise-1-nested-models",
    "title": "Nested Models & F-Tests (Notes)",
    "section": "Exercise 1: Nested Models",
    "text": "Exercise 1: Nested Models\n\nWhich of the following models are nested in the model \\(E[A \\mid B, C, D] = \\beta_0 + \\beta_1 D + \\beta_2 B + \\beta_3 C + \\beta_4 B * C\\)?\n\n\nModel 1: \\(E[A \\mid B] = \\beta_0 + \\beta_1 B\\)\nModel 2: \\(E[A \\mid B, D] = \\beta_0 + \\beta_1 B + \\beta_2 D\\)\nModel 3: \\(E[B \\mid C] = \\beta_0 + \\beta_1 C\\)\nModel 4: \\(E[A \\mid B, C, D] = \\beta_0 + \\beta_1 B + \\beta_2 C + \\beta_3 D\\)\nModel 5: \\(E[A \\mid B, C, D] = \\beta_0 + \\beta_1 C + \\beta_2 B + \\beta_3 D + \\beta_4 B * D\\)\nModel 6: \\(E[A \\mid D] = \\beta_0 + \\beta_1 D\\)\n\n\nConsider the following models involving variables A, B, C, and D:\n\n\nModel 1: \\(E[A \\mid B] = \\beta_0 + \\beta_1 B\\)\nModel 2: \\(E[A \\mid B, C] = \\beta_0 + \\beta_1 B + \\beta_2 C\\)\nModel 3: \\(E[A \\mid B, C] = \\beta_0 + \\beta_1 B + \\beta_2 C + \\beta_3 BC\\)\nModel 4: \\(E[A \\mid C, D] = \\beta_0 + \\beta_1 C + \\beta_2 D\\)\nModel 5: \\(E[B \\mid A] = \\beta_0 + \\beta_1 A\\)\nModel 6: \\(E[B \\mid A, C] = \\beta_0 + \\beta_1 A + \\beta_2 C + \\beta_3 AC\\)\n\nDetermine for each of the following statements whether that statement is True or False.\n\nModel 1 is nested in Model 2\nModel 1 is nested in Model 3\nModel 1 is nested in Model 4\nModel 2 is nested in Model 3\nModel 3 is nested in Model 2\nModel 2 is nested in Model 6\n\n\nWhat is one (numeric) way to compare nested models? Explain how you would determine which model is “better” based on this metric."
  },
  {
    "objectID": "template_qmds/25-f-tests-notes.html#exercise-2-f-tests",
    "href": "template_qmds/25-f-tests-notes.html#exercise-2-f-tests",
    "title": "Nested Models & F-Tests (Notes)",
    "section": "Exercise 2: F-Tests",
    "text": "Exercise 2: F-Tests\nThis exercise involves the MacGrades.csv dataset, which contains a sub-sample (to help preserve anonymity) of every grade assigned to a former Macalester graduating class. For each of the 6414 rows of data, the following information is provided (with a few missing values):\n\nsessionID: A section ID number\nsid: A student ID number\ngrade: The grade obtained, as a numerical value (i.e. an A is a 4, an A- is a 3.67, etc.)\ndept: A department identifier (these have been made ambiguous to maintain anonymity)\nlevel: The course level (e.g. 100-, 200-, 300-, and 600-)\nsem: A semester identifier\nenroll: The section enrollment\niid: An instructor identifier (these have been made ambiguous to maintain anonymity)\n\n\n# load necessary packages\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(readr)\n\n# load datasets\nMacGrades &lt;- read_csv(\"https://mac-stat.github.io/data/MacGrades.csv\")\n\nNOTE: Questions (a) and (b), since they are exploratory in nature, can suck up a lot of time if you let them! For the sake of getting to the rest of the activity, please spend no more than ~5 minutes on them.\n\nHypothesize two relationships between the variables in the dataset (pick any two relationships you want!). Your response should be written in a paragraph form.\n\n\nResponse Put your response here\n\n\nExplore the relationship between course grades and other variables in the data. Make two visualizations, and describe any patterns you observe.\nNote that the level variable is currently quantitative. For this activity, we’d like to treat it as categorical. Create a new variable level_cat so that we can consider level categorically in the following analysis.\nSuppose we are interested in the relationship between course level (categorical) and student grades. Using grade as your outcome variable, fit a linear regression model to investigate this question.\n\nComment on the nature of the relationship between course level and student grades (this should not be a coefficient interpretation, but instead a description of a general trend, or lack thereof).\n\nState the null and alternative hypotheses associated with the research question in part (d).\n\n\\[\nH_0:\n\\]\n\\[\nH_1:\n\\]\n\nWhat is the p-value associated with this hypothesis test? Do we have enough evidence to reject the null hypothesis, using a significance threshold of 0.05?\nSuppose we are interested in the relationship between course enrollment and student grades. Again, use grade as your outcome variable, and fit a linear regression model to investigate this question.\nState the null and alternative hypotheses associated with the research question in part (g).\n\n\\[\nH_0:\n\\]\n\\[\nH_1:\n\\]\n\nWhat is the p-value associated with this hypothesis test? Do we have enough evidence to reject the null hypothesis, using a significance threshold of 0.05?\n\n\nDo we need to conduct a nested F-test using the anova function to complete our hypothesis testing procedure for the research question posed in part (g)? Explain why or why not."
  },
  {
    "objectID": "template_qmds/25-f-tests-notes.html#exercise-3-more-f-tests",
    "href": "template_qmds/25-f-tests-notes.html#exercise-3-more-f-tests",
    "title": "Nested Models & F-Tests (Notes)",
    "section": "Exercise 3: More F-tests",
    "text": "Exercise 3: More F-tests\n\nSuppose we are now interested in the association between course grade and enrollment for classes of the same level. Write a model statement in the form \\(E[Y | X] = ...\\) that will produce a statistical model that will allow us to answer our scientific question. Replace Y and X, where appropriate, with response and predictor variables.\n\n\\[\nE[Y | X] = ___\n\\]\nWhich coefficient(s) in your model is the one that is relevant to your research question?\n\nWhat are the relevant null and alternative hypotheses that address the scientific question in part (a)?\nFit the model you wrote in part (a), calculate a p-value, and report the results of the hypothesis test in part (b)."
  },
  {
    "objectID": "template_qmds/25-f-tests-notes.html#reflection",
    "href": "template_qmds/25-f-tests-notes.html#reflection",
    "title": "Nested Models & F-Tests (Notes)",
    "section": "Reflection",
    "text": "Reflection\nF-tests are useful when the null hypothesis you wish to test is such that more than one covariate is simultaneously equal to a specific number (typically zero). What scenarios, outside of those shown in this example, can you think of where a relevant scientific hypothesis you want to test involves more than one covariate being simultaneously equal to zero?\n\nResponse Put your response here."
  },
  {
    "objectID": "template_qmds/25-f-tests-notes.html#exercise-4",
    "href": "template_qmds/25-f-tests-notes.html#exercise-4",
    "title": "Nested Models & F-Tests (Notes)",
    "section": "Exercise 4",
    "text": "Exercise 4\nRepeat Exercise 3, supposing we are instead interested in the association between course grade and course level for classes of the same enrollment."
  },
  {
    "objectID": "template_qmds/25-f-tests-notes.html#exercise-5-reference-categories",
    "href": "template_qmds/25-f-tests-notes.html#exercise-5-reference-categories",
    "title": "Nested Models & F-Tests (Notes)",
    "section": "Exercise 5: Reference categories",
    "text": "Exercise 5: Reference categories\nOur final research question pertains to whether or not there is a relationship between course grade and department. Again, use course grade as the outcome variable in your linear regression model.\n\nState the null and alternative hypotheses in colloquial language associated with the relevant hypothesis test.\n\nH0:\nH1:\n\nFit a linear regression model, and conduct your hypothesis testing procedure to answer the research question posed in this Exercise. State your conclusions accordingly (you do not need to interpret any regression coefficients, just state and interpret the results of your hypothesis test!).\nAre any of the individual department p-values significant?\n\nWhat do these p-values tell us, and why is this not contradictory to your answer in part (b)?"
  },
  {
    "objectID": "template_qmds/25-f-tests-notes.html#done",
    "href": "template_qmds/25-f-tests-notes.html#done",
    "title": "Nested Models & F-Tests (Notes)",
    "section": "Done!",
    "text": "Done!\n\nFinalize your notes: (1) Render your notes to an HTML file; (2) Inspect this HTML in your Viewer – check that your work translated correctly; and (3) Outside RStudio, navigate to your ‘Activities’ subfolder within your ‘STAT155’ folder and locate the HTML file – you can open it again in your browser.\nClean up your RStudio session: End the rendering process by clicking the ‘Stop’ button in the ‘Background Jobs’ pane.\nCheck the solutions in the course website, at the bottom of the corresponding chapter.\nWork on homework!"
  }
]